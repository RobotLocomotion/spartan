{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/dlutils/plot_image_batch_w_labels.py:16: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to u'nbAgg' by the following code:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 478, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-b723bd616cf1>\", line 24, in <module>\n",
      "    get_ipython().magic(u'matplotlib nbagg')\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n",
      "    return self.run_line_magic(magic_name, magic_arg_s)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-105>\", line 2, in matplotlib\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')  # b/c matplotlib is such a great piece of software ;) - needed to work on ubuntu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "sys.path.insert(0, '../')\n",
    "from scipy import misc\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import LocallyConnected2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import *\n",
    "from keras import Model\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler,TensorBoard, Callback\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib nbagg\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "from common import util\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_vectorize(x,value = 0):\n",
    "    zero_mask = x==value\n",
    "    non_zero_mask = x!=value\n",
    "    x[zero_mask] = 1\n",
    "    x[non_zero_mask] = 0\n",
    "    return x.astype(float)\n",
    "\n",
    "def stack_frames(frames,img_height,img_width,channels):\n",
    "    stack  = np.zeros((1,img_height,img_width,channels))\n",
    "    index = 0\n",
    "    for i in frames:\n",
    "        num_chan = 1 if len(np.shape(i)) ==2 else np.shape(i)[2]\n",
    "        stack[0,:,:,index:index+num_chan] = np.reshape(i,(img_height,img_width,num_chan))\n",
    "        index += num_chan\n",
    "    return stack\n",
    "\n",
    "def grab_frame(files,i,path,func=None):\n",
    "    img = misc.imread(path+files[i])\n",
    "    if func:\n",
    "        return func(img)\n",
    "    return img\n",
    "\n",
    "def grab_frame1(path,func=None):\n",
    "    img = misc.imread(path)\n",
    "    if func:\n",
    "        return func(img)\n",
    "    return img\n",
    "\n",
    "def normalize(x):\n",
    "    return x.astype(float)/3500.\n",
    "\n",
    "def normalize_depth(x):\n",
    "    return x.astype(float)/3000.\n",
    "\n",
    "def convert_rgb_normal(img):\n",
    "    return (img/255.*2)-1\n",
    "\n",
    "def bounding_box(img,size = 100):\n",
    "    h,w = np.shape(img)\n",
    "    non_zeros = np.nonzero(img)\n",
    "    x_min = np.min(non_zeros[0])\n",
    "    x_max = np.max(non_zeros[0])\n",
    "    y_min = np.min(non_zeros[1])\n",
    "    y_max = np.max(non_zeros[1])\n",
    "    out = (x_min,x_min+size,y_min,y_min+size) if size else (x_min,x_max,y_min,y_max)#minuce or plus coordinates\n",
    "    if x_min< 0 or x_min+size > h or y_min<0 or y_min+size>w:\n",
    "        return None\n",
    "    return out\n",
    "\n",
    "def crop(x,x1 = 100,x2 = 500,y1 = 50, y2 = 450):\n",
    "    x = x[y1:y2,x1:x2]\n",
    "    return x\n",
    "def flip_vert(x,y):\n",
    "    y= np.flip(y,axis=0)\n",
    "    x= np.flip(x,axis=0)\n",
    "    return x,y\n",
    "def flip_hor(x,y):\n",
    "    x= np.flip(x,axis=1)\n",
    "    y= np.flip(y,axis=1)\n",
    "    return x,y\n",
    "def rotate(x,y):\n",
    "    degrees = np.random.randint(360)\n",
    "    centerx = tuple(np.array(x.shape[1::-1]) / 2)\n",
    "    centery = tuple(np.array(y.shape[1::-1]) / 2)\n",
    "    rotx = cv2.getRotationMatrix2D(centerx, degrees, 1.0)\n",
    "    roty = cv2.getRotationMatrix2D(centery, degrees, 1.0)\n",
    "    x = cv2.warpAffine(x, rotx, x.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "    y = cv2.warpAffine(y, roty, y.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "    return x,y\n",
    "def augment(x,y,func = [],bias_to_real = .5):\n",
    "    #x = crop(x)\n",
    "   # y = crop(y)\n",
    "    if np.random.rand()>bias_to_real:#bias to real images\n",
    "        function = np.random.choice(func)\n",
    "        x,y=function(x,y)\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(samples,ways):\n",
    "    samples = np.copy(samples)\n",
    "    random.shuffle(samples)\n",
    "    tests = np.array_split(samples,ways)\n",
    "    return tests\n",
    "def grab_test_train(tests,i):\n",
    "    tests = np.copy(tests)\n",
    "    test = list(tests).pop(i)\n",
    "    train = np.concatenate(tests)\n",
    "    return (train,test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = cross_validate(samples,10)\n",
    "train, test = grab_test_train(val,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227737, 22774)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = gen_samples(\"/media/drc/DATA/chris_labelfusion/RGBDCNN/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = generate_data_custom_depth(train,func=[flip_hor,flip_vert,rotate],img_height=480,img_width=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_samples(directory,shuffle = True):\n",
    "    samples = []\n",
    "    dirs = os.listdir(directory)\n",
    "    for i in dirs:\n",
    "        path = os.path.join(directory, i)+\"/\"\n",
    "        if os.access(path, os.R_OK):\n",
    "            gt_depth = sorted(glob.glob(path+\"*_truth.png\"))\n",
    "            depth = sorted(glob.glob(path+\"*_depth.png\"))\n",
    "            samples.extend(zip(gt_depth,depth))\n",
    "    if shuffle:\n",
    "        random.shuffle(samples)\n",
    "    return samples                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_custom_depth(samples,img_height=480,img_width=640,batch_size=4,func=[]):\n",
    "    i = 0\n",
    "    while True:\n",
    "        stack1 = np.zeros((batch_size,img_height,img_width,1))\n",
    "        stack2 = np.zeros((batch_size,img_height,img_width,1))\n",
    "        j=0\n",
    "        while j<batch_size:\n",
    "            try: \n",
    "                rgb = samples[i][0]\n",
    "                depth = samples[i][1]\n",
    "                rgb_img = grab_frame1(rgb,normalize)\n",
    "                depth_img = grab_frame1(depth,hot_vectorize)\n",
    "                rgb_img,depth_img = augment(rgb_img,depth_img,func,bias_to_real = .5)\n",
    "                stack1[j] = np.reshape(rgb_img,(img_height,img_width,1))\n",
    "                stack2[j] = np.reshape(depth_img,(img_height,img_width,1))\n",
    "                j+=1\n",
    "                i= (i+1)%len(samples)\n",
    "            except Exception:\n",
    "                i=(i+1)%len(samples)\n",
    "        yield (stack1,stack2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_custom1(depth_as_mask=True,img_height=480,img_width=640,batch_size=4,path=\"/media/drc/DATA/CNN/\",dir_name = \"test/\",filter_files = None,func=None):\n",
    "\n",
    "    rgb_path = path+\"rgb/\"\n",
    "    depth_path = path+\"depth/\"\n",
    "\n",
    "    rgb = np.sort(os.listdir(rgb_path))\n",
    "    depth = np.sort(os.listdir(depth_path))\n",
    "\n",
    "    if filter_files:\n",
    "            rgb = np.sort(filter(lambda x: filter_files in x, rgb))\n",
    "            depth = np.sort(filter(lambda x: filter_files in x, depth))\n",
    "\n",
    "    i = -1\n",
    "    while True:\n",
    "        stack1 = np.zeros((batch_size,img_height,img_width,3))\n",
    "        stack2 = np.zeros((batch_size,img_height,img_width,1))\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            i= (i+1)%len(rgb)\n",
    "            rgb_img = grab_frame(rgb,i,rgb_path,func)\n",
    "          \n",
    "            depth_img = grab_frame(depth,i,depth_path,hot_vectorize)\n",
    "            stack = stack_frames([rgb_img],img_height,img_width,3)\n",
    "            stack1[j] = stack\n",
    "            stack2[j] = np.reshape(depth_img,(img_height,img_width,1))\n",
    "        yield (stack1,stack2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_custom3(depth_as_mask=True,img_height=480,img_width=640,batch_size=8,path=\"/media/drc/DATA/CNN/\",dir_name = \"test/\",filter_files = None,func=None):\n",
    "\n",
    "    rgb_path = path+\"rgb/\"+\"rgb/\"\n",
    "    depth_path = path+\"depth/\"+ \"depth/\"\n",
    "    gtdepth_path = path+\"gtdepth/\" + \"gtdepth/\"\n",
    "    normal_path = path+\"normal/\" + \"normal/\"\n",
    "\n",
    "    rgb = np.sort(os.listdir(rgb_path))\n",
    "    normal = np.sort(os.listdir(normal_path))\n",
    "    depth = np.sort(os.listdir(depth_path))\n",
    "    gtdepth = np.sort(os.listdir(gtdepth_path))\n",
    "\n",
    "    if filter_files:\n",
    "            rgb = np.sort(filter(lambda x: filter_files in x, rgb))\n",
    "            normal = np.sort(filter(lambda x: filter_files in x, normal))\n",
    "            depth = np.sort(filter(lambda x: filter_files in x, depth))\n",
    "            gtdepth = np.sort(filter(lambda x: filter_files in x, gtdepth))\n",
    "\n",
    "    i = -1\n",
    "    while True:\n",
    "        stack1 = np.zeros((batch_size,img_height,img_width,7))\n",
    "        stack2 = np.zeros((batch_size,img_height,img_width,1))\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            i= (i+1)%len(depth)\n",
    "            rgb_img = grab_frame(rgb,i,rgb_path,func)\n",
    "            normal_img = grab_frame(normal,i,normal_path,normalize)\n",
    "            gtdepth_img = grab_frame(gtdepth,i,gtdepth_path,normalize_depth)\n",
    "            depth_img = grab_frame(depth,i,depth_path,hot_vectorize)\n",
    "            gtdepth_img[gtdepth_img==0]=1.\n",
    "\n",
    "            stack = stack_frames([gtdepth_img,normal_img,rgb_img],img_height,img_width,7)\n",
    "            stack1[j] = stack\n",
    "            stack2[j] = np.reshape(depth_img,(img_height,img_width,1))\n",
    "        yield (stack1,stack2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_custom(depth_as_mask=True,img_height=480,img_width=640,batch_size=8,channels =1,path=\"/media/drc/DATA/CNN/\",dir_name = \"test/\",filter_files = None,func=None):\n",
    "\n",
    "    rgb_path = path+\"rgb/\"+\"rgb/\"\n",
    "    depth_path = path+\"depth/\"+ \"depth/\"\n",
    "    gtdepth_path = path+\"gtdepth/\" + \"gtdepth/\"\n",
    "    normal_path = path+\"normal/\" + \"normal/\"\n",
    "\n",
    "    rgb = np.sort(os.listdir(rgb_path))\n",
    "    normal = np.sort(os.listdir(normal_path))\n",
    "    depth = np.sort(os.listdir(depth_path))\n",
    "    gtdepth = np.sort(os.listdir(gtdepth_path))\n",
    "\n",
    "    if filter_files:\n",
    "            rgb = np.sort(filter(lambda x: filter_files in x, rgb))\n",
    "            normal = np.sort(filter(lambda x: filter_files in x, normal))\n",
    "            depth = np.sort(filter(lambda x: filter_files in x, depth))\n",
    "            gtdepth = np.sort(filter(lambda x: filter_files in x, gtdepth))\n",
    "\n",
    "    i = -1\n",
    "    while True:\n",
    "        stack1 = np.zeros((batch_size,img_height,img_width,channels))\n",
    "        stack2 = np.zeros((batch_size,img_height,img_width,1))\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            i= (i+1)%len(depth)\n",
    "            rgb_img = grab_frame(rgb,i,rgb_path,func)\n",
    "            normal_img = grab_frame(normal,i,normal_path,normalize)\n",
    "            gtdepth_img = grab_frame(gtdepth,i,gtdepth_path,normalize_depth)\n",
    "            depth_img = grab_frame(depth,i,depth_path,hot_vectorize)\n",
    "            a = bounding_box(gtdepth_img,img_height)\n",
    "            while not a:\n",
    "                i= (i+1)%len(depth)\n",
    "                rgb_img = grab_frame(rgb,i,rgb_path,func)\n",
    "                normal_img = grab_frame(normal,i,normal_path,normalize)\n",
    "                gtdepth_img = grab_frame(gtdepth,i,gtdepth_path,normalize_depth)\n",
    "                depth_img = grab_frame(depth,i,depth_path,hot_vectorize)\n",
    "                a = bounding_box(gtdepth_img,img_height)\n",
    "\n",
    "            x1,x2,y1,y2 = a\n",
    "            depth_img = depth_img[x1:x2,y1:y2] \n",
    "            gtdepth_img = gtdepth_img[x1:x2,y1:y2]\n",
    "            normal_img = normal_img[x1:x2,y1:y2]\n",
    "            gtdepth_img[gtdepth_img==0]=1.\n",
    "\n",
    "            #stack = stack_frames([gtdepth_img,normal_img,rgb_img],img_height,img_width,4)\n",
    "            stack = stack_frames([gtdepth_img],img_height,img_width,channels)\n",
    "\n",
    "            stack1[j] = stack\n",
    "            stack2[j] = np.reshape(depth_img,(img_height,img_width,1))\n",
    "        yield (stack1,stack2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen  = generate_data_custom( func=normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height=400\n",
    "img_width=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa268085110>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa26ff32dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAD8CAYAAADzEfagAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzsvXmcHEd5N/59umf20Epa3bdsyYcs37ct2eGygQD5EQgBzE0u+JFACBAHCCSvSSAxIeQl5OV6CZBgAnZsCGDA5raNLWzj+7YsW7YsybqsY7Ur7e7MdNf7R3d1V1U/VV09O7JW0jz7mc9219093d/5PkdVkRACXelKV7rSFT8JDvYAutKVrnTlUJIuaHalK13pSgXpgmZXutKVrlSQLmh2pStd6UoF6YJmV7rSla5UkC5odqUrXelKBTkgoElELyOitUT0OBF9+ED00ZWudKUrB0Oo03GaRBQCeAzASwBsAnAHgDcKIR7uaEdd6UpXunIQ5EAwzfMAPC6EWC+EaAC4CsCrDkA/XelKV7rynEvtALS5GMBG5XwTgPNdFXqoV/Rh4AAMZWLSWDQAEXa4USor0CbzL223w/W18u1rK9TuuCnvs50m8n6LY2fbYxLJcd1kPbHX4+qUl03y5fUIAYyO9KFn8z7r2LrCyzB2PyuEmFtW7kCAppcQ0TsBvBMA+jAF59PFB2soBXn6sgvQmBHricoTLbi3ioyH2yxD9jxRpa5vn+RRzlEmAxU13Xz5LXlkaVdL1urqL76Zz6WX5WtllX4Do75aPiAwaXq5wHI9gZFvllfrBRDufKNskhaz5QPzu01l94W72PSu2OXn4tsbfModCNDcDGCpcr4kTdNECPFlAF8GgOk0a1JMgN/4NxdgfHYMQAHMAsApJ1XBzgWWvkBp9usAY69yFvCx1SXLNbcLlGZdDgxZICwBSrXvgK0vQafYDlc+A0PlesqAUisDvkzhP9S6bqDMAVX/TupBhN3oyoGSAwGadwA4noiWIwHLNwB40wHop2MiLjgdT7yuHxlYuoAS8Gd4bFsHiVU6yh0MVmnWdYFhp1iletwuq+QA0QqEKPYfkLCyzyQtdpY1gVKWD5X8gAQe+8J5WPFnv0FXOi8dB00hRIuI3gPgJwBCAF8TQjzU6X46IXTuqVj3JsWWWgUsK7DKtoHS7NMs68EMvVilmufBKrX6plrP1Z0krDLJ1/M4YH2uWaXJKN1lBQuUep0YrzzvHqxFVw6EHBCbphDiOgDXHYi2OyHb/vwCDB8THzxG2WmQ9CxXSf0uME1Lm2Y9R5/t2Cl92STXTlXVW23bBDAOVDsNkibzdLFJvX2ebZ5yV4AHzzZs812ZsBw0R9DBkB1/uhpDKwSgqEClTp3nglFWqec7tjKwtKnfz7FDRz0uBdL0f1U2qdbplOoNTBwsfVTv0OivDEjVPABovOxc9Pz4DnSlc3JEgObOd6zG7pMEAJG9IZVYpXk+WVklU7agRtvU73bBsk3VWz2vyipdNko9zc4qXao3V/5AsUof1liWb9pBASBEjIAEzrv8Dtz7Y3Slg3JYg2Y4ZzY2v/kEgABqAaKePFDCBBJV2gXL55JV+gKlpQ0OLG02ziqsUi1TxhrVMgeaVWppTLu28rYwITZ/AqyyHaDk+ghRdCKFiDH66lXo/17XKdQpOSxBM5wxiGfednIOjgIY2EwYWSaStIPBKqH0W6rye4ClCygtbUxYBW+TVU7EoVOVVbbr0OHyngv1G0jAUFev83xfoDQBXE1/0d+twW3fq6MrnZHDCjSjF56FZ0/ty0FC6Gr41KcCNAeAsQVRsbIFnCZjiBDAgKRZxuHc4YCyjFGq9Trl0DH7NYHsQMZScnmAzijLvNk+Aec2O6QJki42qbZngqQLPEOKEUAgpBhAFzQ7JYcFaIrVp2PH2QO8UycrBICA+j6g/kSI4ePcwHnQAs8L9XiAKrTB1LWCJQNaxfCiMnbIpdnrlKneXNmJOnTKPOW+jFIec2zPraK7GKMdSNX2TZVb/W8DStmPetz6+VGovfhpdGXickiDJp15MnaeMR0AUNsn0BqgBDhTgASKbFNKz64AjVl62NEhwSpLgDIp7wbLKqxSLdPpMCEb+7TFUnJpVWyUhXIKWJaxyuQ8LuZVAsNinqxbpnbb0lVglCIZppr/OwsexE8wHV2ZuByyoDl8ySo0p+hoUdsnAAJafZQstOFghL27CUEjxNjCqPOsUj33BUqjrE9MZWE4EwTLMqfOZA4T8vF8A9XtlEkaD5YT8XxzQGwDSplnA0p5bqZl/aWgPO3mORh+3rPoysTkkAPNkdedj8ZUx4p2AqiNJQ6f1oDpxUH+tolEVafNIfYvaR1YVtkOULraAKNOG+VIcXixKngJq7Sl+4ClL6ts16HjYpUudZkrx4X/JOlFFbwVB6gFsRcY5nXdrNLmyAH8gLLAMBWg1MYGgRfNXotrMRtdmZgcEqAZrjgW2583r5Cu/MDqIhJMqI8kD05zKlntnbVRYPq6GsZnCozPY+ycQOcYpUvtdvRTtDdahqn04wJKG0i68ohEJfXbByQ5dijrc21qeWSCk51NcvVtnm+zvXueXorWUE8+zv4Iv3XCOgBuRunyevuo3mUgmZ1TzAKkfh6jThHq1MLb1m7EFSeo6+l0papMatCsHbMM2160sJiRvk0icAAnkDHL2j4BEQJxDyG2qO29uwnjJi77sMWKZToBlGWec1LrM327gNLsX2WWVdRvHxtlVaDk2vBhk2Z5m+qd5ZPAIzvmY+/WaYXlNsVoiJsfOAEXnfaI0a6NbfLqt8tG6QOUSV0FpBUGq4EvYoSUgGbCXGNs+usLsOTyX6Mr7cmkBc2df7KaX9uW8v8iAEQIBM3y9igCwjGBEEBrCkEE0NV1AP0baxg9ymjsQIClr9rtAkoGuLQ2SlilzX5pnqtAW0X99lG9XTZKV3kfGyVX3jee8qHtCzCybSqsEhHG4xr6w6a3U0ftwxUexB2rY+ZYZZ1ahf6SNhLGK8EyTD3ql//Rf+Lzl6+wX19XnDLpQHP85ediZEkNghRcMMHTAKm4B6CWhXUawAgAtf0CUS8hNkLXaqPAlKfq2L9cAU4bWHnYKdtilQUw4+uzZNNgl664SptN1Ob8IUXl93Hq+Dh0XM4crrwJfqXeceLDeFxtrts91w2Yqay5fwVefvb9Wl2TVU5U/XYBZQKCsQbCLqAMUiCVsuHvLsDRl3XZZjsyaUCz+dJzsPeoesIgpfqtsEoS0MDTtFHG9aRM0PDrL2wIgIrAGY4DU9fVMbJCBc7Os8qJAqXdEVS0PfrMxOH6l+zSBDeXvbKdMCGfEKEyZ06hXgWwlMe/fuxYiDHP/U1Ee6zSpn5rHvASO6UKlhxQyvKSgYZqe+l4/vXNX8VnLjvR71q7osmkAM3WnAHsPTpBL2EBJZGec+95BrIBEPUBInvrgaAprHbPoCEQ1yjrh0TSFkVAfUcNzXnN5x4kzbbZ+hbATNsqc9qwfWpt6OxSdexUsVWWeb1d4AfYVe+JgqSeHuO2DcvQ3NNnvyEWqQWRF0iq/dpUb9M+qV6L2mYPtTLgczHKEEJjl6HyoxOBEsBdcSyix56ofN1HukwK0PQSAiAsS7kpogImkDh/gobQmSoZ/xnpe5ZAcR2NhSZwdgYsXQHrLKMsqK7FMUuw9AVKK1tFDphhEGtlq9gqyxjlRJw5ZnkuRIgvl7PDXeMDeGjtEus9yC7CcpvqFGVt+qre7ThzAGRgyTFKG1ia4AsAsQgQQiAC4X3XXYt/Oe5k9/V3pSCHDmgCzgfYJXEPIRwXBZBUF/SwedQbi2AHSih5NkBqw5njE09plrN5t816Puq5HE8YxFanDtAZsKwKlMDEWKV02IxHNTy0rgQws4GBtasHGYDZWaQch6+NUo63zE6p5xXBUm0PACIEWZ+xyOOcg9NPRHzfI373oSsADjXQBPI30gJ0WhlFWn2EoCUQtCzl0vakip6nyRderdcZVsl6vblyJR5wV+B5sY4ObJxI1VkCJgeUsi1bqJCtnAl+LqBUjzu1PmUkCDc/sgJolagspjDF6xS17dAxWaXp+c7ad7BKmWYDS9mfVMclcKrynu98F/923Mpq9+IIl8kNmhWBsUziWjK9MhwXefsl0vdUL8aXj6d9TpxVThQo1TzN9ugsXwRqzlFj1lEBs1P2yk7uysiV51ilmnfjAydZQtlEue1HkeOO35Kp5+16vuWY2mWVMl1tM7Q81BIwVZaZjW/VacBt93tf+5Eukw40C8+uCzhLGysmiQBo9RMoRhLozrSvjmFs2XjygpcwysrOHLOMB0iqeT7OHrXvsjAhVSRYdtJWWQaSshy3CrkvSALFfXJk3k/vP7kcFMuAkwRmLBjG7y+/VxtD2ewcF0gm7djDhGQ5zrGjAqXsTwXFyPLiyPQ4BdL3/tfVXbZZQSYdaLLiAE72GXcBbZqn/eDK8mY9AihkAKai6u3yfJetJmTWL3P2mH37hAmZIgGzk7ZKDixdbJLrByiCIeCezrhzfAB3rV2mhFjw15wPlgFOEqDeGH901hqvud4mcGaMVAE9CZQyXWWtBUeOoX5nbRk/MrLPWAQaYNpYpirrrjgLx7/t7pKb0xVgMoGmAli+mlJpmQmw1P2LY8RzG22DZbvOHD5dB1Mi4QWUel0VjJgxKnV9bJWdUL85oHS1yaneeVs6AIckcP39p3iAJBgnT/oApm1edPojOHbKDgB+rDI5LtopbaFCst0ysDSB0gwjAorAyNkxASAyyv3r6qvweXRnCfnI5AHNDgkJkYcdaelFbzmXBgDDZ4yBgjSouwBySoPyUMsvB0vfMCAVLAvOHuP6bDZKn1AhNX2i4UIusCzzenPtVWGVMj+AwJ07LItSWDzhHHA+77S1WDmwjQ0Tci2OIVVwG6uciApus1mqIgFUBUwXy5Ty5JWnY/kb7ystd6TL4QOaJorYVO4SGTlrNC/O2SsrgGWnWKX2X2mPAz2zPdc6lWq6CpZlQKke+9grbVs3TMShI/MAHUjv2bkYz2yeBafYgDOVcKCFd5x2izI2P1tlkl6NVWZtMbGVPYgK4GyCpt1uaWGYRnn1/FPnfAdfxHFsva7kMrlA06WiG+BXwdGZicVclaWNnD3Kq+BlQMmAIe+RLuZxdkxV/S5Tu83jqmtUyj6eS0Zp2ij1PLdDx8yT6etH5uCh9YuhifoVmF8+cWWAd6260cooOTaZjId36Mh6NpCUdTnvd8ZGUbxnABALUhhlrppzIOrDMqWMvH4Vpl59m3f5I1EmF2gCbdk222rTAOH9i4Q3s3SxynZUb/2cV4+L9dxAKeu6gFJNd4UJAZ1hlHw4kZtRuoBSrV8ATHmRLuA0bsRrz75TmeVjAhi/OIY6tjLVOxl/kVFy6ncIuQWvH7PUy/BqeQayDhD9wCe+hS9ffUxpH0eyTD7Q9BQvQDWA0SaNmUC8eKwte2WVlYRsrFL7n6a3Y6Pk6rmAUv53BZ/rfbbPKm1ACZSzStf2tb/achye3TEN3iKvT+S/oq85824s7t3jVL3LbJSynkv15tpWwTJUrt/HdqlKFTZZJnveuhozvnFrx9o73GTygKYKcDa26QmCZe2bbLNx7CjU7SEAC1g6QY9Ls+dp+el5FfXbBGSbWm1T79Xyqlfcd9VzF1i61G+1jm3TMdc2tiqQ3fHs0eWAydkvgey7fNWZ92Jp3y4vVhkaYzDDhOSxL6tM2hIFVhlYADM2Hn45h9wUbvaPi6GazPMv//Zb+Oo3llvLH+kyeUDTV2zAqYGi4kH3ANosFtNMJ1FqrzxQm46px76sUk2zqfemGu5rq8zKM2DpEyIkpUwFd7FKeXz3rqV4ctNcWKWglivH6fMwZcYo3rFyjTerzPP91O9kzPy0xmR6pFD6E+n89CJYRh2xT6ntpTGbFkeRlJ1/shqzv9Jlm5wcEqBpU8UL6WUAyeSPnJ94y9tVvX1smC6nThVGWRUkbeVV8Ktip7SVK4uj1NvxY5RADpQqcF775KnYv9djGTfO0QMgnNLC+8/4hda+i1HaQDLJL1e9s2s1GGU+m6hotwR8bZcTA1S1vgmiH/zgt/DVr3TZJieTCzRVUDMAzqam2wGVj9dUZf8F+/JHpU3V25ZfOE7/c0DmAksbC7WBpau8BDhnCJEDLLkyZZ5tvX/OhlmcXmj2FyLGjzacjOGhfljF+iDkhycufwa/O/8+rT/fWEpuIV+b6p2fy/Z0sFSB0uR7MarJU805AIDFtd2F+MyJ2jkHb5mNod/aOaE2DkeZXKB5oIQB46GTW6gDHbdTamWUNF8V3AWW7QJldg7e6VOFVcpyPmDZjp1S/S+Z4C+2rMDwXgdgZhef9s2A5+JFu/C78+9zqt/yWnooYoPO5XkyNrvqneenaVrMpsiAMlSGGaVDD8ADp2rPjAXh6dZM7GhNzwByR2saTu3byN4W02tepppLec3cu/EfONqr7JEkkw80HWzTVs7OQu1ssz44XkjzUcOrsEqgM8yyDCydTpuKoUK2cn5hQOo43KwS4MHSTLvygXMgHHY9IkCY2q3xI/bXq67P+jNV8GxLCAerNIEya4thlXVNdddV8AAJUHLsMqQEOMuY5jOtQWxtDSISgcIok/vzwNhSrOzdAnPNTJdw4Ueqg2nJbVOxadWIV1tHikwK0PS1zNg0MC9RwHTohMh54WWA6XLqAOWsMimjp00ELFk1u4OsUp675ny7nT3+rDKpn5S56tGznYApJfP5GeBZ72/i0tN/XgDLKqzSVL+TsfLe7yyPUcFVsJS7EEWlV5bIflHDusZ8RAjQFKE1zjJCgEfHF+Kkvs1KWnlspk0CivGSmQ912aYhkwI0C3IA2ebQeWMI6jGEoAR00oUZkheOSmMrudk6E/V+V7FVlgWgyzKB8lHzkjr2MKEyRulik3kZ3vNtAqdWhmLIFc6vWX8mRkb67IttWNRwVQs4bfkmvHr+PQXvt1w42GfOtxyrb4iQjVGGAEKS4JVfVAg3cD4wvjABybQl1ZNuskzbtElTTNU8pBiRCBAgtqrtmz98ARZ/srtzpZRJA5pOFmkDR1cbDNgOXzCqPRYmcObpKnspAinbtydYuhglcODAsgqjlOVtcZRSXLN0VEbJqd1J/TirL4+vf+akBDCVe5SxzUIoUfF7CWoCf3P2j6xTGuvUaivwPD/XwdLGKFWglM9cnKZFBiXm7Jg7on4rYHKixmZK1ZzzrrvA0SZ//Yf/jSs+aVkA5QiUUtAkoq8B+P8AbBdCnJKmzQLw3wCWAXgKwOuFELuJiAB8FsArAOwH8AdCiI4v0lcGjpwMLxc5GAqCIMFWU0FSP9aZjBS7E6bzrFIrg2K/KmBOVP12MU49349VBmRsU5uxs4Rh7m5NwXcfOaNon0QJcGaFgKMW7MI7jr5ZGYe+SEadWlYPuO8snTJW2WMAZQhCpICqDTil7Il7sLE1Cw3huZWwImEBequLZJ5m4PxT/7Aayz7ajdsEijZpTv4TwMuMtA8D+IUQ4ngAv0jPAeDlAI5PP+8E8MUqg9GIg/lMlZO9osg6BMTzE8eP+axmL6MgLU8/Jr4OcvVIFXObiIDUPXfyDyEN6CaBWhA7ywYkUEsBRtor1XK1IE4+sgzFmS1RbVttp5blR6gHEepMugRSyTqT/EhrX+5lU6dIY5byvB5EGTgGEJmKXKcoO/7uo6cXvz9bhAIVP/94/nfxrqNvQp1a6SdCT9ZPKwPMAHHGNnvS/uvUQg+ilIkmaT2IUafkE0Kgnp6HJNBDMeoQqBOSDxKw7CFCAKAOQph+AGT/bRIjcQINixo2tma1ZX+U0hc0AUw8hlOK/DH5X6+9uiPtHQ5SyjSFEL8iomVG8qsAvDA9/jqAGwF8KE2/QgghANxGRDOIaKEQYovvgJxquq1cifo+dGLEqtiymqmCy3PTzlk4h05yy1R5jln6qt+2IHfTw+0zrdFUwQGgprJAZoaOms45b0wV3GfDMQDY1RrAdx45I+uD9YZneUL7wSISeMtpv8FYXC+wSteCvtm1GwtlcLN05H1QWWXSXq6CJ8cpw2TUkViIjHHaQooebs5JPOIWwAxJlKroAQksqu12lnHWL1Hd139qNY75YJdttmvTnK8A4VYA89PjxQDUYLFNaZobNG0446l6l5ad3tTKCAEQUgcQFOAEICDSF1dX0wGw5wIJ2wyQv9QxBALGqQQUAZP1cCs2Si3dAMt2pzTaFsgoW0nItFPKNNPrrdoobZuNrd0/H798/ARe01a94eoPo3JPPnX+d4w+i3GUGbv0dOpws3RMO2WSnwNlQDmjDBTAidPxRNaHG9jQ6sf2aFpavnxvHykhxYoWFWTXNrs2gh6KSutLcJQquLxnErA5FT1EjI+96mp8899fgGjdemf7h7tM2BEkhBDk4ykxhIjeiUSFR8/ATL9FOjxBlGObudNHbU93AOVlkQEnIKA6hcxzjqGq5zF0sLOJCnIcWBbZYjlQ5vm688ZnNSGbp9uHUZZtOPaF+1/gHUqUsc60eG9fEx8/7drSaY2+a1SagefJtfgxyhCkAaWvPNXqwa5oanFB4ApqeQ5scWbqmBaMVlbLTdtlGdt88w9vwhUnHNlOoXZBc5tUu4loIYDtafpmAOodXZKmFUQI8WUAXwaAgTlL3ahiAc4CODJ5QycIFmw1D7marYQgqZpWGUDGgjKgydimUV9ltmViCwcywdIGlMn/6mtUcqxS/vdhla49dHa3BnDlw+fYL5r5tVO/g+ULnsW7j7qh4PFO+ubB0qZ6A8jslUl9pNelOnHcrDJkVPFIiIxlaulpPzGQAaYPSJorFskfihhBdg/6qIm5tb3udgw2qYKjyjg5tglKguXlWILTViK+/9HSsR+u0i5oXgvg7QA+mf7/vpL+HiK6CsD5AIaq2DPLbJNVpTmVIGY08uYdbFOkx2UhSOa5BEMy+1BEAqoKrDI9TNMBNyM1QbEWxIU0X0aptld1lo7JJE2gTNrQVWAAuGHnCjy4eVHhugrOcMsXf+mZP8Wi+u6s33bB0hYmlB1DDxVSwVICpckuOZAEiqr5g41pGBP1tp008ruLBEGd1TQrzGfsyOuy9aGCJwecgKK+KyCqzjJ6yzU/O6LZpk/I0ZVInD5ziGgTgMuQgOXVRPTHADYAeH1a/Dok4UaPIwk5+sOJDM5HTXeB6+iCOLdfQqT2TCaMSAFJM193BBVtm6pTyARFs0yWppQtMFYQQhQBVhXeEWS3VQJ+YGm2W9j/RgFJm62SY5i37jkWDz6zkL0WwAKcqfzF6Tfg6J4dBRVc9i377IG+4rprlo4JlEkZP7AMtJWB5D1NYEYNI8qYpRB4oDkFY3G9cnykFDOUKBs7xeijZoF1m8f5mPKx24LauXTTtukbSH+4io/3/I2WrIuZsgLAuyc0Il/91QacafroPEI0q6mUz4HT1g+npgNu26YJpNIpBCBT0TPbJpDZOCVwQkmHzFOANAtpIp1tqqIGopsgWhZ4rv7nQDIZnxsos/4UtnfH3uW4ZYOxbYLyRRU84dCBs7e3iU+d/h3NTin74xw7PkCZjJ136iR1ikCZXH8CliEpzhqRA5nKNGPEiCCwK4qwoTU9U8MlALF7+BggJEFK1RzUvGTcCWAGis22TArlZPtZclBMJ7Bs87GvnYMVf3SnV7+Hm0yaGUGalACij4wuS9VyQRAK4AEV2KZRV2WeJpDahhcLaLbNJI1nkRpAihxg1XSIALHQHUPJgxzDZJfqsQmWan2TZRbUbgmaDFDKcxXInhibhzVPLy9cX3INdtVApvb1N/DPp36HBUvfaY1VPeCQxyVgCfCAKW2ZEQQebPSiKfRXy/RYc6LZFBkgVCMF5Jz5HsonYnIgy0ksdAZpsktTVZfPXQbkAvjEhd/FFTgyVfRJA5ouNdvHvpmVIWD4aEdBi5rPZWvpJTOFTHYoXzO1jmSbqkqvli+o8Yr33bR7xoIyepalgdJGAwM4Tdsnr3onNXlWyQGlPDfb+sHjpzB3sCgm2+zpbeJfTv92wU6Z9Ff0gttWFMrT3GAJAHUKMhulCpaqqCAJ5Gq5VMnV8KIHG7351EfPX3hNFbYAnzRJSLA0Y05Nsc0OioxnA6LILiVwwnAMSeCU8vQ1p+Ko1z3gdY2Hk0wa0CyIFb3gBD0AaM7yXT8mbUayTS2tfAplHgTvDng3PekA7xySIAnkjaiME0iAkWOqsQgQUrXrNkOGVLCsB/mSaXaGqXvbr3/2FDy01W67zESOXblvz1v+BC6Zc7sVLF2xler0Tp954C413BQJkMlxzirleQSBWAhsjYChOAHMiS7+GxiAx8WeAjpY2kBSdQwl986I48zeJQU8DeCU59n40rS/Oe26I5JtTirQ9GabZcBZt6gpUgUXgIDuSXcFvHO2TRUwC4OQYKh0LQPeTbYJQGORKnAGJBCDMuCUaWodlW1GCvuUltIAuupuE83JQ7qtkGOT6kK+a4aOxy1PH2NtGzAC1pX7tGLBDrx/6U9L1W/Zty0IXV5xfj0oQKANyiIhEBKxoJiVkexSCEQQaAqBYUHYE/ek6fye43r/xaByyeTkup7Z+DPwLzq98jJxobzWn6ptaONQ+kmvNaLUwSMCIP0ekrEFGXBKNT25N0n/b1u78YjzpE8q0ATat2GyYKvUL2OTWkMMwOhhRlxbgGbvVFhkvtiEAOcUkk2pDNIVhhSLAAFFDHAWb0Gisqe9UQBIVTM7DliVkANMbvfFR0cXYc1Gi/2SEfW+LZo1hEuX/iTrrwcRyyjluW3Gjinqiui2ZS/i5FcS8ucoVm6zGiokQTJGAqZNAHviGmJBaCBfVYjfFbJov7TNxpHlZZnkOuymELOOzLfl2bzpoZxBlD0jKLBOEzgjEWhOoZHXnY+p19xeaP9wlUkHmoAdOL0BtRl4sc12PemmbZObq24CJ8A7haRo6jqK9k22HKOi5w8+kEFz9iIkwBkwqzhGQr4Y+QsRak4GxcaZxghesWk1Nu6ewV8QijZLVd6w8i68aOojBXbp8oLLc3llyXj4vnXWmRSSP1YRkrngsRCa+p3di8xmmQBlBGBcAA0RoIkAsaAMDDlVXAWUpH8+sFzJhY6aAAAgAElEQVRnyXGhvGmmyPKNMQcM8JrHgXH/kj5J+eFNFXDtvUieiVizayIFzjAb15994tu44pojh21OStCsJNxLUzMA05zlw3rK0+dByRdpXc62aTqArIt8gHf0yDSVbXLB70CRbcZI66qAxLDNpL3i7clflJxtJiqZzA90BkIARJwB1FVbzsOGXTOLDcuhUK6Gm+D/muPvw8sG72cdO9ysHVvYkBSX9dC1uJptTrgEyqQM0ExNI2MiRJSCjI+DxwTOZNyqA4ZP5+zGarravlnHPDaB0nSWBYIQEyFIv2dp94QSi5kw4uQZ0eI8U3unvMaht6zC4H/d5rolh41MHtB0MUcb22Rk/wKJasgZl0e75eMrAi3nOdeqGOmuxTzkUGxB7wVbpwgK6higs80AwmCfdjU9pCLbBONc+vHOU/D0bjtgjm+fgsU/V8ZTA7afGyDqFXjJ6vvwu4P3pNegL+4rr4t7wW0B6TZRwdKc6ijZpnouRQXLBDyBJghRCpTcMoBlYouh5Jx2nWSVOqMsMvVkDEL5YYz1/+kzk/yAFtV0CarSKfTnf3sNrvivI4NtTh7QNMQnzIgrP7akaVHBLWxT6E4hJ9tM4zbTHmXD1gU8XGILeGcZJwhqYHsebqSGxcOilpvniT00MfrnwBkIob04cfpfspBxUcfXHltdvO8ayyYNMAEgaAELbk36fvim03ApnZZcQ52w6N2P49Tpz+DE/s04uWdrEgdovOBmUHqS7ifc3HDADpQyT25wFqWAGTO/rCbbzGyU5vfhGp/DPgn42yh51dtu0tBFFAHTAM48LW2F4sRJ5uH8OhxlcoFmO6xQSR85ylIuY50GcKpFfAPeJyhlIUg2sXvRc+B0BbyX9ZXMg8/PVWP/NZvOwN7RPq08t+7llJ9NzcZi2wlU/rgFTYGtnz0WW+hY/FRpVBAQ9RJ2vWwUP73wc2iC0EfCCZg2cHSJPuUxEQ4wbaLO2jGDxfMyFrs6yr3dtnJV1G8uskB+x5HRfcA4raSqzgFnxKQP/Gou9j1/h/WaDxeZXKBpSFW22ZirhG2YbNMATrYM8nLcKkhqCFKVVZAE7AHsXF01nQs/kg4mqYJLKQt4d6npRfU8QiwCXL3hTIy3EqiyOXZk+p7nj2HKd3pA3CrClh898zsmAYTjAnO/34c3X3spBAH7FwTYP19gzunb8T8nfz0db/sMxwTMOEvPAVMKxzKds3os6rgpttk77EygAgP1A0szBKtMQiiOMcYmq5Vl8n9//t1HRNzm5APNKnZG5GWHl1XshmGbfMFcTfc2f3q0HQvkcZsq2IKP28zYIQGIAyCQqjUyEOSA0rSdRRZGm6vrCWDetON4bB2e5oQA0+ETN/WXyLXvvF7O8uOYXu6UrTGmbAVw/1y8BgmQ7jqZ0LNyL85YsBn/vOSHqDv64dRxgAfMvFyxPXVmlho07jt9UZUycOWZaGfBsmzUgUNNjxBqUyzl7KET76rhkbNbJS0f2jL5QNMQH7YZh0BjtgIOJqv0ZJtswDv7Mutqu8uuyZ0DRcbJLVisLeghCFGaHwaxzubSBzdO09Q2zID3gGI2dlOtsz/uwdbhacyFF9mmen8GBscATIUgysanAWd6X7Pv1PwlSm+4C0Rl+VkPCeDhaVhHJ+LVOBEgYMf5MX5v1R04feBpvGTKUxmQGpbf5L6oxx7kMFfHi8A5EXG1wf242aaMAuVhWFJs12tbEIYTdbZQ7mmPce7UJ7G273jEY2PebR1qMilAs7a3oScYL5P1JUtl9xkMYGbnCkAaUhaCZDqRtHwUQdW+1xBlKnWZjVGNw5QiWQ9XTk6FrAUxEAO1IL0Xnk6hLHZTCIAC/GzTCjRaRcuhCpY22+6HT/oJPn7GJZh7r85uOcZpfqf5eRE4ORU+z0vbFsDc2wPc/JvzcTPOx+coL9PqJ8S9wP6FAh971dV4Xv+GDEjLADOAyFT0DODIyG/Dq6714bBlm84doBjc7xNVoDJqtp+MROT95gt02NlmUjfW0t947+P45soljtEc2jIpQFM0m+0tOkzAzrNVwFQZJAzwhM42fcemAKOramFhYgirii7Bxww/kiBb9hK24gC1QJ3kF0BElHhJDOA02YMtdvP2HcswMt4DYYB2hrWw2zQvOfZuLO/dgRAx/v6N38T/ufcN9sFXNb+42GmWl4MyB7a1MQGMAfW9wGf++fX4DAH7FhPGjh7HLy/6LB5ozMOpPduLfaeiAiegq+aAG/TakYIN0wBLk1GWRRWUrUgQQmCfqBWeO3ZGU7Zwh842tYB3Etj4txdg6cd/XdLzoSmTAjQBYM5P12PHbyvzl21sM0sAdp4V8S+hD0ByIUipEc1rPyEr+5wY27Q5WtRycrV3Te0koBmFiFNElMCZvOzu2M09zV6MjPdYblQROFW5aOk6HNXzrLVu1obLvllVTYeH2YYBVzVvYLPAlGd68Nrb/iprZ2ilQG0fIa4DcU1g8cnb8H9P+CaAInACfup5vniv0NJ86qo/eLZ91m0xqbZ91aXI52ZMhKhTjD6KMIYweb4cnnTfBYg//Oar8c2PH55sc9KAZmvrNpA4xptt7jq1wq+7zbbJloUOnHCr4a456b7OJhvDVFc/kiJ3wATAAifiAA0kYNkTtBCnqqsJnFLG4xoeenZBYUymiUAFTgAY6G3g7UffhjpFiJEHyG9rzoAIAUS5XbN4wdDZo+yjxJbJ5rPMk29HPgKyvPZIEDC4NskQAQEgDK9fiFfveRe+d/6XknvCAB3nXVfFxxtuigmWsk6AZJ91ddUma79EVuBUARNIzD99FCOkFvahpgGnOfMnb78Y8B5qc9T9wPVQlEl1ZbOve0w3P5rfeWqnGjkaEPU4z9fqkJ7mej7TsiU/ynoVpn6hjCAN6Ez2GKf5QlC2UATHMGNhn4Ui0+OsnfQDQhQHaEQhxqI6Wz8pG+CObUfh7u2LtTxzf3WVFROAKT1N/NHxt+JNRxVX7Y4Q4Cv//juIevgxayBq3vPCueNLkaZFpYgNoMn2HAimnFlWAFNunpqwPIsKHqSOGZ+Pb30AWZ9yemmdEsDsIUI9XRPU9rGJnOHUEAEaCgj2ZcsDlkuyHKBftMD6TxUnQxwOQqIKYhwgmU6zxPmU7J4Rv+BMDC3TA6klWIKAkaWE8XmJ6ilkCE5aRq8j8nRi0lBMJyqWIaUPmZ9VI6HVJfY4t20mx0n9bJX1QAam8/fGBDGZpq6OJMFNpgeUvHxEyQZsfWETPUEEufna+qE5GG8lSoa6uo8L6GthjNcefQ/qFGkrINUpQogY//Kt12DwifRlSk1dYUOwYJap6fL3zfg+8nMqyefySC9nHGvliW9LBKSVAYD//qt/LlyHS1yB8TaxrQsqwTI5lisrSbuqZKKkr86EfLGRGMkceqlym2PrUUBQnWOftK8vTpKcB/lxupVHBEJT1BCl6bEI0BAhrnrl8w+ZfdJ/Lr59lxDCsV1qIpNGPZcS3HQP6OjVLIkbnUcYn2uYtctU7swYh6Ka3qZo1T1sm5yabs4xt62AVFgR3jgmGKFJsiIhmUEkCI24hoAEaoixv9WLRpRbwgLKgdPmIV80fS9WzXoSALS5ygGAT9/3Ysz6wRQMihwwZf++4u1JZ743X5VdHsvHQHWEm44jLkzKtrSaTWzMVAKWLR8oAmafXDBZ2UJYvSbb4iOAX4RAmY3VFcxfJq//wS24cmVxF9JDWSaVei5l+vrR/ERhAvsXp4BpU7U4Nd0lWT2lrKneK8dC6GV1ZgbLsT4OIcj6eMZC/2h1DHU8O87qklNV39vow/qh2dg0nCzlJlkvoIN1xoZJYMG0Ybzy6Adx7swNAPKlzaSsH52L2ddOKajAUtW1TSjJ2GebSg6rcmd9l6jpxrFZnxUB/PfQ2YVkqTqrnzJxqfraSk9IAHOAAvRRiDoFkCvNJ2X158oETskyfUQuSKKySuv4kW+VYoq6pxOQXGuAGMFpKz1HcmjIpATN4JZ7C47qxmDxy6QqKhAHkD7VOBA1T61AqoOqbW1JW7oJnlyd2ABhFThbcYCWCDDc6MWesX4AxfAYFTjVz4uXPIYzZmxK1S/KADNW1K8bvnK+MqC0vRQws8gvz+mOLPAmF8rms3VVEe46bF9Qxy+0etd8+WIv22FbQApzVaLEQz4jqKGXagiUvyQ/7z0w7m8MORVUn0tfJpEgNLPvuPw7y5bzk+urqs4r6CvMX3L1Lz1GcOjIpARNTkbnGewke+B5UEvSjC+fYSYqmArl2Nqm0S7HNoXaFnhQFNBZow04gRw4VaC0tSfzhUhY5lirGH9XcPAYx8fP3ME7kBQWcvXlL0XvkP6dmIAJTEK2adNSbGNxAK+vA4YDUhuoSsDsI0KdQoRE2kdvl7T/eQC7KKjlLgYZg1/JqXC9bUwXzfp43plt151sMukcQVKo3oNdbzobIgB2nqnMs84K6P8F5/jJygqr84dzFBEV65FSXs0nLS+v73IKcQ4h2ywb3RGkp9kcQ1JUINXSLQB93rwNkDtVqrtWyllHdYrww396oXqpOUhqwCkKacRQZo2FEvTfuOx7zW5Efl7IA5OX2/5sjiO1vC1PdSwNX7gfv/ytz2VDLLMVVoEYNWC9DmBO2G+0Fad9Cv0cycZuTQjN8SPHpi5AYjqATKDMHT6knet5vDMIAJoIc8eQCNAUISIE6fJ6waS3bfo6giYt0xTNBoKWgKjBaWf0b9BRz6W6M+XNyJlchfdXv6VtMy5hmra6pphquktUu6U8P2vuJsRCbuUQZHZRebx2ZL4fYKr57ttSYIQO34guBRWey1PYs6Vdk3la+0/Tp62ZoiWHlH84KWOgNpY6LaghQLqiuiLchm9AApxVANOXWebt28tKFd1HHv/G4cE2Jy1oAsDglXdg90klb5Kppitp7noGEGvqtFEGFlDzsF+qKrutLRU4OXU9V8thrZeVVT5A+RQ/IoFTZm1l21Xl8f84ISnvAEyKBcsovcSiNlttm4yUgq6wAC13npXXM15443vZl8YHPH3V+alBH0KS9kuTHeYMsyliNEWMsXTDt6bIbZguwOyUuBZJ5vI/cs71Hev7YMqkCznSJI5Q209oThWYULiQWdfVhhJClKcp5dNjnxWQsqS0LLcikhr3mZfXwVCfRql7u8sWLwbcwLlyRjLnOl8NKVkJSS4V98NrV2NwXYwaRBEwU5EqOSvyWgNiAdWcXsmGEHGSfg++IUfOdpXy2mOSpQkIJPdn8PY+4IX8ykmAXJfUY/yWugAwFI9iMNDV88ROme+zPiZillkC/mBp20EzJJE5hdhxJjoSO6UyZFY+AgVWh+ahKJOaaQLAso/eWmQQJltQ2aZLDTfF4fBh19G1sE2rVucISbKxTetQmdAl89x3tR0CMLXewNJpexJVTSTecLm1g5wy9+PvrMLg43IldtmR2mlqq/Swi1cMWij2Bbj7MZkpShxCaR0yrqd0DACe97P3AbAzRhfrdEmUMsWhOMKW1kjGNiW73C8iDMcRhkWMMZEwS7nxW1MJGYpEvuq8qYqXhRYlQeoT38bCVNllONIz3z1pQu1OBpn0oAkAR/04StkMFYHOYdfS0+zqeLGeolo77Kku26ZNLefOZTNZfKUFIE01nfOmm/PF1U9IAvUgxkC9gXoYoRGFaMZhATjXj8zGbZ85B9M2xKxKm3nJDfqgfj0FcbyDpifd/JG0nlvK63miMG6zjs2rroNr3s6Mu3vw1d3nZUXLwLMqgEYCGBPAk80RPN3aj2da49gSNTAcC4wLYEwQmqAMKBvp9ybDhiRQxtDjL7UV6ZVnzfxoY1EcQLbtiqvYNf985Y3Y99rzywtOYjkkQLPnx3cAUB9i4ynUXgA7e+QB0FGea8LVBkyQLKaZedm5X/fW8ajhS6ZkUyspWcBYSub4Uerd8MszsOurRzE2RePcNSayffzRww6Ugh1X6RjLGKVM50wPTDvf+/ILC9U79TLFzMcEPgmSmXagMEofkHSJyTJd2164hF+sJMbbPv6DttqbLHJIgCYALLu2CYqgfAjUouxBRwxdTZfiesk92KZ2XKLGa9mOB9PGPr0sChankCrcSyHnpqtlYhAacZiAJwh3XnE6Zj2YlykNDq8owrGNpDVu07PvUrbJlLMxT9++ucuZyAslAVIVE/xsqncZUPoKF2ZkE3NmkAxyV1lnwkSLLHTozau8xzTZ5JABzdov70pVQumpTT5BkxA0EwClBoHGCWhRAqoRb4P0SkvTnZ50y0vlY8v0CYC3tekrrndevkytOFlD8eHPn4LeIWXBDQYwC/4x2yojtvFUvISqarp6PCG2abStzRJSyvzDtosrbCdsz+PA8sXXfQBv+fCleNMnLsUfPvJWK6sE4AWUpppu+5htmiyzj1peQe6BcUVySqWUP/joocs2DxnQ1IR54El50SlCAqJNQjAW8G5OX1uo95gYtmhV2+1OnSps06dtl8SC8NDmhXjiSyurz9BRvOLORUOJtI+Xil7GNqssA1fCNvUylnOtjzxxzX8m89FN4KzyUhXA8qfvx1s+fCkW/CpppT4iEH99Hl5x3fvYYHQ1IF3bIsUChj7iU74T+yPteNehuXTcpJ0RZJMn/mVVOWOh4rEgACQgeow3gpRyjjSvWUKyLJT8CrOEsnPwYULmdEdAnyVkzi4KlLbUvFoQY+e+KahfMyu9JMEDRQnTM8v42DuDljuW81CYJZS1RcB1H/pUNjxzgYyCqi34vIca83D5ZW/TWK3tmv/v5f+q92EAJSftesI5W2adotTDHmZhRdwyccmMoFq2ToGcGZSk5XPcv3vS3LbGdiDkkJ8RZJOBTYHuEuaihpV8YTz42dPKedNLnDx5mmVwDrZphhmVhRBpXnDDHmkTl+1KzXtq22wdMDmpwjiVF5vzoKvpUd39Ah+MWUIuT7o13C09vuC6D3gOsCibo6l442V/hcsve1tiaooAipIflTJ7ssosOTbZLssskzFR08C0qpMoNGycg7fM7tjYnis55EBzwWfSzZosL6sIhPYBiWRhySBhiCRSW6cKkqpYnEhlcZuc7dMmBc+5Yds8kNx/w9oFmHV9EjhdFupTSVSGBjuAVhJPNb3yLCEbmy7pj4sBnfGA3/wQlWUOxXVc8om/wof+5l0Ix0Xq2DTBUrDA/t7H3uBklzaglCFDttAhrrytXTn33BxD3AacvGDWY5XrHGw55EATAKY9mQxbZ5GqUVP5GMwzE8koBCnHRj4nFZgnNye9LG5TlbJ56UkZvl2unQ2b5mDub5J71w5gegEgd68L7UyMbZbNEff1hPuEFrEqs3F84bV/CcC+G6Qqr7rnHfjTj/4F6iMiYZctgSBKjtWQJ1KvRelv/D/y/ZxMxw0XKmQDSRNEXaCag2WQfWQbnZCgr6+80CSS0qsmoqVEdAMRPUxEDxHRX6Tps4joZ0S0Lv0/M00nIvo3InqciO4norM6Pej5/2ZsDao9/cp/5liQyFdEkmKyS06/hMEQWbDKy7PvrYO5Vo3brOLw2bO/H2PfWoB5N9bBeYGrSCXmSJZPodEKY7GwTVfZQpC62Y5x7FKNOUAefKQcLt/+6Fvxxo9cioH/HgRFiSqe2HeRR4SY/Vp+PN758FucoUG+bLKKqMySa1uudGQzB4SIrYsvn7Sm0YERPnfic2dbAP5SCHESgFUA3k1EJwH4MIBfCCGOB/CL9BwAXg7g+PTzTgBf7PioAZzw77vzE9ebbIKlyUDVNkxbZjMAxgIgMvRM1vaZ/iuAoZttmucm+7SxTV8bZ+PK+Rj43nRr2Ew7oplDPD6mRL28CUSVjs0SYup72zaVMXJsUx3jK/7pg9p4pNXu5V/6IC758KVofW0+woZA2Ew+QUsoLFMUWWb2XxT6Dv8rtwPq4UY8WFYJNeLr5cxSth+XgCQg7ZfCGqIklx5c1rcTmz5ygbWdySaloCmE2CKEuDs9HgbwCIDFAF4F4Otpsa8DeHV6/CoAV4hEbgMwg4gWdnrg0UNrO9ug4hXPpBYDvXFiD02lku2yLF9YmKpnfU40e9dV87wD1AuX00H/Ae8comI/JWMsmyXk2w5bzqIO+7JNCOBDm1+Rnf721z6I1//1pZj+ZIygmYBjICdmxAIU5UDp/WOm5P3xZ97nnOJY1RHkC6Rmnaoi56CrzqCAYrzrzT+q3NbBkkqrHBHRMgBnArgdwHwhxJY0ayuA+enxYgAblWqb0rQtShqI6J1ImCj6oK9T6D0eUVFdLBOB5CWW/2EeU/GNEYBAuvmZpWzZJmtmGe5clTKWuX39bMy5Q3mJMkCoCMPyXlQV8/okHqgEM0gApCDKPcxWQFLvq1FG69bneUjrqqsr2eqZ6fIrhpaWt/PAf52CixedjJkPC8yIYwQRACHS/8hCu0ziJfGOG1PSpwCB8nMC+nfYA8zLwExbgchTfJhl0mb7m7A9/bELcNTHfl1e8CCLt+GDiKYC+A6A9wkh9qp5Ign2rPR6CSG+LIQ4RwhxTh29VapmsvKzWxS2YajMngzLW5R2yrDHB5vsiw7b8zgpTMq5Zk4GmPo0wTZvhMsW6Wuz9E3zGU6Jmp4JYyMstWjYGCbzdJuMNGgJ9A7FmPWgQNBK7ZWSXca5Cq46fLK2YvB9l4z3XZ96r8Yw+dCjoPBx5cl8vnwnGUpR/v/XXXdA2++UeIEmEdWRAOY3hRD/kyZvk2p3+n97mr4ZwFKl+pI0rePSenKDd1nnQh4ATzXKwMthp8zS1XzOoaTUte1caYu/NHeRxDVz7GomN6iqUgKOZbZMVZzxmhb1132eJHiBqvpDwgCq06tu9E0R0DMsUN+fg2WmisfQwJIi8OCojbeYqNk2lTH1DOdlbWDZjpj1fJxKcoM19VwV2xx0cxLHk5dP/llCPt5zAvBVAI8IIf63knUtgLenx28H8H0l/W2pF30VgCFFje+4HPe+27zZpgacVYBDbccEPsWBJLRjpazWlun0sXTJhA3ZwHX4rjkZYKqOBKtY8KpKXKWPw0cPCbP3y3eQVrOuiKKWkec8cLJlmTzncXou6wcNoL4vAa7auEA4LhLbpWSZKbukSDkX+kcbl7bgjMj7clwzRcBto8cannQeLKuEGNnEnDpZp0jbHM7cmTK3X+b15Bx01a6Z7EOVlPuz370ewSmTe8tfn7t1IYC3AriIiO5NP68A8EkALyGidQBenJ4DwHUA1gN4HMC/A/izzg9bl3m3t1nR9QJNRK3XXjy7Gq6f54BoBrvv28+bL/Y8OBv4zmxMf1JoL/REpR2PeFl7mbSh4R3IWUI+7ZrgVRtD4gVviAwQM2Yp2atAUe0uGadtXC5P+qr+J7JjF1iWiS+gSpDsoyaAfEuLTsxFl/KSq37TsbYOhJQ6goQQt8D+qBcmjKf2zXdPcFyVZPqVt2H7+emc9Nx6rjt10v8kqBin6RKzPSVNc+qk+Tanj0jrkWrhl1XTcjZnUF9f8oDGgpJIKRLY/dRMzF7rZiNSWHtmu04eWbdMPNoWRMnYbGNR77mrH81JlNxMl1NIzbM5m7IySnqyqpZUwXXVW7ZVBSwLW33EKGx57LqOz/zD5612xk7HaapStspRoqYHhbn4nWr/YMshOSOIkwVrKiIAyz4s6j1bT3UF+9tLyzZgU/OkBEGsNd26cTZm3x0Yjh5L3y6pwvpcjh6PtjvGNi3X29FZQkwdioFwXGeXeWC6yAHTtFuWiC3YXrW3amNLZc+xQUEtl9JurGYVR4/NXuq7grssa9o5JWONf+sM73aea5ncG6tVkKnX3A668DlkmzJLZZtZGoEgUuaZ5qd9ktmm2VWKLjlb1c/xk1noHS2qaW1LGwCWh8WUDEBl4SheclwnhI0OXISFbbrKlrJNFTgJqI0qc8NTdTlbrUlhmxMVK9tEOjYh8L8/+QUA+vRGKTbA9BW1rK/KHVDsxWwDihGJIJkZJOJsrnqQnoPy8b/8S7/CT06Z7j3u51IOG9AEgBUffQBr//HU5MQBdJmYoMrVy/4rbzybxqnx0IETRTVdpIXIVPeR12vdOwNTN8o3pwiYZfifqcEVpWyOuCtf609Rv7NbRkAcAkEFFd0ENzaO0qFiu+Iu1bbzHzZkM3fkVEfVWaPFmTpub9n9t4F28nSQlvby/3UTAB4IzfAjTqIScFOZotlGxgLTHUtVCVK0Vxf1MOtGIISWXSxV+eaT5+HZjTMw+N5accr0JJDDCjTjffvsNiAOIG3iA7hqcYVNFgLaLW2UBbPL82YzxPQMMNtUw1HODqvs31Olv6xPDjgBP9uq43uwA6Ww2zYLzBQZcCanyXbDYbPEbqkNxOM6PKXANuXtI+DFUx9i1fIyplcGlrZyHIiGECxwyvpVVjsKU/YJAN96+lw889ScJEMAQytb2YyZySSHFWgCQG1fuk86YFWBAUVF9wFTlVlC6OUts4RYNT6lOy5tUqTURwhgyi+nob+JImBWYJmFS/EBx07ip1CYVgeApeosIWu+45xjl1pM54H0UxjjUp1FzQFzqiQPTvqeQnwZE9jM7SnM+iZ4SuAEyh03KjDmabmK/q2N52HD+nl5Zva8E7a+/4J8OchJIocdaC776K14/F+N1d0Lqjb82SabV2zHm22a+QCkJ16q6QDQe+s0zd5XCRzbAacKQKne2/FZhJNf+wh2jg1g0cAQZtX3ISCB8biGp/bNxtaRaWj8ZC56d8tuRAqkSZdRDyEctzPgvFP7GL3UcPX3zsiT15TtO9VK2WQHAbOqiSRjm8pYP/uxz7FlbSyTA0wbC+TSVSA1wVMCZ5kEyG2XoaToAtjYnIUpQQOf/vVvAy07Mx0+NsICa+7BkcMONAGgPhSgORjrTiFGCmyTEy8m6sc2fbIbW6dg1n2B5lXVmvZ971S7qiufG5sjL+4Bhl40iucf83i6jUay5Nesnv2JoR8BAkToD5s4ZfozOG1QIPzjh/GNW7NeWrsAACAASURBVH4Lc28PkM6i7owqW8ouFQqviAaySNrIAFNZ27ITK0L5ihZ+xLDNHW8aBeAzr9yeX3WRYLW8BFAVPFXGabJNCZBaeRFgR2s6Pr3mZfk9Ne9t9kOVX8czl16ARZ+ePGzzkNsjyFc0tpnNPwP7X6j55jNHgqnHtJemEVOmsJdQmk/I85ojPZh9e01jODCaSgZbvNb27ZwlBQhoTCe0pgCnvPJRBBAaUPLnMeoU5fusI0ZAIpk9QjECCHzl5hdi3q2UXWs4nkw9tF2fOaZk7KSlcSFN+fdPxrnRZDrdUQNLyzhKmWbJ+F1ss3BNKdPcdDFh+YlbEJDAx5YnE+84j7lNLefAsopH3WSUEkAl61Tz/+Gp38G0+hie2DUHe3cNFLeXAYqAyYT6kTK7DgI47qpx0Jp7vcfcjvjuEXRYMk0AOOa741j/e73uECRfqcA2rSFIXHgRgNa+OmbfXld+YVHZZumyJHBlraLkjSwhrHzpumxucCxImyccpfp14ECJWFC2dW0sUYBi/MnzbsTXZlyAOdf1Zv16q6+ZCm4EhouSa1PLKN8jRbntUg0hsnYfTMym6bpOk21SDOw6sYajV1ZbusHl9Gln0Q0zDCkDYQH8YvhkPDs+Fb98cGX5F8D9+HtOL3v8kl4cv8Z/zAdSDlvQDG66B/i9kg3psxdQids0gdExI6hoK1XQyxqHqds+C4DZplSd2gjA+gOw+6WjOHHx1pTBxDlwQv765+pYsphIUi65BwGi1MgfkEgYkYgL+36/47Rb8N3rL9bsm9mYqt6HwncG3bbJ/pJZVPHnQHx/IPbNDzHz4nzZhlgQ7h07Gmf0bVDSdJZpAqbKMguLelSYNaTaMgFgzfAKXHn3eUBcZImacM+Y69I5dX2SyWEzI4iTY68eVTzOxpfLie2LZ8vY27Nui6EyyE19mHFDf3XAbAccZT3zow6TgJ0Xj2HO2zfghEXbtDxzT+0kLdmSlZNYJKt9y3rJ9q2Btm93JAKEb9kOQUBca++inHPShZGWTL/KGKUETLmQRsfE81J8ohh6X5t8D0K5/9duOd17KC7ArCqRCLIPAFx5VwqYUjPKPqR/YiqWk+LzS68Uab747AldQ6fksAZN+vV99kwDIMn1BVYCXNV2U2wzeKYPgzf1YfBxFB+isj6k+D7/FoBUhzeymDDtLZsx6y0bcdyiHRnQxYIQg4ztWmVa3qA8lyAalwxOBc9XL7kPx/zZWucY+YFXKAsdTLPpj3LuOFuhWvvt1i8DTvW7yNJAuGP0mNK2XU4fFfzU8mUfAPj80xfhPTe+hQFM07CMIkCan4ry1CvrlescCDmsQRMoWTqOkw6xTa78zF/1YXAdCgvRulim9dnimKPx4Z5TkZoWG9MJA2/cgvkvTOxlEsgE85Imq92YYJoDqqyr7h9jss1YkAaYst0zB5/GtvPbNC9AYZvcvTTucQ6WE1PHvbTaCsApMkdV8n90TojRP01itATyZQHlffzR1lMrjdcVt6kCYpn8xa/ehMfWL8wBMzYAUAXRWPkfK+dlt73kvm34+4O/3uZhD5pO8QHIrKzlV5T1CirLuwlC/6N9mHmLsbybDTAnwHKsP+QGmPa+fhum/05iKxMGGErhVHJOpIouy0aCNKDNyqmqovHivufinyKu5eDhdQ8c3xmZ5g5pAulgUHongRPIATPqI4y+aghjzRp27R3AyFivBpxIjz/3TB5tYrNnqnlcPutVV9Rw9fP+m96QbDCYgZ/x3AvmGIkGJz+siu6SwjMs0Bg8+EbOIwI0p60PvNhmYXV37vup8J0JAfQ+1Yu+nULp379+1o6LNXqApJRdpwnUXpcssO8LiqaabrLNMpFsU9ZVX9woXa8xAmHr89t/GWzxlBI8qcOAWUl8GWcN2PrWMQy9eRgAEMcBhABGx+rYO9pXUNV3j9v31ZJg2A5gcnLpTZfk7BKwEIX8WANKRdoGT0Oe/MeDyzYPW++5KvP/7dcY/leLJ116Xk0PLFuWdAqj1jHa6d3Yg4HNgL6isPG/k2IZ+9gswuiCGLNO3Im5ciaSUVyk1xWnbuxA5OF1apiRGXaUpKmec8D0pCfHAQIhkv+IsvCjQFnxYsWJm7Dv50uqXTP3nfl8j0DlGTpsGxMMPwKA7WfVEJ24DwDQ29MCAMQxodnM91IfH69B9Kf3X5BzO7SvbXseptXG8fuz77SW8QXMzz55MTZumMODJVB4nt1+AeRhXiJdqMbx3gkSeXvZs5VIa0CA6j0QzYOzX/oRAZoAMP2xAHtXtDFLqI0Xc8bdPQiawstmyQ/C6KusjFo8TY/6CMHLnkUfAX2WJiRrCVLANAFRK0e5WhIrD7OM05RxmxDMZm+FPhPAzGI3AVw871FcS0sgwM9TV8GpVDXuBHAa/Vvb8AFOsy0CdpxWQ+uUfQDGs+Txsbr1Ydk72ofB/jEt7QtbLsK7FtwIAPj69gvx6yePyUxDv1y3AnNnDeNvV/zQGejOgeUXnn4h1j85Xx9zVcB0aWlkAKcppJfV750ABOGJT5yNYz50K1P5wMthOyOIk47OEjLrkMC0h3tQ32dxTDhYZjvxmdyPugiB0ZcOo7eeMBYiASKBgJAF1wdZWr62p5y9I9ODdHaPeg4ANUr2cimm+c0SqgUJuqizg9T/P/jYRbqTxgKagAGc2Xdm3BTux84QrznvHlIGnCIERhaGoBiIQ8Kec8dR643s2zGr8b6K9PU3EMcBoihAHBPq9Qij+3sAAW2Ba3UW2ulHbcIfLFzDAuZw3Iddran4P7dflF+vqkIzY+Dsllqer1D6nnHAqYIxZy8VhIVrgKlX31ahQ7cc8TOCOFn52S149C8WVp8lVNBn9bpT19ZRHynG+1WdpdOWFz2VvccDfSv3oI6iGh0LIIA+K0mW4ZpV62tsNGOGsZEGqGq5bZaQbFdlm1Ku33Kycq3lqyJpy6el34M5S6gj0gHGObIwTMYq8jFTyDeaL/LCX8fYaE92HAtCqxlCxO5rvu/pJdgxdzpm10aytL9/5Hewe+fUHCABHTBtUrAbtwmYSv3SBcE55kkCWy4kHH919T4nKkcUaCZb/i50F8peQL/V3Wk0TAHTaMNawZ7vFXZjlBleDtRP3Av5Kmlm1rTBAlgqaQIKMCKBQwmGgYCVCalgyuepIIpsZlAyoOQ4pgSEv/nY2Wg2QyyQrP7gKz+6tAGcIgCaUwI0plH6PKk/jgLT7+zDvlX72Sm23Pcm0wWQqeBJomUsQllhC8A/3vlyTJk6jn17+/L6BQ+4AaBgylQVs45perIRFu6ec6r6QZAjwnuuyspPbyzea59fVvMXthFg5r01zHiEt+U4v88qZIiMD1JV7yhg/8uGUVu5N+9aecCLM3j0fK6MmR5Djw8sC3hXA9y1WE+LEfKu3UfhGw+dlzg9BDl/NLgmOGbnVLdZh0NnmakIknUv988N0ZxKhbUEsuejBUSOJdGAFCTVTzLgUsBU66vHGmCq47G9D7YvhLNjumyYZWkO0cxkUpTjPW977j3pRxxotjYpix/4GrcZu+Tg2tAPIG3C2UrNPCqeiwDYd/EI6MRc1SpsymY0Z748asyfWt6MBbQJF4eZ5WmAGmhxnGqw+7VPnoq1z+jrcj97RpBHhFUFsw68oE7x1AJGZ4doDBjPjczW7IACg79JXHTcfvaC+VQBzKwd+Z0WmCVjt6yglpd33GY5CzgW0tL/O8557mPJjij1XMpx77stdwrZbJsWG+eM+2vp0mHMU9GOWu56MGSzBOw6K0LPzDEtW1Xj5CLG2Tl0VVwIQgyhebZFGr5ies9dIUhAMQxJvb6ABCLTpgrCjU+vQKMRZi+uyXoBoLl0HLi7br1XnN2wsm2TabvyRnGKjE8Ps/5V23T2daim2eyLS8cZ5av4cz9i5o+6F2Da8kyV3ARMs34VlllVjHer4Ek3vqMs/Mjyfq77t/Nx/Htvb388FeWIY5qsOB4A+XDUhkLMeKBmd/b4/LLa2CWjgksZnUMYftF+1GeMZS+NMFQsE4DKzgGeSarB02a+ykB95qWrKvum/TMSwHQJCYhmUM104SMVGFIVdhv1BAlgmiDssm2LolZCG/udVQBA+402Ab9URSYeMNV8W11fOZCmxUzzMNR0QxOrLT/6AA5ClyOSaQLJPulbL7TY0dRfMwHMeLTmfhm4Jsihtpe8m4KA8ZmEsdP358+GpUHXBm0Z25yAJx1IADAAH8vpcghtG5uGddvmsqRcsl9Vwr4IgsKEhQHwXeWd3/a2GtuU4ordFESIa0Crj+caurNH/y0uOMTTZ2zaU8DeJYzTcSJMzqjPAqaTrVrMVgdSzMtVv6P0WGOcqcxethuz+vfjnO89idtOf24W9DhiQXPqNbcDF65S1lxUdClFBh9VbZcWlglY1D63vdP2XozOIzROHOXrCGibsOXHKcwo55z31eVJV9PDLDSIB0RAgqleDwTsGh/A49vnsAy3VMiBaOBVdL0AvO2Pfj9+KXOuJfsZmep//vxY+lVffNf41IfFBDzb2Gws07P+hIFZiue99GrH1i4BwbQmBqfvx4sWr8O8nuFsARgZoC8uON29slmH5IgFTQBYdJPAMy9gflXTL2twLaNSVnw4Kj2XlIQQRUtGtXcMQondo+ImbGXbAXPgyIIqctap5rtsm7ERt/nE7jkY3q8vTkJUfHk5QK3VW5j2ru0Y+cIS/T57vJQc29QuzGYW4WynCkONegkisHyPLqAkO9ssAO2WXmDhePsgVgaYZSyzionJV1zfWcV3AoHAO1b9ClPC8WytglgECClGLMJs/6HVX7zzOWGbRzRoTvnu7aidsxqtAZGxzZ4hQv92KjxIXvOUTfXLlmeIIGD3mS0EU1rFDBMYDfpqLkpeYJ+K2p3ZJRWHkARU6RDSulccRaaYLHTzyCD2jOiLSHAquJlHAI6atwsnDG7H9rGpCEhgOLVJZCp61e1/M9Cyb1aWD8SoGiQzdnJbGjIFRAU6G9s0y5ljyrpV6kx/gjC0QP8eAYYlCjWPuRgfwOyQaHPDAf778WT83KygDzzvJ6hTK424KC5AIiWAKF3DtdNyxDuCln00mb9KApj+eKADZip2G5ejYTI+Rj35GZ1L2LN6HNQfGaFBSnlLF2YoUR66UnypfOI2ZV+FhW8V54+63mYzCvHMyHQ8un0+hvb1W0Zpl2lTR3HCom0YqDfwzP7BrJ+9y0ucRhbR1PYqP3ZIwbJGEI6J86zTTxh53LHjXG1HGB8zPzss8XYXANPSjrcQf1yww1bFLjL+p/Kh5/8IvUEzO+f2Vbfttb7u62dVHER1OaKZppT6UIDafsb+6GXzctstZZmCELDn5BiY2irmqazEwjaFZr8sshRrc+lgOPXdxjZts4I2Dw26bWacpG0tnj2E3rBlLP6RjKW2H25nTburCzFsU6Q/aoXvyKJi65fCOJu0m23052CbU+/sx8i5vB1ba94XMEsbOjAM1EsjsADvjIV78Z4VN2rLCVaRAAJvOv0O3IH2fnT9++kKjr7s18mLCrT1EKnMkfsA0FjnnhMFdp/XgBhQAFP6AErYZpXwIpV9Ap1hm1uGp2HL3ul4evdM6wuq7USbvhREyWfejBEsnbMHtSAuzjZK1bBjL3ksGUemIufAWiYTWqrNMktAZYftsM1CeeY8bAiIKNVy1D6U87LwIn2mj7tsYdEZLk9L448FCZ5xqhqWRevK1HICemaO4T0rbmQGUxRzS2FA2U6YYmz89ile7bQrXdBMpXeIf2EK4vsDyDwoggi7z2lCTGNsl2afthdATRb6sQ1wzeaSfAYsLUApBGG0Wcf24anJCjvpArm+0tfbxPwZw1g4cy/qYVRa/r5bj9d/bNqViiq6FKvmwGGJZQFktY65LoENXKff2aeV05pRnxGVYSommQnLBO61dZ0G1vQqNMCszxjDX5/+4/Y7N+Q1xx1YD3pXPU9lxhW3Yvu7L0hOKqgXNlHVtrgX2HtSM39DRNoIKfukc2o4dNVbanfSgWLzpAPS0ZIP1jlLCLnDx3T+NKIQo4168q6WqEyq44cIqNUiTE/XgDQXBQF0U8DGPTMgbp4JAOgXaQPKghPmykcuFd0Zt8mo6OmtTFXl5Ia7QolK84CCaq/NEmLOIYCgZXecFX/1HCp5Gcv0Ec4uwZkd1B/uMjuV2k5af9mx2/D2pRNbF5NzBp17b4Q7zjgwanoXND3Ed5Vv074lQmDo1KYdZDmDqM0mlpbV3lOHJ70sDIk7z8AMCbCNNeoJ04QbMM3QpyAQmNLbKF3cGAC2DU9D6+ZZBbV3fDqhb4+iQlZ8+TPgtICkJmkZt/1SB0odEBlAdh1DP5e/mRBAuKUX0cJ8YWKtTnbuuKA2bIFmPGR+bLkhZnlujFwd5bh31ig+ctr1ANw7Z5oSUJyZcaQzyFxIWYYg7Xnrasz4RucXKu6q54osvOFZa566ayCXruWlv6R7T2BUUZctyrBHFvIdYtoyrXnQbZuJSp4fC0GI4iADzOySSGggm5kZU1tlLYzR19NEf28D/T255xMo2lIDEpjfP4xnfnwUWjfPUgbKXNdE1HNDSlXprFORlpfnrrJcP+XH3DmQzBBytl/4bh1jqyIWm6XVVmF+L6Y5Sv0E+nn/nP0ZYEqJsrCi9iEpNFSPiz+wpu22XNJlmopEDz8GvGiOk9k4p+al/8dnAKNLWro6blbzYUBmOR8V3mCbgH2WkDyXarqabhNTBc9Xhy/eMDOoPiCBOf0juOuR5di+bknR3if7EEBzGqF3SM83Gb+3F933XqMa29TzJuBJN45rm3vRWmSwTaDzgFmIvkd1xqkNiElP69amN/Dqlffh1CkbS4eV7WzaBoCaqvrOP16N2V/tLNssBU0i6gPwKwC9aflvCyEuI6LlAK4CMBvAXQDeKoRoEFEvgCsAnA1gJ4BLhBBPdXTUB1AW/mwbtrxkfnW1g4DxQWB0qenkQRH0MuNkWtG0baY6W5ltU2vTEBdAkqI2q+lqWVfIh8447XPW1S00ZvXvxwOPL8HORxdhqrwI8z7BTmz0ATD1uWIes4SsQGjaNmWbpp1SU9kFBMip6rO2TaP81KeBPYvKr4+/aAfqy37Ve1cFOIHytgGce8oTGIvqeMXcBzAlYMC/AxJCaMHunF0zIIGXvmcN7vpqZxVqH6Y5DuAiIcQIEdUB3EJE1wP4AIDPCCGuIqIvAfhjAF9M/+8WQhxHRG8A8E8ALunoqA+gRI89gamnzMHIwtSIXMZQCNhzUosvpwJaBpQMyEl7pQKMZbZNQAFGkS6tBQmASKcu5g3Ic+kQIuhTJtsR6/42Sr4QhD03z8feJhKwlNeDovpbaC51BmWXL6qzTdO26VzIwwWcMPNkewzoGsfqb6GVbUJva/A3fRg6T9lIrV2Wyf3AmIRAp/N2diwT6wLHHb0NI40e/MmyXAU21WMgsVcGjv0zVXumGZ8ZItbYpgmUar82u+aBkFLQFMnOa3LF23r6EQAuAvCmNP3rAD6GBDRflR4DwLcBfI6ISEyGHdw8Zcr/3I4R6UmXwrxnI0sFoqlyfwPkD6iPKu6pMrJsUySr4pCRr5Z3Ta/06T7fy0e5LIaRql75MB2HZJdDa+YjbBiGcwtgcjI6mzBlRwcfGxM4TeBDPjb2d8RS3iYsKzXayvKgt02RZyc+YmPmnDalMkpjvMGUFj5ytm6LVEWClAmeEhhd4CklIJGBZSwoOzYdQLbdAJJ+dNbZe9MCjL9ga2nfvuJl0ySiEIkKfhyAzwN4AsAeIYTURTcBWJweLwawEQCEEC0iGkKiwj9rtPlOAO8EgD7YN74/WDJtUwvDS/nbs2+xQGuaZxS1i21qahAZD6wAx16ss39EPkvIuVycEpIE8HuZSzEXGy6DL9nP8G/mIhwDsr3DyhilRQrExaY6erTjTTosbNNVttSTbtTJmCfXn1q0GYDqHVqZnANIM0/NV76koDfCR85JwFJnfvzYysBTlQxIyWhPACAdOE0JIYASAAWAE6dvxf19fYjHxpzlfMXrURJCREKIMwAsAXAegJUT7VgI8WUhxDlCiHPq6C2v8BxL//d/U0gTBIzOE2hN1R8G67YYWmW/ftkF4S2MQzD5Ewl4l5I5d4IYtTD5hOn2u7ZNwASAPdunYXzNHIRjDJPkALPknlAM+zRHtZk2tLCynUOt5y7gZ9JIOOpYzmW5mXfbOY1zvWTXrxIZn5L8E47ZwgKmPFc/pviox2Us1PxB52YDZWUhCkAt6x/1qw6xdlT0ngsh9hDRDQBWA5hBRLWUbS4BIDff2QxgKYBNRFQDMIjEIXTIyfSnWhhantwiEQAjy2OIwGSNKFezy2ybrPpewjalA0mtavGul52rbJOU/wEBYRCDSCAMCFGD35x3+NkBDDzegwGb6l2RYUppTiH07qlWxybWuE2v76/DbBOGum6yT6Vt0SJQrQ0ThY/jBvbxHXf0Nrxh0R3ZuY8nW5ZRGWOULuHWaTFVdJddM0SMGfVR1BYuQGvLxNV0H+/5XADNFDD7AbwEiXPnBgCvReJBfzuA76dVrk3Pb03zf3ko2TNV6fvhb7D506sgah7PXmpndL6EZS+o6RBinT5KGyIHTjhCkADVQy6yc9eQAtJZpbRTRoq3ffyRQfTspQwsk/sAnUH5siz9cpM63LtGQFvLxZl9u2ybDBDKa2DzmR+2wtJxUOowx2w5Anq21dFc3GjjIpVBV5DLVv8AgM4Sq4b+mCo8B2aqmGo7x1DDdDtoThXnbJycN/2kH23D/R1YBMmHaS4E8PXUrhkAuFoI8UMiehjAVUT0CQD3APhqWv6rAL5BRI8D2AXgDRMf5sETwc3EqsIyszoWtgmht8V61wFXCJKtH9OWaTJNAPr0SssLJuv21lvY36hj9NEZ6Bki9BgA6QOMZaI6TtIBIg3kSW5DlTaN78fHtmkHxuSGs0Bp1rWNwXUMPm9gI7BHegs6IDbC/EdnrcGSnl0A/ABT9XK7ZnxFCAq2T5vN01dsXnQf2fB3F+Doy37dVl0pNBlI4HSaJc6niw/2MFipLVmMRy9dCkDa1yR1Avtf2wDK/F5JMPWY9jIVuViPlPJktEEyX6lP7LEoniOPqwSAMFDVdIE9I/1oPT2A3p2BNmyrN9x4rNoF1GmbYkCIjMHqNkKRH5vvn9q+8j1keKCmGasoicJ3Ks+pJJ/LI72c69iSt28x0Fw6AbZpivJlvP2M27C8dzuAnPG5QNN3yTYOSG2OIxU8ub7VYHdzQWJ1JXe5/YVsR26FIbePlm3dfxb/8P1cfPsuIcQ5pddWVuBIF22fdCB/K0pAwio+5bgHU5JUzrFjljfCgrhj7lzf9zxJ2z00gD0j/QjvmVYOmFWF+2Fpp412xOt7SLvIrlfo5xPt1/VDohwPGI9gWyJ/TNPBz5u7Fx9f9f0MMMvEXCqwnfI21hqlgDfRuMqQYphW92zJOAWwn7x89YT66YKmh5xw+XoAfi+LtgWAq7wJeAz4tqMEuDzpejm9DtfVyGgveh6cgvCeaVlaRwFTPfZ8H4WlrPN9K7GTmnPSva+JYdnF+yPYdgvHrjwAPRtKIkxUUGQ/AAio9bbw/nN+gT8/9gZEyrPiwzLbERM8bZ52TmzMlBN2dff0ppqs95Uvndge6V3Q9JBo23bMU++zD9tk02z1yFqW235VCFKO8/I5IVL1xBxIZQiSbVsMgeQh37NhBsZunYPg7mn8y2wDlYlaekj/kLneSYFQO5CWsy3KrLiYZpazmx/K2SZ7n3yP1f6U4/7tgBgvrHensUc9T/8EocD8uUOYO2MEV2082z54RlwM0ww7soEixzrLwFPmq6AXZMzR/bDZ7KVBam6iM0921ndJFzQ9ZfqVt+kJHNClQi41xlHPKSywpv9EISmrk5Eoh2ougXRk03Q075iJ/i2h1/icjIyxGPh8bG248ias4kNhm1XFg52qbTvZptGm1kYkMPNexW9bBpSQtmuBvikNzJoxoq0p8JNdOWhUWZpNShnocfkc+PqwzjKzQBmA5uVyID35K4941eGkC5oVZNFNQv9h51TxMrZpCsc2jbSypePyRJstE9qxmtcYryG+exDRnTPQtyW0vvylLJOtxJwbL7YpGoh6PJ1OtlkorAzFxTZlmRK26epjQmzT1p7M09AWLFBKh16tJ0JfTzMDTPndrx+aY78GOOyPFdRrrrwNONUyVfswRbVr2lR0ABCr/197Zx5tyVHf98+v771vnTeLNDOaVdJIGiEkISQBsgQK0RHYwYTjLcTg+Nj8geMly7FNvICTOMeJkxNsTsBOQgx4OTixDTGLITg2YJZgHw0SkiUhDVpGyyDNIGk082Z789Z7u/JHV3VXV1f1ct+bee/J/T3nnttLdXV13+7v/f6WqnrlkPW3qI2JT9/teSkkIzrrU+glZKOO2vQRs7vPWi5Tm+mitdxf6qAe2ET34GT5i065iqpEGUm6JGr/B3WgO+tXUyHhsdzxGWqPt5mWNyd21svqdsqVqU1ffWrWyoHzEaW+jyNjfSY2LDA2aggz+/3rdIldaVQRpylTRZZVXSZ9ZnnIVL/2vx0srSuEdjzNpnAfciExg0XvE/vlkOQlT9+qkjp1PaE3byUT3nl4im5M0YfmXF9jpG2oX17deprNE3MMlLBv43SS3rQ4zraxGf76r69nfLrifCpRm4Kqd17rXtUZ3d3NvXTzNsvOsay8TSkeIwq2PNTh1K2eGUwlaU7UGRBFSRdYpZIOF7ES/Syq3Myj4IwydJ5GBUrrt3I20+lPlvWPnEedXkIuFt/0Gkb+8hulZQrnGap1f4dx1bu+nph2hnD0ssTaJx8LxJJs1x+JJdmeUxIlQSHfNheur1KVmPFK6J8YJzq4geihqSTAUpMwy57poKvJpyCdT38C9r/lEFdvPcaOyTPsnTpFTGJCbh5JSHT7veGTu/mQ6Xb7iW7i8O44AgAAIABJREFU60zvRUBt1nRbeNfdup1yQbXpWUfB+GPZBGxGXYqQsrRSQqwnv4vjxEBNRuSXnJn+ocOv915TSBXWUYHmE4LPzxlKZ7K3NzXX/R1+E7+mCQZ1iHnlf7y/Ub1J3S0ao3c6cnyb+e/kRRFyprsmTy9x+uAhsgIp+sw3x0zvnxqh88gkoy90CuZf2tZlIBjE8ew3n8VNiv13PlX6clX53Lwwt6XqqbZJqiyrpYwIIb3ZjVV6qJz7+wR+r8gjNMESqgqyTAlyASB32ubpfjLK6XJVpvtbVhFnKMJeRqJQTC3ypRqFEFK1h/6wWd/KljSHwKX/PuuGFerpUvB56n0F4rSOKfdxOkEhe1cgKBQdHmfs+a7TnsB5AucujVp7ypZFwwfjMHr7cfa/7rB3quBYSao2j9+9I3wit21Cs4BQsEJd3TJ7yZUFgRqrTc+fc++MSp8rN2XMRpZaln0b/6a53588ciNPL2xreok5hAiyyv+4EnmgBsOmIAG85dqHGp2rJc0hYXrHpPC8IIX9+iPGfAe/aqwVKMqXsXM3o2fG6D0+Tmc+KbPcAI+XOK2PirKPzxQ/d/Ui0W0nGbnxJHEcsTRIghmx7v7mw6YnTNfJhgRWl+Qbqk13fUXzNsvO59YBqG74D8K4Y2yVafLr7eh5RqDCXz33svT4pqZ5FTFeSOI0sAnS10vIh6f/pH4kvSXNIbHnP91V6RMMpiYpHSRyidNTR04ZlqQgAXSOjjHyREaWZW1rClc9htSk2TZ7zQILN8zCLaeZ3DKX63V0fHYiMxc1cbpqs7NYr8EhtVnL2ixR3FXjbVbVWfe+1+ol5NRx6vp+1ulBlzPrDpfnSZJsm1GbBo/PVyt7F1WEaJerY66XpTmtJOx8TePjfNP+b9U+vo2eLwP7/myep39gLI1sQvEdyQKiUtgrSs/tE0Eafk+LiSMvKEbYz3UZORGBEq/53UjhFJsXhM8bsLhFMXrVmfRlHTdlVRZkVkoYAMQRZxbG2Dw2l/qZYhUR6S5Az/z21dUBEac9JpNB4Ymk17i2YSZhcyPp3mi51f7sJ/YMSUdx2T63wZkrgV6sCVJfrakT8nNB6RPGcYSYOcKBLGnJzEQKT85s5eqxbKzJJv3M68Kdr9yHMoK0idcesMNetxGabM0e0WuY62yV5jIQ/c0DyYJPLXgJq6goC4qzzEy3FObIMyN+wlwOjEldts/5LFykmHjdccb2nw5W6ybUx0qYW+qmy3H6AgjPfnB//kDqqTyf4hxabZqqLlAvoVAd9vFLk8L0q/r0t/SzP0br+ciyKLSitMxzyP8Gxky3I+lHZzZdMKVXV6GaslVKtS5WajDkljSXias+puecC5hZZl8oMFQw1a1j0vLWevfFHqNHevl9nuMaEY0LH0E6mN3bZ+G6Oba8PBmUv5tOh1FyPvOykqTCuP/y9x+6DBkMH9G3iXOYrpXno5dQPuXIU8BjBbjl4i6cuaaflbGDP87xqZlu+U7cSLo5xI2k/99jr/Bew/lAFRGW7R92LM2VQmueLxPq3ofhbbdldpX9ezrrOfOtsE9IxjZ1zHRdpjvdI1oo+kkbq0zHVLWtfXtbCIvXzbJxwxxjzkFRFMPAN2KzH270HIHt/69n5UnWV5k20v+YjhD1VTbFb5mJ7v5uFupMXZEvnzfhy85XGN3dd5zAyVcOsosr+IHMn3BqrKebjOcAY8JLft57N+H9hdkk/eh8mOYhNFGQLlnWHcUd/Kb6sGhJc6VgHuiA/yvvv7KdVxr2S20ftySMTHe9L3wo3WmYpufWe7C4MUZ1FUSgIsXUrrMIMCrZNL72SPBmegw3zcXtkZQqG1GcWRhj4+g8kSgeObqD7fN5wlw29H2smhs9d8hq9xKycPLlEE/E+f02WRqCdE5r32sxDcTce/0b6d8qVoqIxK/ZxOxcCXO5LuooS3c09/M573lrnq8ArvzFA8mCY0qnCfD2B7PN8W8aM12BLEWMHO8ycqzLyHQnf5w+jy9RPQjXXPWY3/0NiviVZ1E3nkWuP8PYZWcZ3z3DxK4ZNuycyfnDQuh13LHcMpM8Wc5vO7cwwiCOeOqv9rHtM2PkCtnXm6uw4lotxB0pXncIJeda6V5C3rqtcnEXTtwUE4/HhfK5DhOe7WnqmTbDM7M8u++x0zPImOmxEv7gcL0BepsklS8HZYRZtw2hGSrT/TqCXhctaa4QNh3SC27wBsIEVwj8QOdch97pqPBSeNOXXNg8LJ6iHl/f3L5F5q+eJ7pyJjvWIjqzXmi69cKl1YvKqU8fbBFpCHjzobheTmZDAZpLPmjwpNd5F4PEGNhffmxmdcRdmL4hzvZbf6ZF0rYIUmXryl7H7HcDQ9m3+1sfmruk+gZw/okzRJi2v9Nugy/BvSpHc5i+7y1prhC2fvhAtuIhzty6Kr5k0VxE90xEZzFfDmr49UrUVIg85y5bon/dOXqTi3RH+v5eRfh5zFWbNnGWTc5m16mUcGZ6krMf3uO9vuW6HiQGiTMyqvV+D6M2C+X9BSp9z/p3Onm9z37XxJmewyJR64/XDQqZr5RE0yZmf4K+7pV3HdsXuroCIonTj7se+gwLNzi03Kj6sIOFtKS5gtjiG9e0xEQTBdFCRPdsRLQ0fNQ4qSywbE4rsLRB0Z9QLL58lu7kUrbDLud5ucyyOy2GrVIMcXai8Eth13fu5Dg7P9/1Rp+XQ5gSQzRQlE66VoY6xwTa1yS31H0elibg+M0qU8Q5YgycN9eGTFXmSNT6zQq9hDxq05js35rdVXIyP+oS4nLJM4QLFVVvSXMFcdHvHwj3AsJ5URR05oRoybPP/g7AKww9fsv+hGJx/xyLV8+h9swTXzqf1ZEz3+yXyH2hnHOTV5s2GbozXbpYmB9h059OsfMLyyNM4y8WlRFlNFCZuhyGMENw1WaorWl5zw0joDYF5rdEnLo20FCLOMUixAKpOma68W2m63rUozxxZr2wXN/mPccuq7gpy8dy1ecAyX3ASnbX32UR82GzBFrSXGFc/cGjTk5mMTgULYq/q2MdWIXL/Jbzu5eY37VEvNsmyaxY+H33k6E7vxAU+zIblKnNi744lndPDEmYKZHEKmg6L5cwV3QEJCisx13h2C2KM/uLAbSkDp/JkJ3L9mnmzHXsMvn68sG4rEjs+W0NmkxwNix8xFk2CEcTVVkWRR+GONuUoxVG//AzwO78Rv3DpInbITVZlzydvBWzuLBzCSKF9PT/q0k78SViWmkovoGK3e6Pdh0mZSX0uHWimEGc5O2YF3HiL6YYmUkuviohPNkXvnSv8qtDmHXvr6Ik3SjLrWwET53Tr6jZIEeZ+vfbvylOd0p9nMp3r7SJ02w3KUgiik8/dxM/uLP5eJMGVWTr9jQK5VhWwT2mKtXIN8lbE/JsleZ5wP7ffKJgPhYIU2Ponju68v6GmP6EYmHvIvRipGsRak5l6C8Pz9i9R9J1z7Jv3bdNJBk53GD+O5OaMP3Xm6ZmlcAtU0aYK4kV7SVkVgVeuE0Rjzb/8QuRdNcNlCpPKwXJLlcICuWbljPTG6i5DnHhU/cYG01N9TLCXKlkdhet0jwPGLz4Iqirsg3KIxCq3hfJl3G5ajChGGzuZ2VLUKY2XQFUpjZB5dclP/iBfZ6OKM6+OMnOr0ZFMjGVBxucX3VTtqoIc8X8mFVqs6RMvnyOs3jhtSpHoNUReatQUGnm9+US3nU7la5LMO6WrCyQJrgnPYagH1tTRyzDRPdFqV3z31adVYozZJqXmuErSKCt0jxP2PH1OPO7mY2elyNHhtofWVi3PotbByzu6DPY1Pea+aHR3ZWlUEJDy7l9lbM666tNpYSz58YY/5PN7PxKNWGW+THdzgE5/6VTNj0m9G5XKlnfj+PUGVK61FObg1Hhhp9/sPg7FyrxrFq/n1dtqnwZV236ho7zje4eq2SqDIDFmt1izfQRoY+BrSzd/cOQsh30sQM/5uMip0KHDAJBqzTPGyY+fTd8123D/b/lpF/ytbQlRvXsHiLah5X6q/SB2oflTsJWhlSBeMqGuugly3m1qZQw+9AWtj6scMkvV4Fvu3WtuX3n0fTOzqXS70p/ZUGa4yfAwkngvf/uQ+zqnuWL3NCgbc5/Wyo4zWBw1vnNM6D/qZMh4/S6eVSMr9OqzFgRkEzEZrZ1Sp4dn3qsS3ymnFGXZrg2V3HWgc8cdxWnTaCxkvQcZjn5s6h/3pY0zyN2/c2A526v/rc2L4XvGY170N84yIjR/g5VZhMnWflQ0Md3fFLGLq+DWSawQJ5kzx3cwsUPKcat/MjCNdUwyV1TPKvHObYJmTYoGyLO0vE202MtS1qydSXwkf/8fk7Fo3z53NUoUflk9co2mXpL/gTdfdafKFi/pUqIUciTJZD+nrEmzNu2P12ZAJ4b0LemX8SQmkueZSibg8gmxZCJvpL90FvSPI8Y+9w9cHugL29OHehFo+A6MJiIw2acfYCtNsvKVNVBmFRDkXSzPPHxTUx60n7KCNNHjrUJM4Dz0qvPd29TUiz3bSoBRPjhX/1L3vf89zAwisb+Xd1Ls6Sl96ez74tRm4W67N9Uq00pqk0rrK63Zb/p9due48qxY8Hb0iEu9ukuUZr2VMHLGUxj4BCkqy7LzHKfyhwGrU/zPGP3V62pA71+LGu7wGBMMXAHagiQSbIuxXKWb9I9LvVtOvtz1daIpJtjJz++KdjbqVDY3We1OUSYXjQ12assbkdZhlRto15CQH8s4pp3PcyDZ/dmhAn0Ni9UNLgh7N/dvbepP9P6/YxvU+UT3c1n58Yz3L75icJpjE9yRPp67p3s05MBHVG5jw1TzoV/VsrhTPQmhJmdS0+30sCR1irN84yRv/wG3de8lv6k439CP9NdlZ+QzN6Ju81Slu4++zjXlPMp0tRkz5SIeZFSMy5VIJkaAeh+bRMTx/Sgww0Is4m6zI5pypArg0r/pqs2cwcDP/Ei125+kRjJESbADXuOct+pK5IJ9uz6spPn/rhym+xIOjV8m2aH0jvSx0Asb48ijoUogpHuEj/k5GYaVWlIryOqVg6mIc5BhSsiNLd5aEqL2Ar8DBzCK/gzrbqzfvZ5X6b5jeqiVZoXAHv/w13ZH7/5SGKGe4kylJvnwi3vKeOLpitf+QpFCdA7MMWmT25YHmGG1OUqo27Cuk9tuj2STrxtlv2bXwT8hDESDaBT4TLxndtVlK4VYu8LVW1bGul6Zpm8Yc/jaVmjKnvSpycDRvSnJ/3KwTjKSDUlvGXSjx0lNxF039zpIcIcFq3SvEAYOSMsbsoUQeLvqsEarr/MFzUvKMv8Nl/QJ+i/tE+nj188OcaOr0WgHLI0B2jUIswGWC2VaZ9feW8M/m0Cs+84xe2XHKGvopQwY1+qSyyNA0JJm4qcmk7Qp9uA468Ey6KwI+uWO9asXz52HMjUZU+TZLIt/HsYFViWYxl7TfFm129UZoywpDo58ztri19tun5MW2U2cQm0SvMCYc9/uitZCBFmmZJsgjKC8j2gJWpz6cwIF392QhNmsj3E8z7CzPXiUSXlQ+29wFiO2gRQEbxmxzO1iOC6a57NXDKuaybXpkAFttr07g9XZA9UnK4D46NLiW+SmJ70mYrmmYwWUoXZkbjwMTCEWjnP+ZAqc4BUEqY7DbCrOm2fZlM/po1WaV5IBH6jnHoIKcecPzKgNn0YIgVJAVv/fCycsE3J9sBAFe71XlA0OJ8SIT+aeoXa1MdM/IujXD51orZimev39B+oR2365GTZblXDt2n7rxNvNSg9q5BOQfqZq78GJNHtzZ3Zwnl9SjEUCTcElartCpLMCM3jv7QIc0l1tDr0R8GLY73m/aUuYRqV2UTx1iZNEekA9wJHlVJvEZF9wMeAi4H7gB9TSi2KyCjwh8CrgBPA25RSh2u36CWMKz51jqd+aDJZ8eWTpMGF0ncmf3zOCqtjujtl9HknHhpn8jtJL6a6AZsmAR/vvkCZFUVV3dZ9VhGIZ8AhH3FKDKqTEOYrfuVBr++yIyoYYNg2PsNTvRgWIue3qnFNmJ8wf0COOAuBoOQEZvo17KCQwLtu+BKxEnoyYFv3TKocBym5RGkgyCbAgUNyNkLX7kaxfYRpyNJsX1KdgroMRcPLzuUGfpoSJjQzz38WsIfZfS/wfqXUVcBJ4J16+zuBk3r7+3W5FgBf/2ZxW9VLUkJA/vLhB8A7Cvu5Hts+P8rk0SEJ0+7E7JZdB1CdZIi2uCuoqP7LoyI4fn2XV/zKg+m20AtoAiTpi6vJoDPez0h72LhE2Qtv7zPWipLCc/DDL//b3HqP7J+jzI/pIg6Y3nGqOiMvYZoR2M24mLa6XIq7KVmaQM+S6ljRb3/gx3yyfVHBJA8lyFehFmmKyB7gHwK/q9cFuBP4hC7yUeAH9PL363X0/jfo8i2AKz9+JluxHmq/Aqu4bSVR84Jv06lLzXXZ/oURLvmbUP/wmtssrKafctih4FQkOZ+iqml7DUaEO74/IRtXUbkvYOjljJeS7cp3Q2v8+xQyEpQn0yLk91TZ+Jm7R0+mJDOvehxa3MGiqu7JVpagXmaW+1KMDEn6zHHzyacKSaEu26eZL5PP47QDP7GSxilHdc3zDwC/BEzp9YuBU0opk7l9hGwQyd3AswBKqb6InNblj9du1UsY6r6DyNtuq+TDWiZ6oXLy5rjHB6cUbP/SaCFNplxN5tuVq8y3fYXg+hezk1FNiEFiV2ndvnqViL59qnCcOWbLL3ybXROnAbwRcgPfi2jPv33D5Ud58FuXVVxIOYLPif0soLLv5Cjj1eRf3vQVb/u/vbgNgJ702dU7mW73kWCul47tX7QUptmXN6uL5rgd7El9mB5TPJTbmTfVi0nvg9z+ZsPfGVQqTRF5C3BMKXVf49rL6/1JEblXRO5dYoV7SKxxXPW/TlYXCqGg5krUpnPM+JOjbP/ySJgwVWAbzjbw2/rDotTCrH6oa89p7gR4miAlzF8ME6b9EtovZy8a0IsGjHb6jHb6THYW+ekdX+Wbh53BqiEf7As2BkdNSlFt+i8id+wl207TQeXIy3yMupuNR3liYQdn4/FcVQOH8GLL9Lav3ZjkNmHaitJWksYUX1Id5uMeC3E3py77caYyfaoy57f0qMuQH7NpylEdpfk64PtE5M3AGLAR+C1gs4h0tdrcAxzV5Y8Ce4EjItIFNpEEhHJQSn0Y+DDARrlonXnClof4m48y9fRrObvPIq9cxJMCkaS5eOKUByfQQ6Yw+sLY8102PqV0HdbUsIQDNuIx6Wr3I6+AL/6VVFI8Z3ZM/mbkyG8ZI7SrbpGbTl7doX/jDJNfmWTqyCA91zM/GHPndY8SOcEd92UzStL0hpnqznPtxHfYP/o8HRSRZMOjdXoxaSfbwu8fkJDub5YaFc4NzMWDdCDIqu4VVxzlu7c+Eux1k9umIr69sLXYFo2JzgKxiogkZiqazyWPG5L0BXdsRZl+F1KDiirWpyZthJWl/8+tabfNStJUSr0HeA+AiNwB/IJS6kdF5E+Bt5JE0N8BfEYf8lm9fkDv/7JSq5ylvAax/YN3cfY3A4N5DAtDmH1h292dhFCUfwzKWgnqFxoucQSgRHcCbNhWJYLqJEQ7GBHinnDs1cmcTT/4vQdyo+6cu2KUP3/gBrbe1eXlP3mQS9EvowJjoNm9YGIV0Y0GbOzO05MBt2x4isgkiFuBlXTADqB/ZqTZBXgvCq9SLya8p3tAVEqYNmEU1gN9ul1f5un+RLp8kiQ7xMw3HjJ/fTmW9rrP/C4jy2IbAwEfFSLUlfdp+vDLwMdE5NeB+4Hf09t/D/ifIvIEMA28fRnneElj0yE4fXXDgwIviVElF9/TobOUla1LmBcyr9I8n8F6azy/SgQZNGiYQNwT5i6KuOWn7ufU4jhbR2eIdD/qWAlI9uJOL05w27VPwLWkKsqGWR+N+vRVh1umnqQnfUZkQKRzF1NCRQfbJE6j0QfO7S9ta9M/hILadO+hJfG/65qncgRZNZNj2fiUBnUH+A2TYMAvOiRRuuVz9XhSjs7bgB1Kqa8CX9XLTwG3eMrMA/+4Sb1/V7H1wwc4/b5b0/VGwR+LPHvTHTrzwsbDeV9loTzlOZSV26GZL7Pi5a8kTyruR0eSKXt954UkjzISVATPff8id+w/lJrXm0fm9EsVg0REDIiV8MLCFLP9kXQcySgdICWio5M4I4nZMXqG7b0zdIgT4hVrbMiUILPlCE0semDO3zlwR8mFNYTvj1Q5eZuWW+emjc8WTGUoJ8vQoBg+gvSPXBRWdcOSZNKWMFF663TcD+1slC9xiNIja+uHf+KZLpNHjb/SDe6UKMwyf2XhnO4GWVHihCGyBKzj4l5CnCYpPe4mlZ24rsPiy+e4dd/TAFwhMTGS+BxTsgT0cozw5EwSMU76T0uOOC8dnyYSxabubOqbjFWUkaUxu1VCwigYaB90h5iYJDl8oCJ+6p4fhTgflKm6zvR3KLmfObWZ/iHlx9z8e9c/Rk8G3hGDgNS3aK+n12chFL2297nLvvJN/JNpHZ4HpjhIsUOmToDOJswYaUSeLWmuMl72/md47OcvDRew/XySvAQXPRARLSYb/aa2S6DFMmn1DYMyPqyEuT0MZrcl92HHjx1O5pvRZBaJYqeodBqFBJooibS/LVk/tTTO9MJEbr4a81J0oz57x08y3lliLFrKCNcM4y4xqG7SU8YO0FnEqWtiQExHhA8dvYPBmREnU0H897uJ6eEGhdK5ebMil+w9yY1TR3KpPVAkxsJ6bWXoL7dSxBg6PnT+soDQsIQJLWmuOvpHvwNcGvZVWtjyzYjOAhW+yRLCdFDqqywjzoDarGNu+8pXlosyM/z4W+e4dOtJIhQb9ImMKkwCNYYcs+uInAsxAZ2z/XFOzk/obQlMPRGKXeOnGY363mkcBkhOMxmfaBpMMj5Sfeqj/S2872vfqy+IPGGuFOxnyA7uIWzYcYZ/dOn9hWBPXcIsGznIIFymvOdNHeXoQ4js6kTPTTnfdVShJc01gKvf+ySP//KVxR2aFDYfjOjO6W0NCLOAIQI64RShsJle+vzZc2i42yzM7IqY3amILj9HpxMjoti+cYZdGIKCSJGb18aezyYlUl02i3rHdERxYmGSs0ujqUJN5vlOEIni0slpRqMkISjpd62wJwpKh0BLVSeg9MRgkk39MCDiPV95K7l8WpcwA24T957W/TPyCdQfu+oefV/yCrOOuqzq413HJxkiMrecr54qhCP0K0+Y0JLmmsDgxReBPGkqgdHpDlOHrTelxDfpI8xaQR9KFGKVme5OU1m2v2TfYARmdkecu6zP6Na5dHvXOdw7fzt5wjRq0/VLZuXgxfkNnFuy0n0cNbpz/DQ9Z+SOVMXaM6zlzPQoq0crvj86cguPP7GznDCHQdnvkroIskL7rnlOX38zwvSR4UqRZZUP0nd8HdSJoLuzU5ad34eWNNcIrv71R3j8374cgIsekny6kI8ImxBmTYTUTC2z2yHIcrUJ/THh7GVC98ZT9PsdlIJR97wKa7oNOLc4wobRpPeYqzbNQ5+SZ8BMn16YZGZxVDdZFeoa6fQZ1zlbaRqSiuno6zERcLdn9kCTakzMp55/FY8d2pUnS+s7R5hVKnOY/rS2mS7wph0HvT5Me7kqb9JdrvJX1jWRfeV9aNI33G2LfT5zHrf/ejuF7zrE4NRprvjEHCevmWgwcEZRhRbKlsD3HBaI0/N+V8Kp9+zeCHXbaURUSlQCGK0XRTGDQfLQ2lNt2KpSKWF+qcvkyGISzrGUZaRy9JiePyL/ch6Z2ZwuJ2Z5dlxXYi6fOkFP4hxZJvVFREqb55Ao2fRHiBmoDh0U//X+v08863QzqmF6p21277tZ18QZNNE9v5fh2htveColTJ//sglZNlGWVcnlVTmWvmObokpZ+nyuddCS5hqCHHgQrrF6CQ1BmOHK65ctNddr4viNQn8qZuyScwlZkhGia2J3OhlppqfSZezBkfv9TN+5ytJG5s9MzPRjs1MsxVFaXuzj9bn2TJ5KjnUCPPnzZSlGWdAJPvLN2xmc62rzewjCTC+6QdkQcsEg4ebNzw5NmFWmeJWyLEsut8tlZcrJa5icSltZ2vW419DORrmOcfFDZ5m+fqpWutBQaPBiNn1GZ/Ykfr2lV5+lI+RMWHeeIvvdhtK4Uu7Y6dlxLpqY85rWuQi4Js4jZzenxJoSsT63KXPJxNnc+eykd1ttJsuJeR6rDn988DX0Z3pFsnTh3PNh5gYaBrff8q2cr9ZLng0Icxh1GUosH8a/6YNLdr7um+41LYcwoSXNNQd178NwXaI2V5wwDXzE6T43Nc8Xd4Vzu4XFV54DSab5TVRl3h+ZKcei2hS9rkpeFlOfbcbbZnquTVpJvDCbjGSYkqlejkSlxHnp1HQ6uIYhS1+dAC8uTvHMzBYef2pnLqhT698l9GfVUF3WjaJf9YojXLfhudxgGAZlhNkk0NPEb1mXWN3yTeEjS3s5RJhtIOglgPKE9PK3xmdeF7ZVPSM1nqHn39inO9bPFU+aJpoEiwH2LOMoefvN7m53wOJi1ypbjJQrBXEcsTDoMNqxB8FIFKMh0oGKmJ7NhjKLdHtiJTni3O4ozLQ+K9/z3GCce5/fy6njGzJFaRNmEyzH/HYDQjmfp/428cCO4s7tjxUIs8ncOsOY43YZnz+xrtluYxiTPFTHShAmtKS5JrHlowc49eMNRkCyf3MPUdoY9hk8c0XE3L5FopGErESb3zkz25JB+UnbimrTNZV98KlSpWBmfpRZUWyZmEsJMEaYnh0njrMX0gSeYpUQZ7IxqWvn5Jlk/nFMIrpuDBFfOHhNUjY2DE9Glk2wXB+le7wbEHLIMp6IeeNNB7lq4lhtwnQJpYosTV1QP9hTRpYhZVi2rSnKVLLZp95ec1BWWtJcsxiZiVncMETksOwZa/h6uSVNAAAQAklEQVQCT18vRIvC4uXzid/Q6vdeqFoFZrYMqE2bEE2zOp04F+zx1Q0QxwJR4t90/y/MemSZ+ylx6p07J8+kRGu61N1/bCfnTo8nStlVk1UvrmnbsC+4qxpDUfQObNh9mjv3HuLY/BQz/VH+wbaDnOxPMhEtekc+B3/Qx14uI6th04hChOkrU9YVMwRfuZCrxr0OG+odXfrffrbWOQ1a0lyjmPjU3SwG1GZwGogqVDyPSoSFzcLpmxcyonPSfkJT/lapTXvdJdK0eRL2bbrKNZnPzZPs7ukhZJfbNDoPwHdmNjG72OPUyUlyk435FKUxjUtM4iDqqs1A3Tff+CQn5if52cu/xEAlc/gsqQ6LE109FURUSZgGValD9uDByb7mSep22RBh1kkuHypSHjjGdx39fzrJ4PEnG58DWtJc0xg9PWBhk3+CK980EEMRqa7r2B16EE4hIRGSoE5oyl+EbB5tr0meV5uZn9NPpFBMRSq0U2V1mHGtk22kx6dlIPVhZpF2mFka4dDR7X4V6b50uev1EGddeI6xJ1MzkfSt+6aZGl1g2/gMP73jqwB6Sogui3oGxmTaBjPtgzODoxUdB5u8ogJhlgV9kv3NCLPKJPcllxeO9Sre4fM0c/Xo8z56YB/73n1gWXW1pLmGMf6Ze1ho4NusM5+OjeOvUsSTg2A41qcIQ10ZDacqlcyqHfJt2qRqiNRARBVM9JAqzavRTKK5xGyIE+DszDgn+1E4RShEjGXrTUnUEPz4ALUU8euv/xTT/Q1cMfpCMuQccW6AkA6KATEmgSsmSqewzc3P46QT+YZ086UO+bYvlzDdMiEVGTLLQ0RZN6Jup1mZY7517+Vc+a++zj6er1VHGVrSXOO4+K7nOfHaHdmGUOSk4sU9e6mgujC/y1KU6bH6zTcEYCtMrGWP2rT3u03L+y9VLnIeUptRpIgilfgtnXrsdfcmGLPf3j43N5LUYy4jF32u8ENWkWEDsnzjTQeJJGY06vPKyWfpSZ+OKD2Accyu3slsgGLPKO8diRlhoBVnlwGRNa1t2CSvCvzUjZA3zan0TSPhKsxQRNsu66K2yW4VO/dru+l++T6uXAGyNGhJc41j8MTTYJNm6LnxqTGBc7uE2cv6+Y0hMzglzKy+lLB8vkrDoy7JWucpEl7Iz+ma6fkLcgNIhrHcoFK/n7x8/aVuLlm+LAe0NkL9wNN7luy/5NJpYiW8/bL79ERq2YRqAyI9ors15qYzyrshTjNSfKY2E+IwZFmXMA2a+A/LIt++45qUr5NkvpyoeayEmffuYfQvvkF3BcnSoCXNlyDiLpy4OUZFqiSfhwJBFst4CLasvN5NSqr54wt5mpb6NBBJlOZgAK6LNh1X1yFPkSSi3u9n/b6zwE64raUIHWcRZzTeZ2xikaXFLj9y7b0A9GRARDL9hRnEo+OMmGRGee9YE65lI79nZDlAcmrT+DdTX6Ymm2FGKyrr5VNFgAXSrVCG5b1zynMmm5LnzAf2MP6ZexjluUbHNUFLmusAW7/yLC/eubey3GAMTl6roKtygYYqokvKeExws6vCt+lTo+7xtv+yzDw3yEXRHbM6T6ZJ5HswiMKmdhME1GRnok+8FHHxxTPcuO0oc4Meu8eT/uodkmksOumEaoPcxGoGA60wsxGTJFWbNrEmU1FkAxon51DMxqPanylFsqkgzKocyCrCDEamPT5Pn6ntV7xhwqyTs2mnGD3+3Hau/CcPMM53vO1cSbSkuQ7Qf/YIyVTyeZzeD9GSsLBVv3A5bpE8cdpwCbLgjDSVQCPfpl1FDbXprtcyoV0lEkuqbu06lw2BzuiAq3YeY77f49otz+em09jYnculNQEpYebmIpI4Say3SDSbfC1Tm0aV2mozG4YuOce5eNSbWtR0tPVQpDyprzlZ1oUdsQ8RZpOczVgJh49fxGU//BBXXgCyNGhJc51gy0cP8PzPvZaZy5MXTwkWmdHcDPcSoOuXzPaV+jad/UqyUY1CeZpFls58m3EsXpXpzd8MXHb1zjxEYHLTHK/Y/hxd3VOoI2beIWX1GsoG8zAzWJrBPHJTbliDFcco8gMVD7RvM85Pi2GVWVQwkv5uMY8u7GIh7nkJs85o6/6Uo8irLEPpQ+ae+OAjW9/YlbkyFEdPT3voVBDmiY/t5eKPHOAyjpaWOx9oSXMdoXfOdfIFSE5/F9RmLXL1mNhVvk29XJbwbvI+fSlIxr9pXhQ7cp5WUdP0DqnMnLkv0OnGxLHwst3PM9FdzE2sZs9zbuYTijynd5VmrCQ1vW24vk1znG8Gy9z0GVqFLqkuC3GS1O5TmKbO7HzhSLmt9GxSdMkuHNBJ7ksnp7BDZYvKsSlhuor2+Cf3sv2Dd3ExRwrnulBoSXMd4eKPHODkB26tLugiRJYlfsxakXQFiiySXmqmF7a7AR1yPX3qmNhNrPBON2b7lrNsHJ3PEWRiRtt+DVNxoijdEZBstWkI0qhN49t0p8HIzHLbBLeJMh8UMjC+zUcXdqaBn+LgvZFzTH3CrCJLnw/R/KGUjaReFcxxyT52ngFfGYDuG59hO826PJ4PtKS5znDRg8L0K2316BBfVVR8JdWmrwoFppdQwSNgEaXbndImzMpzVJbQENix9TRj3X46gZr9goaGgDOwidKd0RKywI6pZ0BisufqMEEbi0gNzdnK1GemR0Qc7W9O04vS8wZUZhVh+tRlGVkW/JYqT5yl9w7XJM+fvwlZjr1TNe4ffj7RkuY6w0V/cIDpD9xae+qY1ESvUps+1FGb2EEf/znsXkL+RPWsnLutKcxgxiOjfS7ZdLa0a2bs+inTSkgJIl8+rzYjy9yGbPBjM1AxVkQ9qTfWU3CYlKM4FxRyzfQT/c3MDMa04gTbfwnlvX0MYdpk5RJjaJtbj1Hmmaui4p6WrJttdv0uYZp9oz/TZXDoKfqFGlYXLWmuQ1zzvmd59BesaHqFb7Ow3YX2KeaVqkfBGgVpE6c+rhAU8qlNH9F6UDaKO0CvNyAZW1NACRIptkzNJjmeJkHenFJlU/MiFOYT8s2XbhOEMdPToeXIiLMfd0hlY2CEd5tcwRBmftrfmE4aGFpSCQHOxiPeHj++/uRLSnextJRlPzbdLqWgKpPjs22mnCEzH/GlLg3LfWGrzbIePTZx+wjTJssjX9/N5f8m6RtedFisDbSkuQ7RP3IU2FtUmwFSrB0QqjLt68ATjVd62Y20u8RpB2uygJJK9/V6AzqdmG4UZ90uhVxgKTfpmidQYxOnPS2GjeJxEbE27w0GSoLHu7DnE8qb51F6rwYKFlQvSyky3yWR8kxJ+s3wKrL0mcp2sMa9Cqh2adj30EfA9n7bFXP43j1c8csHuJxnatW/mmhJ86WCur7NJoToCxQZIrTN7DR67h/lyFdneHg4m0STmSrTb7KX1o3Cu1NfmKVY5Udrt2ETnq0289twUo7AqKycQi2ZT8gdpypVm8AgVYm6d4+HLE1bjLp0yTIU4LFVpC/oEyJLm0zTe2ZyUM0fj8fHGybIqKBkbZM8esOzXLEGAjx14ftLabEO8LJfewyoFZvR5YpO/fy6+LdXocqx6nH2l+VeJmNqJgN3JCOvW0qSvEJRSojdeJVDBDZsElE5svCZk+XXlR/lpxhpNvUMXLWo8qMTxSobfGNJdViKu7n1GNHbs239OBvpyJBdX3VYUhFLcUevR/rYZFs/7hAroW/2WQTW18fESLqvH+fLuCTr3oN0WyANyt5m7v3ib+wgesP6IUuDVmmuUwxOnkT6oHy/YEBtLitv0xxmbQuqzdTPmfk+TcK791QB/2ZVV0uDWEFHimZ1tj8bV9MuY+YpSstp5ZgqqvR6wfZluurTqC4TTU+Oy7pBugnvkemXjqUwS/yWtpq0/ZdNAzx5VRnOmfSlGmVtjdP6TCTdd79d9Wtvm//ALsb+zz2MrGKu5XIgakX6nC2zESJngcdWux1DYCtwfLUb0RBtmy8c1mO7/y63+TKl1LaqQmtFaT6mlHr1ajeiKUTk3vXW7rbNFw7rsd1tm6vR+jRbtGjRogFa0mzRokWLBlgrpPnh1W7AkFiP7W7bfOGwHtvdtrkCayIQ1KJFixbrBWtFabZo0aLFusCqk6aIvElEHhORJ0Tk3avdHgMR+X0ROSYiD1vbLhKRL4rIIf29RW8XEfltfQ3fFJGbV6nNe0XkKyLyLRE5KCI/u07aPSYi94jIg7rdv6a37xORu3X7Pi4iI3r7qF5/Qu+/fDXardvSEZH7ReRz66HNInJYRB4SkQdE5F69ba0/H5tF5BMi8qiIPCIit61qm5VSq/Yhmcz5SeAKYAR4ELh2Ndtkte31wM3Aw9a23wDerZffDbxXL78Z+AuS1OdbgbtXqc07gZv18hTwOHDtOmi3ABv0cg+4W7fnfwNv19t/B/gZvfzPgN/Ry28HPr6Kz8m7gD8GPqfX13SbgcPAVmfbWn8+Pgr8hF4eATavZptX5UGzbsZtwOet9fcA71nNNjntu9whzceAnXp5J0l+KcCHgB/xlVvl9n8G+O711G5gAvhb4LtIEpa77rMCfB64TS93dTlZhbbuAb4E3Al8Tr+oa73NPtJcs88HsAl42r1Xq9nm1TbPd0Oup/4RvW2t4hKllJkb9HngEr285q5Dm383kai2Nd9ubeY+ABwDvkhigZxSSpnhFO22pe3W+08DF1/YFgPwAeCXyEabu5i132YFfEFE7hORn9Tb1vLzsQ94EfgD7Qb5XRGZZBXbvNqkuW6hkr+xNZl6ICIbgE8CP6eUOmPvW6vtVkoNlFI3kqi3W4BrVrlJpRCRtwDHlFL3rXZbGuJ2pdTNwPcC/1xEXm/vXIPPR5fETfY/lFI3AedIzPEUF7rNq02aycCQGfbobWsVL4jITgD9fUxvXzPXISI9EsL8I6XUp/TmNd9uA6XUKeArJKbtZhExXX3ttqXt1vs3AScucFNfB3yfiBwGPkZiov8Wa7vNKKWO6u9jwKdJ/qDW8vNxBDiilLpbr3+ChERXrc2rTZrfAPbriOMIiYP8s6vcpjJ8FniHXn4Hic/QbP9xHbm7FThtmQ4XDCIiwO8Bjyil/ou1a623e5uIbNbL4yR+2EdIyPOtupjbbnM9bwW+rNXGBYNS6j1KqT1KqctJntsvK6V+lDXcZhGZFJEpswx8D/Awa/j5UEo9DzwrIi/Tm94AfGtV23whnboBR++bSaK8TwL/erXbY7XrT4DngCWSf7t3kvigvgQcAv4KuEiXFeC/62t4CHj1KrX5dhIz5ZvAA/rz5nXQ7huA+3W7HwZ+VW+/ArgHeAL4U2BUbx/T60/o/Ves8rNyB1n0fM22WbftQf05aN63dfB83Ajcq5+PPwO2rGab2x5BLVq0aNEAq22et2jRosW6QkuaLVq0aNEALWm2aNGiRQO0pNmiRYsWDdCSZosWLVo0QEuaLVq0aNEALWm2aNGiRQO0pNmiRYsWDfD/AcSPQCNUqXv+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa26fef5110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "now = train.next()\n",
    "plt.figure()\n",
    "plt.imshow(now[1][0][:,:,0])\n",
    "plt.figure()\n",
    "plt.imshow(now[0][0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAYlklEQVR4Xu3WQQEAAAgCMelf2iA3GzB8sHMECBAgQIAAAQIpgaXSCkuAAAECBAgQIHAGoCcgQIAAAQIECMQEDMBY4eISIECAAAECBAxAP0CAAAECBAgQiAkYgLHCxSVAgAABAgQIGIB+gAABAgQIECAQEzAAY4WLS4AAAQIECBAwAP0AAQIECBAgQCAmYADGCheXAAECBAgQIGAA+gECBAgQIECAQEzAAIwVLi4BAgQIECBAwAD0AwQIECBAgACBmIABGCtcXAIECBAgQICAAegHCBAgQIAAAQIxAQMwVri4BAgQIECAAAED0A8QIECAAAECBGICBmCscHEJECBAgAABAgagHyBAgAABAgQIxAQMwFjh4hIgQIAAAQIEDEA/QIAAAQIECBCICRiAscLFJUCAAAECBAgYgH6AAAECBAgQIBATMABjhYtLgAABAgQIEDAA/QABAgQIECBAICZgAMYKF5cAAQIECBAgYAD6AQIECBAgQIBATMAAjBUuLgECBAgQIEDAAPQDBAgQIECAAIGYgAEYK1xcAgQIECBAgIAB6AcIECBAgAABAjEBAzBWuLgECBAgQIAAAQPQDxAgQIAAAQIEYgIGYKxwcQkQIECAAAECBqAfIECAAAECBAjEBAzAWOHiEiBAgAABAgQMQD9AgAABAgQIEIgJGICxwsUlQIAAAQIECBiAfoAAAQIECBAgEBMwAGOFi0uAAAECBAgQMAD9AAECBAgQIEAgJmAAxgoXlwABAgQIECBgAPoBAgQIECBAgEBMwACMFS4uAQIECBAgQMAA9AMECBAgQIAAgZiAARgrXFwCBAgQIECAgAHoBwgQIECAAAECMQEDMFa4uAQIECBAgAABA9APECBAgAABAgRiAgZgrHBxCRAgQIAAAQIGoB8gQIAAAQIECMQEDMBY4eISIECAAAECBAxAP0CAAAECBAgQiAkYgLHCxSVAgAABAgQIGIB+gAABAgQIECAQEzAAY4WLS4AAAQIECBAwAP0AAQIECBAgQCAmYADGCheXAAECBAgQIGAA+gECBAgQIECAQEzAAIwVLi4BAgQIECBAwAD0AwQIECBAgACBmIABGCtcXAIECBAgQICAAegHCBAgQIAAAQIxAQMwVri4BAgQIECAAAED0A8QIECAAAECBGICBmCscHEJECBAgAABAgagHyBAgAABAgQIxAQMwFjh4hIgQIAAAQIEDEA/QIAAAQIECBCICRiAscLFJUCAAAECBAgYgH6AAAECBAgQIBATMABjhYtLgAABAgQIEDAA/QABAgQIECBAICZgAMYKF5cAAQIECBAgYAD6AQIECBAgQIBATMAAjBUuLgECBAgQIEDAAPQDBAgQIECAAIGYgAEYK1xcAgQIECBAgIAB6AcIECBAgAABAjEBAzBWuLgECBAgQIAAAQPQDxAgQIAAAQIEYgIGYKxwcQkQIECAAAECBqAfIECAAAECBAjEBAzAWOHiEiBAgAABAgQMQD9AgAABAgQIEIgJGICxwsUlQIAAAQIECBiAfoAAAQIECBAgEBMwAGOFi0uAAAECBAgQMAD9AAECBAgQIEAgJmAAxgoXlwABAgQIECBgAPoBAgQIECBAgEBMwACMFS4uAQIECBAgQMAA9AMECBAgQIAAgZiAARgrXFwCBAgQIECAgAHoBwgQIECAAAECMQEDMFa4uAQIECBAgAABA9APECBAgAABAgRiAgZgrHBxCRAgQIAAAQIGoB8gQIAAAQIECMQEDMBY4eISIECAAAECBAxAP0CAAAECBAgQiAkYgLHCxSVAgAABAgQIGIB+gAABAgQIECAQEzAAY4WLS4AAAQIECBAwAP0AAQIECBAgQCAmYADGCheXAAECBAgQIGAA+gECBAgQIECAQEzAAIwVLi4BAgQIECBAwAD0AwQIECBAgACBmIABGCtcXAIECBAgQICAAegHCBAgQIAAAQIxAQMwVri4BAgQIECAAAED0A8QIECAAAECBGICBmCscHEJECBAgAABAgagHyBAgAABAgQIxAQMwFjh4hIgQIAAAQIEDEA/QIAAAQIECBCICRiAscLFJUCAAAECBAgYgH6AAAECBAgQIBATMABjhYtLgAABAgQIEDAA/QABAgQIECBAICZgAMYKF5cAAQIECBAgYAD6AQIECBAgQIBATMAAjBUuLgECBAgQIEDAAPQDBAgQIECAAIGYgAEYK1xcAgQIECBAgIAB6AcIECBAgAABAjEBAzBWuLgECBAgQIAAAQPQDxAgQIAAAQIEYgIGYKxwcQkQIECAAAECBqAfIECAAAECBAjEBAzAWOHiEiBAgAABAgQMQD9AgAABAgQIEIgJGICxwsUlQIAAAQIECBiAfoAAAQIECBAgEBMwAGOFi0uAAAECBAgQMAD9AAECBAgQIEAgJmAAxgoXlwABAgQIECBgAPoBAgQIECBAgEBMwACMFS4uAQIECBAgQMAA9AMECBAgQIAAgZiAARgrXFwCBAgQIECAgAHoBwgQIECAAAECMQEDMFa4uAQIECBAgAABA9APECBAgAABAgRiAgZgrHBxCRAgQIAAAQIGoB8gQIAAAQIECMQEDMBY4eISIECAAAECBAxAP0CAAAECBAgQiAkYgLHCxSVAgAABAgQIGIB+gAABAgQIECAQEzAAY4WLS4AAAQIECBAwAP0AAQIECBAgQCAmYADGCheXAAECBAgQIGAA+gECBAgQIECAQEzAAIwVLi4BAgQIECBAwAD0AwQIECBAgACBmIABGCtcXAIECBAgQICAAegHCBAgQIAAAQIxAQMwVri4BAgQIECAAAED0A8QIECAAAECBGICBmCscHEJECBAgAABAgagHyBAgAABAgQIxAQMwFjh4hIgQIAAAQIEDEA/QIAAAQIECBCICRiAscLFJUCAAAECBAgYgH6AAAECBAgQIBATMABjhYtLgAABAgQIEDAA/QABAgQIECBAICZgAMYKF5cAAQIECBAgYAD6AQIECBAgQIBATMAAjBUuLgECBAgQIEDAAPQDBAgQIECAAIGYgAEYK1xcAgQIECBAgIAB6AcIECBAgAABAjEBAzBWuLgECBAgQIAAAQPQDxAgQIAAAQIEYgIGYKxwcQkQIECAAAECBqAfIECAAAECBAjEBAzAWOHiEiBAgAABAgQMQD9AgAABAgQIEIgJGICxwsUlQIAAAQIECBiAfoAAAQIECBAgEBMwAGOFi0uAAAECBAgQMAD9AAECBAgQIEAgJmAAxgoXlwABAgQIECBgAPoBAgQIECBAgEBMwACMFS4uAQIECBAgQMAA9AMECBAgQIAAgZiAARgrXFwCBAgQIECAgAHoBwgQIECAAAECMQEDMFa4uAQIECBAgAABA9APECBAgAABAgRiAgZgrHBxCRAgQIAAAQIGoB8gQIAAAQIECMQEDMBY4eISIECAAAECBAxAP0CAAAECBAgQiAkYgLHCxSVAgAABAgQIGIB+gAABAgQIECAQEzAAY4WLS4AAAQIECBAwAP0AAQIECBAgQCAmYADGCheXAAECBAgQIGAA+gECBAgQIECAQEzAAIwVLi4BAgQIECBAwAD0AwQIECBAgACBmIABGCtcXAIECBAgQICAAegHCBAgQIAAAQIxAQMwVri4BAgQIECAAAED0A8QIECAAAECBGICBmCscHEJECBAgAABAgagHyBAgAABAgQIxAQMwFjh4hIgQIAAAQIEDEA/QIAAAQIECBCICRiAscLFJUCAAAECBAgYgH6AAAECBAgQIBATMABjhYtLgAABAgQIEDAA/QABAgQIECBAICZgAMYKF5cAAQIECBAgYAD6AQIECBAgQIBATMAAjBUuLgECBAgQIEDAAPQDBAgQIECAAIGYgAEYK1xcAgQIECBAgIAB6AcIECBAgAABAjEBAzBWuLgECBAgQIAAAQPQDxAgQIAAAQIEYgIGYKxwcQkQIECAAAECBqAfIECAAAECBAjEBAzAWOHiEiBAgAABAgQMQD9AgAABAgQIEIgJGICxwsUlQIAAAQIECBiAfoAAAQIECBAgEBMwAGOFi0uAAAECBAgQMAD9AAECBAgQIEAgJmAAxgoXlwABAgQIECBgAPoBAgQIECBAgEBMwACMFS4uAQIECBAgQMAA9AMECBAgQIAAgZiAARgrXFwCBAgQIECAgAHoBwgQIECAAAECMQEDMFa4uAQIECBAgAABA9APECBAgAABAgRiAgZgrHBxCRAgQIAAAQIGoB8gQIAAAQIECMQEDMBY4eISIECAAAECBAxAP0CAAAECBAgQiAkYgLHCxSVAgAABAgQIGIB+gAABAgQIECAQEzAAY4WLS4AAAQIECBAwAP0AAQIECBAgQCAmYADGCheXAAECBAgQIGAA+gECBAgQIECAQEzAAIwVLi4BAgQIECBAwAD0AwQIECBAgACBmIABGCtcXAIECBAgQICAAegHCBAgQIAAAQIxAQMwVri4BAgQIECAAAED0A8QIECAAAECBGICBmCscHEJECBAgAABAgagHyBAgAABAgQIxAQMwFjh4hIgQIAAAQIEDEA/QIAAAQIECBCICRiAscLFJUCAAAECBAgYgH6AAAECBAgQIBATMABjhYtLgAABAgQIEDAA/QABAgQIECBAICZgAMYKF5cAAQIECBAgYAD6AQIECBAgQIBATMAAjBUuLgECBAgQIEDAAPQDBAgQIECAAIGYgAEYK1xcAgQIECBAgIAB6AcIECBAgAABAjEBAzBWuLgECBAgQIAAAQPQDxAgQIAAAQIEYgIGYKxwcQkQIECAAAECBqAfIECAAAECBAjEBAzAWOHiEiBAgAABAgQMQD9AgAABAgQIEIgJGICxwsUlQIAAAQIECBiAfoAAAQIECBAgEBMwAGOFi0uAAAECBAgQMAD9AAECBAgQIEAgJmAAxgoXlwABAgQIECBgAPoBAgQIECBAgEBMwACMFS4uAQIECBAgQMAA9AMECBAgQIAAgZiAARgrXFwCBAgQIECAgAHoBwgQIECAAAECMQEDMFa4uAQIECBAgAABA9APECBAgAABAgRiAgZgrHBxCRAgQIAAAQIGoB8gQIAAAQIECMQEDMBY4eISIECAAAECBAxAP0CAAAECBAgQiAkYgLHCxSVAgAABAgQIGIB+gAABAgQIECAQEzAAY4WLS4AAAQIECBAwAP0AAQIECBAgQCAmYADGCheXAAECBAgQIGAA+gECBAgQIECAQEzAAIwVLi4BAgQIECBAwAD0AwQIECBAgACBmIABGCtcXAIECBAgQICAAegHCBAgQIAAAQIxAQMwVri4BAgQIECAAAED0A8QIECAAAECBGICBmCscHEJECBAgAABAgagHyBAgAABAgQIxAQMwFjh4hIgQIAAAQIEDEA/QIAAAQIECBCICRiAscLFJUCAAAECBAgYgH6AAAECBAgQIBATMABjhYtLgAABAgQIEDAA/QABAgQIECBAICZgAMYKF5cAAQIECBAgYAD6AQIECBAgQIBATMAAjBUuLgECBAgQIEDAAPQDBAgQIECAAIGYgAEYK1xcAgQIECBAgIAB6AcIECBAgAABAjEBAzBWuLgECBAgQIAAAQPQDxAgQIAAAQIEYgIGYKxwcQkQIECAAAECBqAfIECAAAECBAjEBAzAWOHiEiBAgAABAgQMQD9AgAABAgQIEIgJGICxwsUlQIAAAQIECBiAfoAAAQIECBAgEBMwAGOFi0uAAAECBAgQMAD9AAECBAgQIEAgJmAAxgoXlwABAgQIECBgAPoBAgQIECBAgEBMwACMFS4uAQIECBAgQMAA9AMECBAgQIAAgZiAARgrXFwCBAgQIECAgAHoBwgQIECAAAECMQEDMFa4uAQIECBAgAABA9APECBAgAABAgRiAgZgrHBxCRAgQIAAAQIGoB8gQIAAAQIECMQEDMBY4eISIECAAAECBAxAP0CAAAECBAgQiAkYgLHCxSVAgAABAgQIGIB+gAABAgQIECAQEzAAY4WLS4AAAQIECBAwAP0AAQIECBAgQCAmYADGCheXAAECBAgQIGAA+gECBAgQIECAQEzAAIwVLi4BAgQIECBAwAD0AwQIECBAgACBmIABGCtcXAIECBAgQICAAegHCBAgQIAAAQIxAQMwVri4BAgQIECAAAED0A8QIECAAAECBGICBmCscHEJECBAgAABAgagHyBAgAABAgQIxAQMwFjh4hIgQIAAAQIEDEA/QIAAAQIECBCICRiAscLFJUCAAAECBAgYgH6AAAECBAgQIBATMABjhYtLgAABAgQIEDAA/QABAgQIECBAICZgAMYKF5cAAQIECBAgYAD6AQIECBAgQIBATMAAjBUuLgECBAgQIEDAAPQDBAgQIECAAIGYgAEYK1xcAgQIECBAgIAB6AcIECBAgAABAjEBAzBWuLgECBAgQIAAAQPQDxAgQIAAAQIEYgIGYKxwcQkQIECAAAECBqAfIECAAAECBAjEBAzAWOHiEiBAgAABAgQMQD9AgAABAgQIEIgJGICxwsUlQIAAAQIECBiAfoAAAQIECBAgEBMwAGOFi0uAAAECBAgQMAD9AAECBAgQIEAgJmAAxgoXlwABAgQIECBgAPoBAgQIECBAgEBMwACMFS4uAQIECBAgQMAA9AMECBAgQIAAgZiAARgrXFwCBAgQIECAgAHoBwgQIECAAAECMQEDMFa4uAQIECBAgAABA9APECBAgAABAgRiAgZgrHBxCRAgQIAAAQIGoB8gQIAAAQIECMQEDMBY4eISIECAAAECBAxAP0CAAAECBAgQiAkYgLHCxSVAgAABAgQIGIB+gAABAgQIECAQEzAAY4WLS4AAAQIECBAwAP0AAQIECBAgQCAmYADGCheXAAECBAgQIGAA+gECBAgQIECAQEzAAIwVLi4BAgQIECBAwAD0AwQIECBAgACBmIABGCtcXAIECBAgQICAAegHCBAgQIAAAQIxAQMwVri4BAgQIECAAAED0A8QIECAAAECBGICBmCscHEJECBAgAABAgagHyBAgAABAgQIxAQMwFjh4hIgQIAAAQIEDEA/QIAAAQIECBCICRiAscLFJUCAAAECBAgYgH6AAAECBAgQIBATMABjhYtLgAABAgQIEDAA/QABAgQIECBAICZgAMYKF5cAAQIECBAgYAD6AQIECBAgQIBATMAAjBUuLgECBAgQIEDAAPQDBAgQIECAAIGYgAEYK1xcAgQIECBAgIAB6AcIECBAgAABAjEBAzBWuLgECBAgQIAAAQPQDxAgQIAAAQIEYgIGYKxwcQkQIECAAAECBqAfIECAAAECBAjEBAzAWOHiEiBAgAABAgQMQD9AgAABAgQIEIgJGICxwsUlQIAAAQIECBiAfoAAAQIECBAgEBMwAGOFi0uAAAECBAgQMAD9AAECBAgQIEAgJmAAxgoXlwABAgQIECBgAPoBAgQIECBAgEBMwACMFS4uAQIECBAgQMAA9AMECBAgQIAAgZiAARgrXFwCBAgQIECAgAHoBwgQIECAAAECMQEDMFa4uAQIECBAgAABA9APECBAgAABAgRiAgZgrHBxCRAgQIAAAQIGoB8gQIAAAQIECMQEDMBY4eISIECAAAECBAxAP0CAAAECBAgQiAkYgLHCxSVAgAABAgQIGIB+gAABAgQIECAQEzAAY4WLS4AAAQIECBAwAP0AAQIECBAgQCAmYADGCheXAAECBAgQIGAA+gECBAgQIECAQEzAAIwVLi4BAgQIECBAwAD0AwQIECBAgACBmIABGCtcXAIECBAgQICAAegHCBAgQIAAAQIxAQMwVri4BAgQIECAAAED0A8QIECAAAECBGICBmCscHEJECBAgAABAgagHyBAgAABAgQIxAQMwFjh4hIgQIAAAQIEDEA/QIAAAQIECBCICRiAscLFJUCAAAECBAgYgH6AAAECBAgQIBATMABjhYtLgAABAgQIEDAA/QABAgQIECBAICZgAMYKF5cAAQIECBAgYAD6AQIECBAgQIBATMAAjBUuLgECBAgQIEDAAPQDBAgQIECAAIGYgAEYK1xcAgQIECBAgIAB6AcIECBAgAABAjEBAzBWuLgECBAgQIAAAQPQDxAgQIAAAQIEYgIGYKxwcQkQIECAAAECBqAfIECAAAECBAjEBAzAWOHiEiBAgAABAgQMQD9AgAABAgQIEIgJGICxwsUlQIAAAQIECBiAfoAAAQIECBAgEBMwAGOFi0uAAAECBAgQMAD9AAECBAgQIEAgJmAAxgoXlwABAgQIECBgAPoBAgQIECBAgEBMwACMFS4uAQIECBAgQMAA9AMECBAgQIAAgZiAARgrXFwCBAgQIECAgAHoBwgQIECAAAECMQEDMFa4uAQIECBAgACBB+4+AeEFWYvzAAAAAElFTkSuQmCC\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support.' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option)\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width, fig.canvas.height);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to  previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overriden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>')\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f94c9172b10>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib nbagg\n",
    "now = gen.next()\n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(now[1][0],(480,640)))\n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(now[0][0,:,:,0],(480,640)))\n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(now[0][0,:,:,1:4],(480,640,3)))\n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(now[0][0,:,:,4:],(480,640,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 480 #check with old params\n",
    "img_width = 640\n",
    "inputs = Input((img_height, img_width,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, 3, padding=\"same\", data_format=\"channels_last\", input_shape=(480, 640,...)`\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:36: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:52: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:55: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:58: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:72: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:75: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:78: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:83: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:86: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:89: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:94: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:100: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:105: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:108: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:113: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, 3, padding=\"same\", data_format=\"channels_last\")`\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:116: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, 1, padding=\"valid\", data_format=\"channels_last\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_703 (Conv2D)          (None, 480, 640, 16)      160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_703 (Bat (None, 480, 640, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_703 (Activation)  (None, 480, 640, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_704 (Conv2D)          (None, 480, 640, 16)      2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_704 (Bat (None, 480, 640, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_704 (Activation)  (None, 480, 640, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_136 (MaxPoolin (None, 240, 320, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_705 (Conv2D)          (None, 240, 320, 32)      4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_705 (Bat (None, 240, 320, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_705 (Activation)  (None, 240, 320, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_706 (Conv2D)          (None, 240, 320, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_706 (Bat (None, 240, 320, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_706 (Activation)  (None, 240, 320, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_137 (MaxPoolin (None, 120, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_707 (Conv2D)          (None, 120, 160, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_707 (Bat (None, 120, 160, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_707 (Activation)  (None, 120, 160, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_708 (Conv2D)          (None, 120, 160, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_708 (Bat (None, 120, 160, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_708 (Activation)  (None, 120, 160, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_709 (Conv2D)          (None, 120, 160, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_709 (Bat (None, 120, 160, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_709 (Activation)  (None, 120, 160, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_138 (MaxPoolin (None, 60, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_710 (Conv2D)          (None, 60, 80, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_710 (Bat (None, 60, 80, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_710 (Activation)  (None, 60, 80, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_711 (Conv2D)          (None, 60, 80, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_711 (Bat (None, 60, 80, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_711 (Activation)  (None, 60, 80, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_712 (Conv2D)          (None, 60, 80, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_712 (Bat (None, 60, 80, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_712 (Activation)  (None, 60, 80, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_139 (MaxPoolin (None, 30, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_713 (Conv2D)          (None, 30, 40, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_713 (Bat (None, 30, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_713 (Activation)  (None, 30, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_714 (Conv2D)          (None, 30, 40, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_714 (Bat (None, 30, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_714 (Activation)  (None, 30, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_715 (Conv2D)          (None, 30, 40, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_715 (Bat (None, 30, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_715 (Activation)  (None, 30, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_140 (MaxPoolin (None, 15, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_136 (UpSamplin (None, 30, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_716 (Conv2D)          (None, 30, 40, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_716 (Bat (None, 30, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_716 (Activation)  (None, 30, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_717 (Conv2D)          (None, 30, 40, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_717 (Bat (None, 30, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_717 (Activation)  (None, 30, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_718 (Conv2D)          (None, 30, 40, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_718 (Bat (None, 30, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_718 (Activation)  (None, 30, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_137 (UpSamplin (None, 60, 80, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_719 (Conv2D)          (None, 60, 80, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_719 (Bat (None, 60, 80, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_719 (Activation)  (None, 60, 80, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_720 (Conv2D)          (None, 60, 80, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_720 (Bat (None, 60, 80, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_720 (Activation)  (None, 60, 80, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_721 (Conv2D)          (None, 60, 80, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_721 (Bat (None, 60, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_721 (Activation)  (None, 60, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_138 (UpSamplin (None, 120, 160, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_722 (Conv2D)          (None, 120, 160, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_722 (Bat (None, 120, 160, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_722 (Activation)  (None, 120, 160, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_723 (Conv2D)          (None, 120, 160, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_723 (Bat (None, 120, 160, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_723 (Activation)  (None, 120, 160, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_724 (Conv2D)          (None, 120, 160, 32)      18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_724 (Bat (None, 120, 160, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_724 (Activation)  (None, 120, 160, 32)      0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_139 (UpSamplin (None, 240, 320, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_725 (Conv2D)          (None, 240, 320, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_725 (Bat (None, 240, 320, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_725 (Activation)  (None, 240, 320, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_726 (Conv2D)          (None, 240, 320, 16)      4624      \n",
      "_________________________________________________________________\n",
      "batch_normalization_726 (Bat (None, 240, 320, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_726 (Activation)  (None, 240, 320, 16)      0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_140 (UpSamplin (None, 480, 640, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_727 (Conv2D)          (None, 480, 640, 16)      2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_727 (Bat (None, 480, 640, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_727 (Activation)  (None, 480, 640, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_728 (Conv2D)          (None, 480, 640, 1)       17        \n",
      "_________________________________________________________________\n",
      "batch_normalization_728 (Bat (None, 480, 640, 1)       4         \n",
      "_________________________________________________________________\n",
      "activation_728 (Activation)  (None, 480, 640, 1)       0         \n",
      "=================================================================\n",
      "Total params: 1,848,677\n",
      "Trainable params: 1,844,707\n",
      "Non-trainable params: 3,970\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras.layers.core import Activation, Reshape, Permute\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import json\n",
    "\n",
    "img_w = img_width\n",
    "img_h = img_height\n",
    "n_labels = 2\n",
    "\n",
    "kernel = 3\n",
    "\n",
    "encoding_layers = [\n",
    "    Convolution2D(16, kernel, border_mode='same', input_shape=( img_h, img_w,1),data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(16, kernel, border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(data_format='channels_last'),\n",
    "\n",
    "    Convolution2D(32, kernel, border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(32, kernel, border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(data_format='channels_last'),\n",
    "\n",
    "    Convolution2D(64, kernel,  border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(64, kernel, border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(64, kernel,  border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(data_format='channels_last'),\n",
    "\n",
    "    Convolution2D(128, kernel,  border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(128, kernel,  border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(128, kernel,  border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(data_format='channels_last'),\n",
    "\n",
    "    Convolution2D(128, kernel,  border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(128, kernel,  border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(128, kernel, border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(data_format='channels_last'),\n",
    "]\n",
    "\n",
    "autoencoder = models.Sequential()\n",
    "autoencoder.encoding_layers = encoding_layers\n",
    "\n",
    "for l in autoencoder.encoding_layers:\n",
    "    autoencoder.add(l)\n",
    "\n",
    "decoding_layers = [\n",
    "    UpSampling2D(data_format='channels_last'),\n",
    "    Convolution2D(128, kernel,  border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(128, kernel,  border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(128, kernel,  border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "\n",
    "    UpSampling2D(data_format='channels_last'),\n",
    "    Convolution2D(128, kernel,  border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(128, kernel,  border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(64, kernel, border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "\n",
    "    UpSampling2D(data_format='channels_last'),\n",
    "    Convolution2D(64, kernel,  border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(64, kernel,  border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(32, kernel, border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "\n",
    "    UpSampling2D(data_format='channels_last'),\n",
    "    Convolution2D(32, kernel, border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(16, kernel, border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "\n",
    "    UpSampling2D(data_format='channels_last'),\n",
    "    Convolution2D(16, kernel, border_mode='same',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Convolution2D(1, 1, border_mode='valid',data_format='channels_last'),\n",
    "    BatchNormalization(),\n",
    "]\n",
    "autoencoder.decoding_layers = decoding_layers\n",
    "for l in autoencoder.decoding_layers:\n",
    "    autoencoder.add(l)\n",
    "\n",
    "\n",
    "#autoencoder.add(Reshape((n_labels, img_h * img_w)))\n",
    "#autoencoder.add(Permute((2, 1)))\n",
    "autoencoder.add(Activation('sigmoid'))\n",
    "\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer = optimizers.Adam(lr = 1e-4), loss =  binary_crossentropy, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa23a6ff0d0>"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa23b9aa410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import struct\n",
    "cpp_int_size = 4\n",
    "cpp_ushort_size = 2\n",
    "with open(\"/media/drc/DATA/linemod/driller/data/depth200.dpt\", 'rb') as f:\n",
    "    rows_b = f.read(cpp_int_size) # I assume that the C++ int in question has 4 bits ... trial and error\n",
    "    cols_b = f.read(cpp_int_size)\n",
    "\n",
    "    R = struct.unpack('<i', rows_b)[0] # small endian\n",
    "    C = struct.unpack('<i', cols_b)[0]\n",
    "    depth_image_str = f.read(R * C * cpp_ushort_size)\n",
    "depth_img = np.fromstring(depth_image_str, dtype=np.uint16).reshape([R, C])\n",
    "plt.imshow(depth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(depth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "cannot identify image file '/media/drc/DATA/linemod/driller/data/depth572.dpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-52df8d702534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib nbagg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/media/drc/DATA/linemod/driller/data/depth572.dpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/utils.pyc\u001b[0m in \u001b[0;36mnewfunc\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;34m\"\"\"`arrayrange` is deprecated, use `arange` instead!\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mnewfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/misc/pilutil.pyc\u001b[0m in \u001b[0;36mimread\u001b[0;34m(name, flatten, mode)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \"\"\"\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfromimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m     raise IOError(\"cannot identify image file %r\"\n\u001b[0;32m-> 2585\u001b[0;31m                   % (filename if filename else fp))\n\u001b[0m\u001b[1;32m   2586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: cannot identify image file '/media/drc/DATA/linemod/driller/data/depth572.dpt'"
     ]
    }
   ],
   "source": [
    "%matplotlib nbagg\n",
    "plt.imshow(misc.imread(\"/media/drc/DATA/linemod/driller/data/depth572.dpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to dropout(imgs,autoencoder,out=\"./\"):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights(\"../models/net_segnet_aug.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossentropy(y_true, y_pred):\n",
    "    print np.shape(y_true),np.shape(y_pred)\n",
    "    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_pixelwise_crossentropy(class_weights):\n",
    "    \n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = tf.convert_to_tensor(.01, y_pred.dtype.base_dtype)\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        return - tf.reduce_sum(tf.multiply(y_true * tf.log(y_pred), tf.convert_to_tensor(class_weights.values())))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "_EPSILON = 10e-8\n",
    "from tensorflow.python.ops import clip_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn\n",
    "def crop1(x,x1 = 100,x2 = 500,y1 = 50, y2 = 450):\n",
    "    x = x[:,y1:y2,x1:x2,:]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossentropy(target, output, from_logits=False):\n",
    "    \"\"\"Binary crossentropy between an output tensor and a target tensor.\n",
    "      Arguments:\n",
    "          target: A tensor with the same shape as `output`.\n",
    "          output: A tensor.\n",
    "          from_logits: Whether `output` is expected to be a logits tensor.\n",
    "              By default, we consider that `output`\n",
    "              encodes a probability distribution.\n",
    "      Returns:\n",
    "          A tensor.\n",
    "      \"\"\"\n",
    "    #target  = crop1(target)\n",
    "   # output = crop1(output)\n",
    "    #print np.shape(target), np.shape(output)\n",
    "\n",
    "      # Note: nn.softmax_cross_entropy_with_logits\n",
    "      # expects logits, Keras expects probabilities.\n",
    "    if not from_logits:\n",
    "        # transform back to logits\n",
    "        epsilon_ =  tf.convert_to_tensor(_EPSILON, output.dtype.base_dtype)\n",
    "        output = clip_ops.clip_by_value(output, epsilon_, 1 - epsilon_)\n",
    "        output = math_ops.log(output / (1 - output))\n",
    "    return nn.weighted_cross_entropy_with_logits(target, output, .15, name=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0 : .85,\n",
    "    1: .15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., shuffle=True, verbose=1, steps_per_epoch=100, epochs=10000, callbacks=[<keras.ca...)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9466\n",
      "Epoch 00001: loss improved from inf to 0.03043, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 39s 389ms/step - loss: 0.0304 - acc: 0.9466\n",
      "Epoch 2/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9435\n",
      "Epoch 00002: loss did not improve\n",
      "100/100 [==============================] - 38s 381ms/step - loss: 0.0317 - acc: 0.9436\n",
      "Epoch 3/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9442\n",
      "Epoch 00003: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0317 - acc: 0.9444\n",
      "Epoch 4/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9427\n",
      "Epoch 00004: loss did not improve\n",
      "100/100 [==============================] - 40s 397ms/step - loss: 0.0323 - acc: 0.9429\n",
      "Epoch 5/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9429\n",
      "Epoch 00005: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0324 - acc: 0.9431\n",
      "Epoch 6/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9433\n",
      "Epoch 00006: loss did not improve\n",
      "100/100 [==============================] - 38s 385ms/step - loss: 0.0320 - acc: 0.9435\n",
      "Epoch 7/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9426\n",
      "Epoch 00007: loss did not improve\n",
      "100/100 [==============================] - 39s 385ms/step - loss: 0.0327 - acc: 0.9425\n",
      "Epoch 8/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9427\n",
      "Epoch 00008: loss did not improve\n",
      "100/100 [==============================] - 39s 385ms/step - loss: 0.0317 - acc: 0.9428\n",
      "Epoch 9/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9431\n",
      "Epoch 00009: loss did not improve\n",
      "100/100 [==============================] - 39s 385ms/step - loss: 0.0318 - acc: 0.9428\n",
      "Epoch 10/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9456\n",
      "Epoch 00010: loss improved from 0.03043 to 0.03035, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 39s 395ms/step - loss: 0.0303 - acc: 0.9456\n",
      "Epoch 11/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9426\n",
      "Epoch 00011: loss did not improve\n",
      "100/100 [==============================] - 39s 387ms/step - loss: 0.0321 - acc: 0.9427\n",
      "Epoch 12/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9453\n",
      "Epoch 00012: loss did not improve\n",
      "100/100 [==============================] - 40s 398ms/step - loss: 0.0310 - acc: 0.9454\n",
      "Epoch 13/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9437\n",
      "Epoch 00013: loss did not improve\n",
      "100/100 [==============================] - 41s 413ms/step - loss: 0.0314 - acc: 0.9438\n",
      "Epoch 14/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9434\n",
      "Epoch 00014: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0315 - acc: 0.9434\n",
      "Epoch 15/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9427\n",
      "Epoch 00015: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0324 - acc: 0.9424\n",
      "Epoch 16/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9405\n",
      "Epoch 00016: loss did not improve\n",
      "100/100 [==============================] - 42s 418ms/step - loss: 0.0330 - acc: 0.9404\n",
      "Epoch 17/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9417\n",
      "Epoch 00017: loss did not improve\n",
      "100/100 [==============================] - 42s 420ms/step - loss: 0.0322 - acc: 0.9416\n",
      "Epoch 18/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9437\n",
      "Epoch 00018: loss did not improve\n",
      "100/100 [==============================] - 41s 407ms/step - loss: 0.0317 - acc: 0.9436\n",
      "Epoch 19/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9438\n",
      "Epoch 00019: loss did not improve\n",
      "100/100 [==============================] - 39s 386ms/step - loss: 0.0315 - acc: 0.9438\n",
      "Epoch 20/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9441\n",
      "Epoch 00020: loss did not improve\n",
      "100/100 [==============================] - 39s 386ms/step - loss: 0.0316 - acc: 0.9441\n",
      "Epoch 21/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9444\n",
      "Epoch 00021: loss did not improve\n",
      "100/100 [==============================] - 39s 388ms/step - loss: 0.0311 - acc: 0.9443\n",
      "Epoch 22/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9395\n",
      "Epoch 00022: loss did not improve\n",
      "100/100 [==============================] - 39s 386ms/step - loss: 0.0342 - acc: 0.9396\n",
      "Epoch 23/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9440\n",
      "Epoch 00023: loss did not improve\n",
      "100/100 [==============================] - 39s 385ms/step - loss: 0.0313 - acc: 0.9441\n",
      "Epoch 24/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9401\n",
      "Epoch 00024: loss did not improve\n",
      "100/100 [==============================] - 38s 385ms/step - loss: 0.0332 - acc: 0.9400\n",
      "Epoch 25/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9425\n",
      "Epoch 00025: loss did not improve\n",
      "100/100 [==============================] - 39s 385ms/step - loss: 0.0323 - acc: 0.9425\n",
      "Epoch 26/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9456\n",
      "Epoch 00026: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0304 - acc: 0.9456\n",
      "Epoch 27/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9426\n",
      "Epoch 00027: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0321 - acc: 0.9425\n",
      "Epoch 28/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9394\n",
      "Epoch 00028: loss did not improve\n",
      "100/100 [==============================] - 38s 385ms/step - loss: 0.0336 - acc: 0.9396\n",
      "Epoch 29/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9456\n",
      "Epoch 00029: loss did not improve\n",
      "100/100 [==============================] - 39s 389ms/step - loss: 0.0305 - acc: 0.9456\n",
      "Epoch 30/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9417\n",
      "Epoch 00030: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0327 - acc: 0.9419\n",
      "Epoch 31/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9418\n",
      "Epoch 00031: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0328 - acc: 0.9419\n",
      "Epoch 32/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9425\n",
      "Epoch 00032: loss did not improve\n",
      "100/100 [==============================] - 38s 385ms/step - loss: 0.0327 - acc: 0.9428\n",
      "Epoch 33/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9427\n",
      "Epoch 00033: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0321 - acc: 0.9428\n",
      "Epoch 34/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9440\n",
      "Epoch 00034: loss did not improve\n",
      "100/100 [==============================] - 38s 385ms/step - loss: 0.0314 - acc: 0.9441\n",
      "Epoch 35/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9413\n",
      "Epoch 00035: loss did not improve\n",
      "100/100 [==============================] - 38s 385ms/step - loss: 0.0326 - acc: 0.9414\n",
      "Epoch 36/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9422\n",
      "Epoch 00036: loss did not improve\n",
      "100/100 [==============================] - 39s 385ms/step - loss: 0.0324 - acc: 0.9422\n",
      "Epoch 37/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9393\n",
      "Epoch 00037: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0339 - acc: 0.9394\n",
      "Epoch 38/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9446\n",
      "Epoch 00038: loss did not improve\n",
      "100/100 [==============================] - 39s 386ms/step - loss: 0.0314 - acc: 0.9446\n",
      "Epoch 39/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9444\n",
      "Epoch 00039: loss did not improve\n",
      "100/100 [==============================] - 38s 385ms/step - loss: 0.0308 - acc: 0.9444\n",
      "Epoch 40/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9455\n",
      "Epoch 00040: loss improved from 0.03035 to 0.03014, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 39s 390ms/step - loss: 0.0301 - acc: 0.9454\n",
      "Epoch 41/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9469\n",
      "Epoch 00041: loss improved from 0.03014 to 0.03002, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 39s 390ms/step - loss: 0.0300 - acc: 0.9468\n",
      "Epoch 42/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9418\n",
      "Epoch 00042: loss did not improve\n",
      "100/100 [==============================] - 39s 386ms/step - loss: 0.0324 - acc: 0.9418\n",
      "Epoch 43/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9425\n",
      "Epoch 00043: loss did not improve\n",
      "100/100 [==============================] - 40s 397ms/step - loss: 0.0322 - acc: 0.9424\n",
      "Epoch 44/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9463\n",
      "Epoch 00044: loss did not improve\n",
      "100/100 [==============================] - 41s 412ms/step - loss: 0.0301 - acc: 0.9463\n",
      "Epoch 45/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9451\n",
      "Epoch 00045: loss did not improve\n",
      "100/100 [==============================] - 41s 413ms/step - loss: 0.0307 - acc: 0.9452\n",
      "Epoch 46/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9442\n",
      "Epoch 00046: loss did not improve\n",
      "100/100 [==============================] - 40s 404ms/step - loss: 0.0312 - acc: 0.9442\n",
      "Epoch 47/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9455\n",
      "Epoch 00047: loss did not improve\n",
      "100/100 [==============================] - 41s 405ms/step - loss: 0.0305 - acc: 0.9455\n",
      "Epoch 48/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9429\n",
      "Epoch 00048: loss did not improve\n",
      "100/100 [==============================] - 40s 404ms/step - loss: 0.0321 - acc: 0.9427\n",
      "Epoch 49/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9422\n",
      "Epoch 00049: loss did not improve\n",
      "100/100 [==============================] - 41s 409ms/step - loss: 0.0321 - acc: 0.9422\n",
      "Epoch 50/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9417\n",
      "Epoch 00050: loss did not improve\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.0320 - acc: 0.9419\n",
      "Epoch 51/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9443\n",
      "Epoch 00051: loss did not improve\n",
      "100/100 [==============================] - 42s 421ms/step - loss: 0.0319 - acc: 0.9442\n",
      "Epoch 52/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9418\n",
      "Epoch 00052: loss did not improve\n",
      "100/100 [==============================] - 41s 413ms/step - loss: 0.0329 - acc: 0.9417\n",
      "Epoch 53/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9441\n",
      "Epoch 00053: loss did not improve\n",
      "100/100 [==============================] - 42s 418ms/step - loss: 0.0310 - acc: 0.9441\n",
      "Epoch 54/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9453\n",
      "Epoch 00054: loss did not improve\n",
      "100/100 [==============================] - 43s 433ms/step - loss: 0.0306 - acc: 0.9453\n",
      "Epoch 55/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9433\n",
      "Epoch 00055: loss did not improve\n",
      "100/100 [==============================] - 42s 423ms/step - loss: 0.0317 - acc: 0.9433\n",
      "Epoch 56/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9428\n",
      "Epoch 00056: loss did not improve\n",
      "100/100 [==============================] - 43s 432ms/step - loss: 0.0326 - acc: 0.9424\n",
      "Epoch 57/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9444\n",
      "Epoch 00057: loss did not improve\n",
      "100/100 [==============================] - 42s 420ms/step - loss: 0.0308 - acc: 0.9445\n",
      "Epoch 58/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9404\n",
      "Epoch 00058: loss did not improve\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.0332 - acc: 0.9406\n",
      "Epoch 59/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9430\n",
      "Epoch 00059: loss did not improve\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.0321 - acc: 0.9431\n",
      "Epoch 60/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9453\n",
      "Epoch 00060: loss did not improve\n",
      "100/100 [==============================] - 43s 428ms/step - loss: 0.0304 - acc: 0.9454\n",
      "Epoch 61/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9429\n",
      "Epoch 00061: loss did not improve\n",
      "100/100 [==============================] - 42s 422ms/step - loss: 0.0320 - acc: 0.9430\n",
      "Epoch 62/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9448\n",
      "Epoch 00062: loss did not improve\n",
      "100/100 [==============================] - 43s 428ms/step - loss: 0.0309 - acc: 0.9447\n",
      "Epoch 63/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9413\n",
      "Epoch 00063: loss did not improve\n",
      "100/100 [==============================] - 43s 430ms/step - loss: 0.0328 - acc: 0.9411\n",
      "Epoch 64/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9459\n",
      "Epoch 00064: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0304 - acc: 0.9460\n",
      "Epoch 65/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9436\n",
      "Epoch 00065: loss did not improve\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.0317 - acc: 0.9433\n",
      "Epoch 66/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9430\n",
      "Epoch 00066: loss did not improve\n",
      "100/100 [==============================] - 43s 433ms/step - loss: 0.0315 - acc: 0.9429\n",
      "Epoch 67/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9420\n",
      "Epoch 00067: loss did not improve\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.0322 - acc: 0.9416\n",
      "Epoch 68/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9424\n",
      "Epoch 00068: loss did not improve\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.0328 - acc: 0.9425\n",
      "Epoch 69/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9445\n",
      "Epoch 00069: loss did not improve\n",
      "100/100 [==============================] - 44s 436ms/step - loss: 0.0309 - acc: 0.9447\n",
      "Epoch 70/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9427\n",
      "Epoch 00070: loss did not improve\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.0318 - acc: 0.9427\n",
      "Epoch 71/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9440\n",
      "Epoch 00071: loss did not improve\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.0311 - acc: 0.9439\n",
      "Epoch 72/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9452\n",
      "Epoch 00072: loss did not improve\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.0309 - acc: 0.9453\n",
      "Epoch 73/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9482\n",
      "Epoch 00073: loss improved from 0.03002 to 0.02930, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 44s 435ms/step - loss: 0.0293 - acc: 0.9482\n",
      "Epoch 74/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9431\n",
      "Epoch 00074: loss did not improve\n",
      "100/100 [==============================] - 44s 437ms/step - loss: 0.0320 - acc: 0.9433\n",
      "Epoch 75/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9392\n",
      "Epoch 00075: loss did not improve\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.0337 - acc: 0.9393\n",
      "Epoch 76/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9427\n",
      "Epoch 00076: loss did not improve\n",
      "100/100 [==============================] - 43s 430ms/step - loss: 0.0320 - acc: 0.9429\n",
      "Epoch 77/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9425\n",
      "Epoch 00077: loss did not improve\n",
      "100/100 [==============================] - 43s 429ms/step - loss: 0.0325 - acc: 0.9424\n",
      "Epoch 78/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9439\n",
      "Epoch 00078: loss did not improve\n",
      "100/100 [==============================] - 44s 436ms/step - loss: 0.0313 - acc: 0.9439\n",
      "Epoch 79/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9446\n",
      "Epoch 00079: loss did not improve\n",
      "100/100 [==============================] - 44s 437ms/step - loss: 0.0311 - acc: 0.9447\n",
      "Epoch 80/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9417\n",
      "Epoch 00080: loss did not improve\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.0324 - acc: 0.9418\n",
      "Epoch 81/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9421\n",
      "Epoch 00081: loss did not improve\n",
      "100/100 [==============================] - 43s 433ms/step - loss: 0.0327 - acc: 0.9419\n",
      "Epoch 82/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9421\n",
      "Epoch 00082: loss did not improve\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.0320 - acc: 0.9421\n",
      "Epoch 83/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9438\n",
      "Epoch 00083: loss did not improve\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.0315 - acc: 0.9440\n",
      "Epoch 84/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9417\n",
      "Epoch 00084: loss did not improve\n",
      "100/100 [==============================] - 43s 432ms/step - loss: 0.0327 - acc: 0.9417\n",
      "Epoch 85/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9405\n",
      "Epoch 00085: loss did not improve\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0333 - acc: 0.9406\n",
      "Epoch 86/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9408\n",
      "Epoch 00086: loss did not improve\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0332 - acc: 0.9405\n",
      "Epoch 87/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9414\n",
      "Epoch 00087: loss did not improve\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.0328 - acc: 0.9415\n",
      "Epoch 88/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9447\n",
      "Epoch 00088: loss did not improve\n",
      "100/100 [==============================] - 43s 434ms/step - loss: 0.0312 - acc: 0.9448\n",
      "Epoch 89/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9442\n",
      "Epoch 00089: loss did not improve\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.0313 - acc: 0.9442\n",
      "Epoch 90/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9452\n",
      "Epoch 00090: loss did not improve\n",
      "100/100 [==============================] - 43s 428ms/step - loss: 0.0306 - acc: 0.9453\n",
      "Epoch 91/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9403\n",
      "Epoch 00091: loss did not improve\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.0336 - acc: 0.9403\n",
      "Epoch 92/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9441\n",
      "Epoch 00092: loss did not improve\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0313 - acc: 0.9442\n",
      "Epoch 93/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9437\n",
      "Epoch 00093: loss did not improve\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.0313 - acc: 0.9438\n",
      "Epoch 94/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9440\n",
      "Epoch 00094: loss did not improve\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.0317 - acc: 0.9440\n",
      "Epoch 95/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9438\n",
      "Epoch 00095: loss did not improve\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.0318 - acc: 0.9440\n",
      "Epoch 96/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9426\n",
      "Epoch 00096: loss did not improve\n",
      "100/100 [==============================] - 43s 432ms/step - loss: 0.0323 - acc: 0.9426\n",
      "Epoch 97/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9443\n",
      "Epoch 00097: loss did not improve\n",
      "100/100 [==============================] - 43s 435ms/step - loss: 0.0313 - acc: 0.9443\n",
      "Epoch 98/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9422\n",
      "Epoch 00098: loss did not improve\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.0322 - acc: 0.9423\n",
      "Epoch 99/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9433\n",
      "Epoch 00099: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0319 - acc: 0.9436\n",
      "Epoch 100/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9430\n",
      "Epoch 00100: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0317 - acc: 0.9431\n",
      "Epoch 101/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9430\n",
      "Epoch 00101: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0318 - acc: 0.9430\n",
      "Epoch 102/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9412\n",
      "Epoch 00102: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0329 - acc: 0.9410\n",
      "Epoch 103/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9458\n",
      "Epoch 00103: loss did not improve\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0302 - acc: 0.9457\n",
      "Epoch 104/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9413\n",
      "Epoch 00104: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0326 - acc: 0.9415\n",
      "Epoch 105/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9416\n",
      "Epoch 00105: loss did not improve\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0325 - acc: 0.9418\n",
      "Epoch 106/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9400\n",
      "Epoch 00106: loss did not improve\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.0328 - acc: 0.9401\n",
      "Epoch 107/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9425\n",
      "Epoch 00107: loss did not improve\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.0322 - acc: 0.9426\n",
      "Epoch 108/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9438\n",
      "Epoch 00108: loss did not improve\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.0312 - acc: 0.9438\n",
      "Epoch 109/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9448\n",
      "Epoch 00109: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0308 - acc: 0.9449\n",
      "Epoch 110/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9430\n",
      "Epoch 00110: loss did not improve\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.0317 - acc: 0.9430\n",
      "Epoch 111/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9447\n",
      "Epoch 00111: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0306 - acc: 0.9447\n",
      "Epoch 112/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9440\n",
      "Epoch 00112: loss did not improve\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.0311 - acc: 0.9442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9408\n",
      "Epoch 00113: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0327 - acc: 0.9409\n",
      "Epoch 114/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9379\n",
      "Epoch 00114: loss did not improve\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.0343 - acc: 0.9378\n",
      "Epoch 115/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9439\n",
      "Epoch 00115: loss did not improve\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.0313 - acc: 0.9439\n",
      "Epoch 116/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9445\n",
      "Epoch 00116: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0308 - acc: 0.9446\n",
      "Epoch 117/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9435\n",
      "Epoch 00117: loss did not improve\n",
      "100/100 [==============================] - 43s 435ms/step - loss: 0.0322 - acc: 0.9433\n",
      "Epoch 118/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9433\n",
      "Epoch 00118: loss did not improve\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.0319 - acc: 0.9433\n",
      "Epoch 119/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9445\n",
      "Epoch 00119: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0305 - acc: 0.9446\n",
      "Epoch 120/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9462\n",
      "Epoch 00120: loss did not improve\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.0300 - acc: 0.9463\n",
      "Epoch 121/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9420\n",
      "Epoch 00121: loss did not improve\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.0325 - acc: 0.9421\n",
      "Epoch 122/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9427\n",
      "Epoch 00122: loss did not improve\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.0317 - acc: 0.9429\n",
      "Epoch 123/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9422\n",
      "Epoch 00123: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0324 - acc: 0.9424\n",
      "Epoch 124/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9468\n",
      "Epoch 00124: loss did not improve\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.0299 - acc: 0.9468\n",
      "Epoch 125/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9448\n",
      "Epoch 00125: loss did not improve\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.0313 - acc: 0.9448\n",
      "Epoch 126/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9418\n",
      "Epoch 00126: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0326 - acc: 0.9417\n",
      "Epoch 127/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9399\n",
      "Epoch 00127: loss did not improve\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.0333 - acc: 0.9400\n",
      "Epoch 128/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9405\n",
      "Epoch 00128: loss did not improve\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0334 - acc: 0.9405\n",
      "Epoch 129/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9431\n",
      "Epoch 00129: loss did not improve\n",
      "100/100 [==============================] - 45s 448ms/step - loss: 0.0315 - acc: 0.9432\n",
      "Epoch 130/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9426\n",
      "Epoch 00130: loss did not improve\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0317 - acc: 0.9427\n",
      "Epoch 131/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9445\n",
      "Epoch 00131: loss did not improve\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0315 - acc: 0.9445\n",
      "Epoch 132/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9465\n",
      "Epoch 00132: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0298 - acc: 0.9465\n",
      "Epoch 133/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9412\n",
      "Epoch 00133: loss did not improve\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.0329 - acc: 0.9413\n",
      "Epoch 134/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9427\n",
      "Epoch 00134: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0321 - acc: 0.9428\n",
      "Epoch 135/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9411\n",
      "Epoch 00135: loss did not improve\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0331 - acc: 0.9410\n",
      "Epoch 136/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9425\n",
      "Epoch 00136: loss did not improve\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0325 - acc: 0.9426\n",
      "Epoch 137/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9425\n",
      "Epoch 00137: loss did not improve\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0321 - acc: 0.9424\n",
      "Epoch 138/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9448\n",
      "Epoch 00138: loss did not improve\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0308 - acc: 0.9449\n",
      "Epoch 139/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9473\n",
      "Epoch 00139: loss did not improve\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.0293 - acc: 0.9473\n",
      "Epoch 140/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9440\n",
      "Epoch 00140: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0312 - acc: 0.9441\n",
      "Epoch 141/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9412\n",
      "Epoch 00141: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0331 - acc: 0.9413\n",
      "Epoch 142/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9439\n",
      "Epoch 00142: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0317 - acc: 0.9441\n",
      "Epoch 143/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9462\n",
      "Epoch 00143: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0302 - acc: 0.9462\n",
      "Epoch 144/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9422\n",
      "Epoch 00144: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0324 - acc: 0.9421\n",
      "Epoch 145/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9418\n",
      "Epoch 00145: loss did not improve\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.0322 - acc: 0.9420\n",
      "Epoch 146/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9450\n",
      "Epoch 00146: loss did not improve\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.0304 - acc: 0.9451\n",
      "Epoch 147/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9426\n",
      "Epoch 00147: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0325 - acc: 0.9426\n",
      "Epoch 148/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9463\n",
      "Epoch 00148: loss did not improve\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0304 - acc: 0.9459\n",
      "Epoch 149/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9467\n",
      "Epoch 00149: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0299 - acc: 0.9466\n",
      "Epoch 150/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9421\n",
      "Epoch 00150: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0324 - acc: 0.9421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9431\n",
      "Epoch 00151: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0318 - acc: 0.9429\n",
      "Epoch 152/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9451\n",
      "Epoch 00152: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0308 - acc: 0.9452\n",
      "Epoch 153/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9426\n",
      "Epoch 00153: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0321 - acc: 0.9424\n",
      "Epoch 154/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9426\n",
      "Epoch 00154: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0322 - acc: 0.9427\n",
      "Epoch 155/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9429\n",
      "Epoch 00155: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0316 - acc: 0.9429\n",
      "Epoch 156/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9450\n",
      "Epoch 00156: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0302 - acc: 0.9451\n",
      "Epoch 157/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9467\n",
      "Epoch 00157: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0300 - acc: 0.9468\n",
      "Epoch 158/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9436\n",
      "Epoch 00158: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0314 - acc: 0.9436\n",
      "Epoch 159/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9468\n",
      "Epoch 00159: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0302 - acc: 0.9464\n",
      "Epoch 160/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9429\n",
      "Epoch 00160: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0324 - acc: 0.9430\n",
      "Epoch 161/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9424\n",
      "Epoch 00161: loss did not improve\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.0320 - acc: 0.9425\n",
      "Epoch 162/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9435\n",
      "Epoch 00162: loss did not improve\n",
      "100/100 [==============================] - 45s 448ms/step - loss: 0.0316 - acc: 0.9436\n",
      "Epoch 163/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9440\n",
      "Epoch 00163: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0311 - acc: 0.9441\n",
      "Epoch 164/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9462\n",
      "Epoch 00164: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0304 - acc: 0.9461\n",
      "Epoch 165/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9435\n",
      "Epoch 00165: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0319 - acc: 0.9434\n",
      "Epoch 166/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9432\n",
      "Epoch 00166: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0318 - acc: 0.9432\n",
      "Epoch 167/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9415\n",
      "Epoch 00167: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0329 - acc: 0.9418\n",
      "Epoch 168/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9448\n",
      "Epoch 00168: loss did not improve\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.0310 - acc: 0.9449\n",
      "Epoch 169/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9438\n",
      "Epoch 00169: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0316 - acc: 0.9439\n",
      "Epoch 170/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9388\n",
      "Epoch 00170: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0351 - acc: 0.9388\n",
      "Epoch 171/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9451\n",
      "Epoch 00171: loss did not improve\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.0310 - acc: 0.9450\n",
      "Epoch 172/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9432\n",
      "Epoch 00172: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0315 - acc: 0.9432\n",
      "Epoch 173/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9478\n",
      "Epoch 00173: loss improved from 0.02930 to 0.02923, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0292 - acc: 0.9477\n",
      "Epoch 174/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9458\n",
      "Epoch 00174: loss did not improve\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.0303 - acc: 0.9459\n",
      "Epoch 175/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9427\n",
      "Epoch 00175: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0319 - acc: 0.9429\n",
      "Epoch 176/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9438\n",
      "Epoch 00176: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0316 - acc: 0.9438\n",
      "Epoch 177/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9403\n",
      "Epoch 00177: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0333 - acc: 0.9405\n",
      "Epoch 178/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9467\n",
      "Epoch 00178: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0298 - acc: 0.9467\n",
      "Epoch 179/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9415\n",
      "Epoch 00179: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0328 - acc: 0.9415\n",
      "Epoch 180/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9407\n",
      "Epoch 00180: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0327 - acc: 0.9408\n",
      "Epoch 181/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9467\n",
      "Epoch 00181: loss did not improve\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.0295 - acc: 0.9467\n",
      "Epoch 182/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9439\n",
      "Epoch 00182: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0313 - acc: 0.9439\n",
      "Epoch 183/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9439\n",
      "Epoch 00183: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0310 - acc: 0.9441\n",
      "Epoch 184/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9418\n",
      "Epoch 00184: loss did not improve\n",
      "100/100 [==============================] - 45s 455ms/step - loss: 0.0325 - acc: 0.9419\n",
      "Epoch 185/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9417\n",
      "Epoch 00185: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0321 - acc: 0.9419\n",
      "Epoch 186/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9429\n",
      "Epoch 00186: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0316 - acc: 0.9427\n",
      "Epoch 187/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9446\n",
      "Epoch 00187: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0309 - acc: 0.9447\n",
      "Epoch 188/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9424\n",
      "Epoch 00188: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0323 - acc: 0.9424\n",
      "Epoch 189/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9421\n",
      "Epoch 00189: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0321 - acc: 0.9421\n",
      "Epoch 190/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9425\n",
      "Epoch 00190: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0323 - acc: 0.9425\n",
      "Epoch 191/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9470\n",
      "Epoch 00191: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0300 - acc: 0.9472\n",
      "Epoch 192/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9451\n",
      "Epoch 00192: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0308 - acc: 0.9452\n",
      "Epoch 193/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9442\n",
      "Epoch 00193: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0316 - acc: 0.9443\n",
      "Epoch 194/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9458\n",
      "Epoch 00194: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0307 - acc: 0.9456\n",
      "Epoch 195/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9467\n",
      "Epoch 00195: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0299 - acc: 0.9468\n",
      "Epoch 196/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9444\n",
      "Epoch 00196: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0309 - acc: 0.9445\n",
      "Epoch 197/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9456\n",
      "Epoch 00197: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0308 - acc: 0.9456\n",
      "Epoch 198/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9446\n",
      "Epoch 00198: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0308 - acc: 0.9447\n",
      "Epoch 199/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9470\n",
      "Epoch 00199: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0295 - acc: 0.9472\n",
      "Epoch 200/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9437\n",
      "Epoch 00200: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0316 - acc: 0.9438\n",
      "Epoch 201/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9423\n",
      "Epoch 00201: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0323 - acc: 0.9423\n",
      "Epoch 202/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9428\n",
      "Epoch 00202: loss did not improve\n",
      "100/100 [==============================] - 45s 455ms/step - loss: 0.0319 - acc: 0.9429\n",
      "Epoch 203/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9472\n",
      "Epoch 00203: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0296 - acc: 0.9471\n",
      "Epoch 204/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9414\n",
      "Epoch 00204: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0329 - acc: 0.9414\n",
      "Epoch 205/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9460\n",
      "Epoch 00205: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0307 - acc: 0.9459\n",
      "Epoch 206/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9434\n",
      "Epoch 00206: loss did not improve\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.0314 - acc: 0.9434\n",
      "Epoch 207/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9429\n",
      "Epoch 00207: loss did not improve\n",
      "100/100 [==============================] - 46s 455ms/step - loss: 0.0323 - acc: 0.9428\n",
      "Epoch 208/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9416\n",
      "Epoch 00208: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0327 - acc: 0.9418\n",
      "Epoch 209/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9447\n",
      "Epoch 00209: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0311 - acc: 0.9446\n",
      "Epoch 210/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9453\n",
      "Epoch 00210: loss did not improve\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.0308 - acc: 0.9452\n",
      "Epoch 211/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9452\n",
      "Epoch 00211: loss did not improve\n",
      "100/100 [==============================] - 45s 455ms/step - loss: 0.0309 - acc: 0.9449\n",
      "Epoch 212/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9458\n",
      "Epoch 00212: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0302 - acc: 0.9458\n",
      "Epoch 213/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9448\n",
      "Epoch 00213: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0307 - acc: 0.9449\n",
      "Epoch 214/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9430\n",
      "Epoch 00214: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0318 - acc: 0.9430\n",
      "Epoch 215/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9448\n",
      "Epoch 00215: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0310 - acc: 0.9445\n",
      "Epoch 216/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9425\n",
      "Epoch 00216: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0325 - acc: 0.9427\n",
      "Epoch 217/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9445\n",
      "Epoch 00217: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0308 - acc: 0.9446\n",
      "Epoch 218/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9466\n",
      "Epoch 00218: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0297 - acc: 0.9464\n",
      "Epoch 219/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9483\n",
      "Epoch 00219: loss improved from 0.02923 to 0.02882, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0288 - acc: 0.9484\n",
      "Epoch 220/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9446\n",
      "Epoch 00220: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0306 - acc: 0.9448\n",
      "Epoch 221/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9450\n",
      "Epoch 00221: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0312 - acc: 0.9446\n",
      "Epoch 222/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9458\n",
      "Epoch 00222: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0304 - acc: 0.9460\n",
      "Epoch 223/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9449\n",
      "Epoch 00223: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0309 - acc: 0.9444\n",
      "Epoch 224/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9395\n",
      "Epoch 00224: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0336 - acc: 0.9396\n",
      "Epoch 225/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9424\n",
      "Epoch 00225: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0324 - acc: 0.9423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9465\n",
      "Epoch 00226: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0300 - acc: 0.9465\n",
      "Epoch 227/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9455\n",
      "Epoch 00227: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0306 - acc: 0.9454\n",
      "Epoch 228/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9426\n",
      "Epoch 00228: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0319 - acc: 0.9424\n",
      "Epoch 229/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9464\n",
      "Epoch 00229: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0299 - acc: 0.9464\n",
      "Epoch 230/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9470\n",
      "Epoch 00230: loss did not improve\n",
      "100/100 [==============================] - 46s 455ms/step - loss: 0.0293 - acc: 0.9471\n",
      "Epoch 231/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9440\n",
      "Epoch 00231: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0309 - acc: 0.9440\n",
      "Epoch 232/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9428\n",
      "Epoch 00232: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0318 - acc: 0.9429\n",
      "Epoch 233/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9433\n",
      "Epoch 00233: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0314 - acc: 0.9435\n",
      "Epoch 234/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9426\n",
      "Epoch 00234: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0316 - acc: 0.9426\n",
      "Epoch 235/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9430\n",
      "Epoch 00235: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0317 - acc: 0.9432\n",
      "Epoch 236/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9458\n",
      "Epoch 00236: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0300 - acc: 0.9460\n",
      "Epoch 237/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9402\n",
      "Epoch 00237: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0332 - acc: 0.9403\n",
      "Epoch 238/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9418\n",
      "Epoch 00238: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0320 - acc: 0.9421\n",
      "Epoch 239/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9440\n",
      "Epoch 00239: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0314 - acc: 0.9439\n",
      "Epoch 240/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9443\n",
      "Epoch 00240: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0313 - acc: 0.9443\n",
      "Epoch 241/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9451\n",
      "Epoch 00241: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0303 - acc: 0.9450\n",
      "Epoch 242/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9475\n",
      "Epoch 00242: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0296 - acc: 0.9476\n",
      "Epoch 243/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9428\n",
      "Epoch 00243: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0318 - acc: 0.9429\n",
      "Epoch 244/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9429\n",
      "Epoch 00244: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0321 - acc: 0.9431\n",
      "Epoch 245/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9458\n",
      "Epoch 00245: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0303 - acc: 0.9458\n",
      "Epoch 246/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9451\n",
      "Epoch 00246: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0305 - acc: 0.9451\n",
      "Epoch 247/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9438\n",
      "Epoch 00247: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0316 - acc: 0.9440\n",
      "Epoch 248/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9429\n",
      "Epoch 00248: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0319 - acc: 0.9432\n",
      "Epoch 249/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9451\n",
      "Epoch 00249: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0307 - acc: 0.9452\n",
      "Epoch 250/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9475\n",
      "Epoch 00250: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0294 - acc: 0.9476\n",
      "Epoch 251/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9461\n",
      "Epoch 00251: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0299 - acc: 0.9460\n",
      "Epoch 252/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9456\n",
      "Epoch 00252: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0302 - acc: 0.9455\n",
      "Epoch 253/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9487\n",
      "Epoch 00253: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0291 - acc: 0.9486\n",
      "Epoch 254/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9456\n",
      "Epoch 00254: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0303 - acc: 0.9456\n",
      "Epoch 255/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9441\n",
      "Epoch 00255: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0312 - acc: 0.9440\n",
      "Epoch 256/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9442\n",
      "Epoch 00256: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0311 - acc: 0.9442\n",
      "Epoch 257/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9471\n",
      "Epoch 00257: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0294 - acc: 0.9472\n",
      "Epoch 258/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9455\n",
      "Epoch 00258: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0306 - acc: 0.9456\n",
      "Epoch 259/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9429\n",
      "Epoch 00259: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0319 - acc: 0.9428\n",
      "Epoch 260/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9425\n",
      "Epoch 00260: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0312 - acc: 0.9426\n",
      "Epoch 261/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9451\n",
      "Epoch 00261: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0304 - acc: 0.9452\n",
      "Epoch 262/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9408\n",
      "Epoch 00262: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0334 - acc: 0.9410\n",
      "Epoch 263/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9441\n",
      "Epoch 00263: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0311 - acc: 0.9442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 264/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9455\n",
      "Epoch 00264: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0306 - acc: 0.9456\n",
      "Epoch 265/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9469\n",
      "Epoch 00265: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0302 - acc: 0.9470\n",
      "Epoch 266/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9447\n",
      "Epoch 00266: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0305 - acc: 0.9448\n",
      "Epoch 267/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9439\n",
      "Epoch 00267: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0310 - acc: 0.9441\n",
      "Epoch 268/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9431\n",
      "Epoch 00268: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0316 - acc: 0.9431\n",
      "Epoch 269/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9435\n",
      "Epoch 00269: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0312 - acc: 0.9437\n",
      "Epoch 270/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9457\n",
      "Epoch 00270: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0300 - acc: 0.9457\n",
      "Epoch 271/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9428\n",
      "Epoch 00271: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0323 - acc: 0.9429\n",
      "Epoch 272/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9453\n",
      "Epoch 00272: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0308 - acc: 0.9450\n",
      "Epoch 273/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9457\n",
      "Epoch 00273: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0304 - acc: 0.9456\n",
      "Epoch 274/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9454\n",
      "Epoch 00274: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0307 - acc: 0.9454\n",
      "Epoch 275/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9445\n",
      "Epoch 00275: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0310 - acc: 0.9444\n",
      "Epoch 276/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9395\n",
      "Epoch 00276: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0346 - acc: 0.9393\n",
      "Epoch 277/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9446\n",
      "Epoch 00277: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0313 - acc: 0.9445\n",
      "Epoch 278/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9466\n",
      "Epoch 00278: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0299 - acc: 0.9466\n",
      "Epoch 279/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9409\n",
      "Epoch 00279: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0329 - acc: 0.9410\n",
      "Epoch 280/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9449\n",
      "Epoch 00280: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0311 - acc: 0.9447\n",
      "Epoch 281/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9442\n",
      "Epoch 00281: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0310 - acc: 0.9443\n",
      "Epoch 282/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9463\n",
      "Epoch 00282: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0302 - acc: 0.9462\n",
      "Epoch 283/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9458\n",
      "Epoch 00283: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0300 - acc: 0.9458\n",
      "Epoch 284/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9447\n",
      "Epoch 00284: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0310 - acc: 0.9448\n",
      "Epoch 285/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9466\n",
      "Epoch 00285: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0300 - acc: 0.9468\n",
      "Epoch 286/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9452\n",
      "Epoch 00286: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0306 - acc: 0.9454\n",
      "Epoch 287/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9459\n",
      "Epoch 00287: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0303 - acc: 0.9459\n",
      "Epoch 288/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9478\n",
      "Epoch 00288: loss improved from 0.02882 to 0.02880, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0288 - acc: 0.9478\n",
      "Epoch 289/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9479\n",
      "Epoch 00289: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0292 - acc: 0.9479\n",
      "Epoch 290/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9444\n",
      "Epoch 00290: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0311 - acc: 0.9446\n",
      "Epoch 291/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9430\n",
      "Epoch 00291: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0320 - acc: 0.9431\n",
      "Epoch 292/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9445\n",
      "Epoch 00292: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0310 - acc: 0.9444\n",
      "Epoch 293/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9450\n",
      "Epoch 00293: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0309 - acc: 0.9447\n",
      "Epoch 294/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9443\n",
      "Epoch 00294: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0314 - acc: 0.9438\n",
      "Epoch 295/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9429\n",
      "Epoch 00295: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0315 - acc: 0.9430\n",
      "Epoch 296/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9456\n",
      "Epoch 00296: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0301 - acc: 0.9457\n",
      "Epoch 297/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9449\n",
      "Epoch 00297: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0306 - acc: 0.9449\n",
      "Epoch 298/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9464\n",
      "Epoch 00298: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0300 - acc: 0.9467\n",
      "Epoch 299/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9450\n",
      "Epoch 00299: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0304 - acc: 0.9451\n",
      "Epoch 300/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9432\n",
      "Epoch 00300: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0310 - acc: 0.9435\n",
      "Epoch 301/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9430\n",
      "Epoch 00301: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0318 - acc: 0.9432\n",
      "Epoch 302/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9457\n",
      "Epoch 00302: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0299 - acc: 0.9459\n",
      "Epoch 303/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9453\n",
      "Epoch 00303: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0304 - acc: 0.9454\n",
      "Epoch 304/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9444\n",
      "Epoch 00304: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0314 - acc: 0.9436\n",
      "Epoch 305/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9451\n",
      "Epoch 00305: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0306 - acc: 0.9447\n",
      "Epoch 306/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9475\n",
      "Epoch 00306: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0293 - acc: 0.9475\n",
      "Epoch 307/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9478\n",
      "Epoch 00307: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0295 - acc: 0.9479\n",
      "Epoch 308/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9457\n",
      "Epoch 00308: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0303 - acc: 0.9457\n",
      "Epoch 309/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9455\n",
      "Epoch 00309: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0304 - acc: 0.9455\n",
      "Epoch 310/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9475\n",
      "Epoch 00310: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0293 - acc: 0.9476\n",
      "Epoch 311/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9477\n",
      "Epoch 00311: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0289 - acc: 0.9477\n",
      "Epoch 312/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9474\n",
      "Epoch 00312: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0294 - acc: 0.9474\n",
      "Epoch 313/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9448\n",
      "Epoch 00313: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0307 - acc: 0.9450\n",
      "Epoch 314/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9462\n",
      "Epoch 00314: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0298 - acc: 0.9463\n",
      "Epoch 315/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9440\n",
      "Epoch 00315: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0313 - acc: 0.9435\n",
      "Epoch 316/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9467\n",
      "Epoch 00316: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0300 - acc: 0.9467\n",
      "Epoch 317/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9461\n",
      "Epoch 00317: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0300 - acc: 0.9462\n",
      "Epoch 318/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9465\n",
      "Epoch 00318: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0301 - acc: 0.9466\n",
      "Epoch 319/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9455\n",
      "Epoch 00319: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0300 - acc: 0.9453\n",
      "Epoch 320/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9474\n",
      "Epoch 00320: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0298 - acc: 0.9471\n",
      "Epoch 321/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9458\n",
      "Epoch 00321: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0305 - acc: 0.9458\n",
      "Epoch 322/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9430\n",
      "Epoch 00322: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0317 - acc: 0.9431\n",
      "Epoch 323/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9428\n",
      "Epoch 00323: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0325 - acc: 0.9429\n",
      "Epoch 324/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9433\n",
      "Epoch 00324: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0313 - acc: 0.9433\n",
      "Epoch 325/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9495\n",
      "Epoch 00325: loss improved from 0.02880 to 0.02838, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0284 - acc: 0.9490\n",
      "Epoch 326/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9456\n",
      "Epoch 00326: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0310 - acc: 0.9455\n",
      "Epoch 327/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9435\n",
      "Epoch 00327: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0316 - acc: 0.9438\n",
      "Epoch 328/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9413\n",
      "Epoch 00328: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0324 - acc: 0.9415\n",
      "Epoch 329/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9438\n",
      "Epoch 00329: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0315 - acc: 0.9439\n",
      "Epoch 330/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9452\n",
      "Epoch 00330: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0304 - acc: 0.9453\n",
      "Epoch 331/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9461\n",
      "Epoch 00331: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0302 - acc: 0.9460\n",
      "Epoch 332/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9447\n",
      "Epoch 00332: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0311 - acc: 0.9447\n",
      "Epoch 333/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9448\n",
      "Epoch 00333: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0309 - acc: 0.9447\n",
      "Epoch 334/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9446\n",
      "Epoch 00334: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0312 - acc: 0.9446\n",
      "Epoch 335/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9432\n",
      "Epoch 00335: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0314 - acc: 0.9433\n",
      "Epoch 336/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9465\n",
      "Epoch 00336: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0297 - acc: 0.9466\n",
      "Epoch 337/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9445\n",
      "Epoch 00337: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0304 - acc: 0.9447\n",
      "Epoch 338/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9461\n",
      "Epoch 00338: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0299 - acc: 0.9458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 339/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9471\n",
      "Epoch 00339: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0294 - acc: 0.9468\n",
      "Epoch 340/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9428\n",
      "Epoch 00340: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0319 - acc: 0.9429\n",
      "Epoch 341/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9456\n",
      "Epoch 00341: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0305 - acc: 0.9454\n",
      "Epoch 342/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9457\n",
      "Epoch 00342: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0301 - acc: 0.9455\n",
      "Epoch 343/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9446\n",
      "Epoch 00343: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0307 - acc: 0.9447\n",
      "Epoch 344/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9427\n",
      "Epoch 00344: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0318 - acc: 0.9429\n",
      "Epoch 345/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9451\n",
      "Epoch 00345: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0306 - acc: 0.9453\n",
      "Epoch 346/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9452\n",
      "Epoch 00346: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0305 - acc: 0.9455\n",
      "Epoch 347/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9459\n",
      "Epoch 00347: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0300 - acc: 0.9459\n",
      "Epoch 348/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9466\n",
      "Epoch 00348: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0297 - acc: 0.9466\n",
      "Epoch 349/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9447\n",
      "Epoch 00349: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0305 - acc: 0.9447\n",
      "Epoch 350/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9434\n",
      "Epoch 00350: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0318 - acc: 0.9434\n",
      "Epoch 351/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9450\n",
      "Epoch 00351: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0302 - acc: 0.9450\n",
      "Epoch 352/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9455\n",
      "Epoch 00352: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0305 - acc: 0.9456\n",
      "Epoch 353/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9466\n",
      "Epoch 00353: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0296 - acc: 0.9466\n",
      "Epoch 354/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9455\n",
      "Epoch 00354: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0299 - acc: 0.9456\n",
      "Epoch 355/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9462\n",
      "Epoch 00355: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0302 - acc: 0.9463\n",
      "Epoch 356/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9429\n",
      "Epoch 00356: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0318 - acc: 0.9431\n",
      "Epoch 357/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9450\n",
      "Epoch 00357: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0306 - acc: 0.9451\n",
      "Epoch 358/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9456\n",
      "Epoch 00358: loss did not improve\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.0305 - acc: 0.9457\n",
      "Epoch 359/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9445\n",
      "Epoch 00359: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0310 - acc: 0.9446\n",
      "Epoch 360/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9452\n",
      "Epoch 00360: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0305 - acc: 0.9453\n",
      "Epoch 361/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9445\n",
      "Epoch 00361: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0315 - acc: 0.9445\n",
      "Epoch 362/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9449\n",
      "Epoch 00362: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0310 - acc: 0.9450\n",
      "Epoch 363/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9460\n",
      "Epoch 00363: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0299 - acc: 0.9461\n",
      "Epoch 364/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9469\n",
      "Epoch 00364: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0293 - acc: 0.9468\n",
      "Epoch 365/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9479\n",
      "Epoch 00365: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0290 - acc: 0.9479\n",
      "Epoch 366/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9444\n",
      "Epoch 00366: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0314 - acc: 0.9446\n",
      "Epoch 367/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9454\n",
      "Epoch 00367: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0305 - acc: 0.9454\n",
      "Epoch 368/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9486\n",
      "Epoch 00368: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0286 - acc: 0.9487\n",
      "Epoch 369/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9463\n",
      "Epoch 00369: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0295 - acc: 0.9464\n",
      "Epoch 370/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9467\n",
      "Epoch 00370: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0299 - acc: 0.9468\n",
      "Epoch 371/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9449\n",
      "Epoch 00371: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0302 - acc: 0.9448\n",
      "Epoch 372/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9444\n",
      "Epoch 00372: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0307 - acc: 0.9444\n",
      "Epoch 373/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9452\n",
      "Epoch 00373: loss did not improve\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.0305 - acc: 0.9454\n",
      "Epoch 374/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9426\n",
      "Epoch 00374: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0318 - acc: 0.9426\n",
      "Epoch 375/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9471\n",
      "Epoch 00375: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0292 - acc: 0.9472\n",
      "Epoch 376/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9473\n",
      "Epoch 00376: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0290 - acc: 0.9471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 377/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9475\n",
      "Epoch 00377: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0290 - acc: 0.9475\n",
      "Epoch 378/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9432\n",
      "Epoch 00378: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0317 - acc: 0.9430\n",
      "Epoch 379/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9471\n",
      "Epoch 00379: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0291 - acc: 0.9471\n",
      "Epoch 380/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9420\n",
      "Epoch 00380: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0327 - acc: 0.9418\n",
      "Epoch 381/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9441\n",
      "Epoch 00381: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0313 - acc: 0.9442\n",
      "Epoch 382/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9457\n",
      "Epoch 00382: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0305 - acc: 0.9456\n",
      "Epoch 383/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9443\n",
      "Epoch 00383: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0310 - acc: 0.9445\n",
      "Epoch 384/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9438\n",
      "Epoch 00384: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0312 - acc: 0.9436\n",
      "Epoch 385/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9467\n",
      "Epoch 00385: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0298 - acc: 0.9468\n",
      "Epoch 386/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9450\n",
      "Epoch 00386: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0307 - acc: 0.9450\n",
      "Epoch 387/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9463\n",
      "Epoch 00387: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0297 - acc: 0.9462\n",
      "Epoch 388/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9488\n",
      "Epoch 00388: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0289 - acc: 0.9489\n",
      "Epoch 389/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9472\n",
      "Epoch 00389: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0293 - acc: 0.9471\n",
      "Epoch 390/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9452\n",
      "Epoch 00390: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0304 - acc: 0.9453\n",
      "Epoch 391/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9483\n",
      "Epoch 00391: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0286 - acc: 0.9482\n",
      "Epoch 392/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9439\n",
      "Epoch 00392: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0316 - acc: 0.9438\n",
      "Epoch 393/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9448\n",
      "Epoch 00393: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0308 - acc: 0.9447\n",
      "Epoch 394/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9434\n",
      "Epoch 00394: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0317 - acc: 0.9436\n",
      "Epoch 395/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9420\n",
      "Epoch 00395: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0318 - acc: 0.9420\n",
      "Epoch 396/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9456\n",
      "Epoch 00396: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0300 - acc: 0.9459\n",
      "Epoch 397/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9441\n",
      "Epoch 00397: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0309 - acc: 0.9440\n",
      "Epoch 398/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9460\n",
      "Epoch 00398: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0301 - acc: 0.9459\n",
      "Epoch 399/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9438\n",
      "Epoch 00399: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0315 - acc: 0.9439\n",
      "Epoch 400/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9457\n",
      "Epoch 00400: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0303 - acc: 0.9458\n",
      "Epoch 401/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9476\n",
      "Epoch 00401: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0291 - acc: 0.9476\n",
      "Epoch 402/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9466\n",
      "Epoch 00402: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0297 - acc: 0.9467\n",
      "Epoch 403/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9472\n",
      "Epoch 00403: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0294 - acc: 0.9471\n",
      "Epoch 404/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9482\n",
      "Epoch 00404: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0288 - acc: 0.9483\n",
      "Epoch 405/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9483\n",
      "Epoch 00405: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0285 - acc: 0.9484\n",
      "Epoch 406/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9454\n",
      "Epoch 00406: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0304 - acc: 0.9456\n",
      "Epoch 407/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9451\n",
      "Epoch 00407: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0305 - acc: 0.9451\n",
      "Epoch 408/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9458\n",
      "Epoch 00408: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0301 - acc: 0.9459\n",
      "Epoch 409/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9422\n",
      "Epoch 00409: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0326 - acc: 0.9422\n",
      "Epoch 410/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9449\n",
      "Epoch 00410: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0309 - acc: 0.9446\n",
      "Epoch 411/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9446\n",
      "Epoch 00411: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0304 - acc: 0.9447\n",
      "Epoch 412/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9465\n",
      "Epoch 00412: loss did not improve\n",
      "100/100 [==============================] - 49s 490ms/step - loss: 0.0296 - acc: 0.9463\n",
      "Epoch 413/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9466\n",
      "Epoch 00413: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0295 - acc: 0.9466\n",
      "Epoch 414/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9462\n",
      "Epoch 00414: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0298 - acc: 0.9463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 415/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9466\n",
      "Epoch 00415: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0294 - acc: 0.9468\n",
      "Epoch 416/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9472\n",
      "Epoch 00416: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0295 - acc: 0.9471\n",
      "Epoch 417/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9462\n",
      "Epoch 00417: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0300 - acc: 0.9460\n",
      "Epoch 418/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9447\n",
      "Epoch 00418: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0306 - acc: 0.9448\n",
      "Epoch 419/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9462\n",
      "Epoch 00419: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0296 - acc: 0.9462\n",
      "Epoch 420/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9431\n",
      "Epoch 00420: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0321 - acc: 0.9432\n",
      "Epoch 421/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9467\n",
      "Epoch 00421: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0294 - acc: 0.9469\n",
      "Epoch 422/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9449\n",
      "Epoch 00422: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0301 - acc: 0.9449\n",
      "Epoch 423/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9438\n",
      "Epoch 00423: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0311 - acc: 0.9432\n",
      "Epoch 424/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9438\n",
      "Epoch 00424: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0310 - acc: 0.9438\n",
      "Epoch 425/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9450\n",
      "Epoch 00425: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0308 - acc: 0.9449\n",
      "Epoch 426/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9451\n",
      "Epoch 00426: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0305 - acc: 0.9453\n",
      "Epoch 427/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9462\n",
      "Epoch 00427: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0297 - acc: 0.9464\n",
      "Epoch 428/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9451\n",
      "Epoch 00428: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0306 - acc: 0.9448\n",
      "Epoch 429/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9470\n",
      "Epoch 00429: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0295 - acc: 0.9470\n",
      "Epoch 430/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9457\n",
      "Epoch 00430: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0297 - acc: 0.9458\n",
      "Epoch 431/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9453\n",
      "Epoch 00431: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0309 - acc: 0.9450\n",
      "Epoch 432/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9467\n",
      "Epoch 00432: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0296 - acc: 0.9467\n",
      "Epoch 433/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9451\n",
      "Epoch 00433: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0305 - acc: 0.9452\n",
      "Epoch 434/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9440\n",
      "Epoch 00434: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0307 - acc: 0.9439\n",
      "Epoch 435/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9470\n",
      "Epoch 00435: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0296 - acc: 0.9469\n",
      "Epoch 436/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9455\n",
      "Epoch 00436: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0308 - acc: 0.9456\n",
      "Epoch 437/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 00437: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 438/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9433\n",
      "Epoch 00438: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0316 - acc: 0.9432\n",
      "Epoch 439/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9453\n",
      "Epoch 00439: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0307 - acc: 0.9453\n",
      "Epoch 440/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9450\n",
      "Epoch 00440: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0306 - acc: 0.9452\n",
      "Epoch 441/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9478\n",
      "Epoch 00441: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0287 - acc: 0.9477\n",
      "Epoch 442/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9482\n",
      "Epoch 00442: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0286 - acc: 0.9484\n",
      "Epoch 443/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9479\n",
      "Epoch 00443: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0286 - acc: 0.9480\n",
      "Epoch 444/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9462\n",
      "Epoch 00444: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0296 - acc: 0.9463\n",
      "Epoch 445/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9462\n",
      "Epoch 00445: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0302 - acc: 0.9462\n",
      "Epoch 446/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9458\n",
      "Epoch 00446: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0302 - acc: 0.9459\n",
      "Epoch 447/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9468\n",
      "Epoch 00447: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0294 - acc: 0.9469\n",
      "Epoch 448/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9482\n",
      "Epoch 00448: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0285 - acc: 0.9483\n",
      "Epoch 449/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9444\n",
      "Epoch 00449: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0311 - acc: 0.9444\n",
      "Epoch 450/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9449\n",
      "Epoch 00450: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0306 - acc: 0.9450\n",
      "Epoch 451/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9465\n",
      "Epoch 00451: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0300 - acc: 0.9464\n",
      "Epoch 452/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9480\n",
      "Epoch 00452: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0286 - acc: 0.9481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 453/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9462\n",
      "Epoch 00453: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0300 - acc: 0.9462\n",
      "Epoch 454/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9454\n",
      "Epoch 00454: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0302 - acc: 0.9453\n",
      "Epoch 455/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9466\n",
      "Epoch 00455: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0300 - acc: 0.9465\n",
      "Epoch 456/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9483\n",
      "Epoch 00456: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0286 - acc: 0.9485\n",
      "Epoch 457/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9461\n",
      "Epoch 00457: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0300 - acc: 0.9460\n",
      "Epoch 458/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9450\n",
      "Epoch 00458: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0308 - acc: 0.9450\n",
      "Epoch 459/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9443\n",
      "Epoch 00459: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0312 - acc: 0.9440\n",
      "Epoch 460/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9447\n",
      "Epoch 00460: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0308 - acc: 0.9446\n",
      "Epoch 461/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9461\n",
      "Epoch 00461: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0295 - acc: 0.9462\n",
      "Epoch 462/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9486\n",
      "Epoch 00462: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0284 - acc: 0.9487\n",
      "Epoch 463/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9462\n",
      "Epoch 00463: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0299 - acc: 0.9460\n",
      "Epoch 464/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9448\n",
      "Epoch 00464: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0310 - acc: 0.9449\n",
      "Epoch 465/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9484\n",
      "Epoch 00465: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0285 - acc: 0.9485\n",
      "Epoch 466/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9459\n",
      "Epoch 00466: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0304 - acc: 0.9454\n",
      "Epoch 467/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9494\n",
      "Epoch 00467: loss improved from 0.02838 to 0.02829, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0283 - acc: 0.9493\n",
      "Epoch 468/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9464\n",
      "Epoch 00468: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0302 - acc: 0.9465\n",
      "Epoch 469/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9474\n",
      "Epoch 00469: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0288 - acc: 0.9476\n",
      "Epoch 470/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9482\n",
      "Epoch 00470: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0288 - acc: 0.9483\n",
      "Epoch 471/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9455\n",
      "Epoch 00471: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0301 - acc: 0.9455\n",
      "Epoch 472/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9494\n",
      "Epoch 00472: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0284 - acc: 0.9492\n",
      "Epoch 473/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9445\n",
      "Epoch 00473: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0306 - acc: 0.9444\n",
      "Epoch 474/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9442\n",
      "Epoch 00474: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0310 - acc: 0.9443\n",
      "Epoch 475/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9444\n",
      "Epoch 00475: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0312 - acc: 0.9444\n",
      "Epoch 476/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9463\n",
      "Epoch 00476: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0298 - acc: 0.9461\n",
      "Epoch 477/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9467\n",
      "Epoch 00477: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0293 - acc: 0.9469\n",
      "Epoch 478/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9462\n",
      "Epoch 00478: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0299 - acc: 0.9461\n",
      "Epoch 479/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9459\n",
      "Epoch 00479: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0299 - acc: 0.9459\n",
      "Epoch 480/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9454\n",
      "Epoch 00480: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0305 - acc: 0.9449\n",
      "Epoch 481/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9469\n",
      "Epoch 00481: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0303 - acc: 0.9468\n",
      "Epoch 482/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9471\n",
      "Epoch 00482: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0299 - acc: 0.9469\n",
      "Epoch 483/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9456\n",
      "Epoch 00483: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0303 - acc: 0.9452\n",
      "Epoch 484/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9428\n",
      "Epoch 00484: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0321 - acc: 0.9426\n",
      "Epoch 485/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9462\n",
      "Epoch 00485: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0299 - acc: 0.9460\n",
      "Epoch 486/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9479\n",
      "Epoch 00486: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0293 - acc: 0.9477\n",
      "Epoch 487/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9468\n",
      "Epoch 00487: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0296 - acc: 0.9469\n",
      "Epoch 488/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9434\n",
      "Epoch 00488: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0309 - acc: 0.9436\n",
      "Epoch 489/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9436\n",
      "Epoch 00489: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0312 - acc: 0.9437\n",
      "Epoch 490/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9467\n",
      "Epoch 00490: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0293 - acc: 0.9467\n",
      "Epoch 491/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9464\n",
      "Epoch 00491: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0299 - acc: 0.9462\n",
      "Epoch 492/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9456\n",
      "Epoch 00492: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0302 - acc: 0.9455\n",
      "Epoch 493/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9442\n",
      "Epoch 00493: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0313 - acc: 0.9443\n",
      "Epoch 494/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9439\n",
      "Epoch 00494: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0311 - acc: 0.9439\n",
      "Epoch 495/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9449\n",
      "Epoch 00495: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0307 - acc: 0.9448\n",
      "Epoch 496/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9481\n",
      "Epoch 00496: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0290 - acc: 0.9482\n",
      "Epoch 497/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9451\n",
      "Epoch 00497: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0306 - acc: 0.9451\n",
      "Epoch 498/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9463\n",
      "Epoch 00498: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0297 - acc: 0.9462\n",
      "Epoch 499/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9485\n",
      "Epoch 00499: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0290 - acc: 0.9479\n",
      "Epoch 500/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9477\n",
      "Epoch 00500: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0290 - acc: 0.9477\n",
      "Epoch 501/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9478\n",
      "Epoch 00501: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0287 - acc: 0.9480\n",
      "Epoch 502/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9463\n",
      "Epoch 00502: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0298 - acc: 0.9464\n",
      "Epoch 503/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9472\n",
      "Epoch 00503: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0295 - acc: 0.9472\n",
      "Epoch 504/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9425\n",
      "Epoch 00504: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0317 - acc: 0.9427\n",
      "Epoch 505/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9444\n",
      "Epoch 00505: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0307 - acc: 0.9444\n",
      "Epoch 506/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9463\n",
      "Epoch 00506: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0303 - acc: 0.9461\n",
      "Epoch 507/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9470\n",
      "Epoch 00507: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0297 - acc: 0.9471\n",
      "Epoch 508/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9460\n",
      "Epoch 00508: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0301 - acc: 0.9457\n",
      "Epoch 509/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9445\n",
      "Epoch 00509: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0309 - acc: 0.9446\n",
      "Epoch 510/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9475\n",
      "Epoch 00510: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0292 - acc: 0.9475\n",
      "Epoch 511/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9482\n",
      "Epoch 00511: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0290 - acc: 0.9482\n",
      "Epoch 512/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9458\n",
      "Epoch 00512: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0301 - acc: 0.9457\n",
      "Epoch 513/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9458\n",
      "Epoch 00513: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0305 - acc: 0.9459\n",
      "Epoch 514/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9456\n",
      "Epoch 00514: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0301 - acc: 0.9456\n",
      "Epoch 515/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9485\n",
      "Epoch 00515: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0286 - acc: 0.9483\n",
      "Epoch 516/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9480\n",
      "Epoch 00516: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0288 - acc: 0.9481\n",
      "Epoch 517/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9444\n",
      "Epoch 00517: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0314 - acc: 0.9444\n",
      "Epoch 518/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9451\n",
      "Epoch 00518: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0306 - acc: 0.9452\n",
      "Epoch 519/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9445\n",
      "Epoch 00519: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0304 - acc: 0.9445\n",
      "Epoch 520/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9416\n",
      "Epoch 00520: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0325 - acc: 0.9416\n",
      "Epoch 521/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9489\n",
      "Epoch 00521: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0286 - acc: 0.9488\n",
      "Epoch 522/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9465\n",
      "Epoch 00522: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0298 - acc: 0.9466\n",
      "Epoch 523/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9450\n",
      "Epoch 00523: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0307 - acc: 0.9451\n",
      "Epoch 524/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9451\n",
      "Epoch 00524: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0307 - acc: 0.9449\n",
      "Epoch 525/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9466\n",
      "Epoch 00525: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0297 - acc: 0.9467\n",
      "Epoch 526/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9468\n",
      "Epoch 00526: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0295 - acc: 0.9468\n",
      "Epoch 527/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9450\n",
      "Epoch 00527: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0306 - acc: 0.9449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 528/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9471\n",
      "Epoch 00528: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0297 - acc: 0.9473\n",
      "Epoch 529/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9460\n",
      "Epoch 00529: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0300 - acc: 0.9460\n",
      "Epoch 530/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9470\n",
      "Epoch 00530: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0294 - acc: 0.9470\n",
      "Epoch 531/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9482\n",
      "Epoch 00531: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0286 - acc: 0.9484\n",
      "Epoch 532/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9444\n",
      "Epoch 00532: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0306 - acc: 0.9445\n",
      "Epoch 533/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9503\n",
      "Epoch 00533: loss improved from 0.02829 to 0.02808, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0281 - acc: 0.9504\n",
      "Epoch 534/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9452\n",
      "Epoch 00534: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0304 - acc: 0.9453\n",
      "Epoch 535/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9447\n",
      "Epoch 00535: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0303 - acc: 0.9450\n",
      "Epoch 536/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9468\n",
      "Epoch 00536: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0293 - acc: 0.9469\n",
      "Epoch 537/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9470\n",
      "Epoch 00537: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0289 - acc: 0.9471\n",
      "Epoch 538/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9459\n",
      "Epoch 00538: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0303 - acc: 0.9458\n",
      "Epoch 539/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9418\n",
      "Epoch 00539: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0327 - acc: 0.9414\n",
      "Epoch 540/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 00540: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0292 - acc: 0.9468\n",
      "Epoch 541/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9454\n",
      "Epoch 00541: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0302 - acc: 0.9453\n",
      "Epoch 542/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9461\n",
      "Epoch 00542: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0298 - acc: 0.9462\n",
      "Epoch 543/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9478\n",
      "Epoch 00543: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0293 - acc: 0.9478\n",
      "Epoch 544/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9487\n",
      "Epoch 00544: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0287 - acc: 0.9486\n",
      "Epoch 545/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9474\n",
      "Epoch 00545: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0293 - acc: 0.9475\n",
      "Epoch 546/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9469\n",
      "Epoch 00546: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0295 - acc: 0.9470\n",
      "Epoch 547/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9454\n",
      "Epoch 00547: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0302 - acc: 0.9455\n",
      "Epoch 548/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9473\n",
      "Epoch 00548: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0294 - acc: 0.9473\n",
      "Epoch 549/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9468\n",
      "Epoch 00549: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0293 - acc: 0.9469\n",
      "Epoch 550/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9472\n",
      "Epoch 00550: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0296 - acc: 0.9472\n",
      "Epoch 551/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9476\n",
      "Epoch 00551: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0291 - acc: 0.9475\n",
      "Epoch 552/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9429\n",
      "Epoch 00552: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0316 - acc: 0.9430\n",
      "Epoch 553/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9476\n",
      "Epoch 00553: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0292 - acc: 0.9476\n",
      "Epoch 554/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9477\n",
      "Epoch 00554: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0292 - acc: 0.9479\n",
      "Epoch 555/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9474\n",
      "Epoch 00555: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0291 - acc: 0.9473\n",
      "Epoch 556/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9478\n",
      "Epoch 00556: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0288 - acc: 0.9478\n",
      "Epoch 557/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9440\n",
      "Epoch 00557: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0312 - acc: 0.9441\n",
      "Epoch 558/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9463\n",
      "Epoch 00558: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0302 - acc: 0.9460\n",
      "Epoch 559/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9480\n",
      "Epoch 00559: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0290 - acc: 0.9481\n",
      "Epoch 560/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9468\n",
      "Epoch 00560: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0293 - acc: 0.9468\n",
      "Epoch 561/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9468\n",
      "Epoch 00561: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0298 - acc: 0.9469\n",
      "Epoch 562/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9446\n",
      "Epoch 00562: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0304 - acc: 0.9443\n",
      "Epoch 563/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9467\n",
      "Epoch 00563: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0298 - acc: 0.9467\n",
      "Epoch 564/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9465\n",
      "Epoch 00564: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0292 - acc: 0.9465\n",
      "Epoch 565/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9476\n",
      "Epoch 00565: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0286 - acc: 0.9478\n",
      "Epoch 566/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9467\n",
      "Epoch 00566: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0295 - acc: 0.9466\n",
      "Epoch 567/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9459\n",
      "Epoch 00567: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0298 - acc: 0.9458\n",
      "Epoch 568/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9461\n",
      "Epoch 00568: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0300 - acc: 0.9460\n",
      "Epoch 569/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9474\n",
      "Epoch 00569: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0296 - acc: 0.9475\n",
      "Epoch 570/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9497\n",
      "Epoch 00570: loss did not improve\n",
      "100/100 [==============================] - 41s 414ms/step - loss: 0.0281 - acc: 0.9497\n",
      "Epoch 571/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9464\n",
      "Epoch 00571: loss did not improve\n",
      "100/100 [==============================] - 39s 386ms/step - loss: 0.0301 - acc: 0.9466\n",
      "Epoch 572/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9484\n",
      "Epoch 00572: loss did not improve\n",
      "100/100 [==============================] - 39s 386ms/step - loss: 0.0288 - acc: 0.9484\n",
      "Epoch 573/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9465\n",
      "Epoch 00573: loss did not improve\n",
      "100/100 [==============================] - 39s 386ms/step - loss: 0.0298 - acc: 0.9465\n",
      "Epoch 574/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9478\n",
      "Epoch 00574: loss did not improve\n",
      "100/100 [==============================] - 39s 385ms/step - loss: 0.0292 - acc: 0.9480\n",
      "Epoch 575/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9463\n",
      "Epoch 00575: loss did not improve\n",
      "100/100 [==============================] - 38s 385ms/step - loss: 0.0301 - acc: 0.9462\n",
      "Epoch 576/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9460\n",
      "Epoch 00576: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0300 - acc: 0.9462\n",
      "Epoch 577/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9489\n",
      "Epoch 00577: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0283 - acc: 0.9489\n",
      "Epoch 578/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9488\n",
      "Epoch 00578: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0283 - acc: 0.9488\n",
      "Epoch 579/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9487\n",
      "Epoch 00579: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0283 - acc: 0.9486\n",
      "Epoch 580/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9472\n",
      "Epoch 00580: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0298 - acc: 0.9469\n",
      "Epoch 581/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9476\n",
      "Epoch 00581: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0288 - acc: 0.9478\n",
      "Epoch 582/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9486\n",
      "Epoch 00582: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0285 - acc: 0.9487\n",
      "Epoch 583/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9463\n",
      "Epoch 00583: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0300 - acc: 0.9464\n",
      "Epoch 584/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9466\n",
      "Epoch 00584: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0295 - acc: 0.9466\n",
      "Epoch 585/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9445\n",
      "Epoch 00585: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0305 - acc: 0.9445\n",
      "Epoch 586/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9480\n",
      "Epoch 00586: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 587/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9474\n",
      "Epoch 00587: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0291 - acc: 0.9473\n",
      "Epoch 588/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9475\n",
      "Epoch 00588: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0293 - acc: 0.9475\n",
      "Epoch 589/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9490\n",
      "Epoch 00589: loss improved from 0.02808 to 0.02784, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 39s 387ms/step - loss: 0.0278 - acc: 0.9492\n",
      "Epoch 590/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9470\n",
      "Epoch 00590: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0294 - acc: 0.9470\n",
      "Epoch 591/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9443\n",
      "Epoch 00591: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0305 - acc: 0.9444\n",
      "Epoch 592/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9460\n",
      "Epoch 00592: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0297 - acc: 0.9461\n",
      "Epoch 593/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9485\n",
      "Epoch 00593: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0283 - acc: 0.9485\n",
      "Epoch 594/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9462\n",
      "Epoch 00594: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0300 - acc: 0.9463\n",
      "Epoch 595/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9473\n",
      "Epoch 00595: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0292 - acc: 0.9470\n",
      "Epoch 596/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9446\n",
      "Epoch 00596: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0308 - acc: 0.9446\n",
      "Epoch 597/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9453\n",
      "Epoch 00597: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0301 - acc: 0.9453\n",
      "Epoch 598/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9464\n",
      "Epoch 00598: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0294 - acc: 0.9466\n",
      "Epoch 599/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9481\n",
      "Epoch 00599: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0288 - acc: 0.9479\n",
      "Epoch 600/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9448\n",
      "Epoch 00600: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0305 - acc: 0.9450\n",
      "Epoch 601/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9450\n",
      "Epoch 00601: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0307 - acc: 0.9448\n",
      "Epoch 602/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9472\n",
      "Epoch 00602: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0293 - acc: 0.9473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 603/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9486\n",
      "Epoch 00603: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0283 - acc: 0.9486\n",
      "Epoch 604/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9456\n",
      "Epoch 00604: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0301 - acc: 0.9454\n",
      "Epoch 605/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9444\n",
      "Epoch 00605: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0305 - acc: 0.9444\n",
      "Epoch 606/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9445\n",
      "Epoch 00606: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0309 - acc: 0.9446\n",
      "Epoch 607/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9480\n",
      "Epoch 00607: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0288 - acc: 0.9480\n",
      "Epoch 608/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9478\n",
      "Epoch 00608: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0290 - acc: 0.9479\n",
      "Epoch 609/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9476\n",
      "Epoch 00609: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0287 - acc: 0.9477\n",
      "Epoch 610/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9493\n",
      "Epoch 00610: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0283 - acc: 0.9491\n",
      "Epoch 611/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9486\n",
      "Epoch 00611: loss did not improve\n",
      "100/100 [==============================] - 38s 385ms/step - loss: 0.0287 - acc: 0.9486\n",
      "Epoch 612/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9473\n",
      "Epoch 00612: loss did not improve\n",
      "100/100 [==============================] - 40s 402ms/step - loss: 0.0289 - acc: 0.9474\n",
      "Epoch 613/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9489\n",
      "Epoch 00613: loss did not improve\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.0280 - acc: 0.9490\n",
      "Epoch 614/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9480\n",
      "Epoch 00614: loss did not improve\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.0286 - acc: 0.9481\n",
      "Epoch 615/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9506\n",
      "Epoch 00615: loss improved from 0.02784 to 0.02776, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 43s 433ms/step - loss: 0.0278 - acc: 0.9506\n",
      "Epoch 616/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9479\n",
      "Epoch 00616: loss did not improve\n",
      "100/100 [==============================] - 41s 410ms/step - loss: 0.0289 - acc: 0.9477\n",
      "Epoch 617/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9473\n",
      "Epoch 00617: loss did not improve\n",
      "100/100 [==============================] - 40s 402ms/step - loss: 0.0289 - acc: 0.9473\n",
      "Epoch 618/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 00618: loss did not improve\n",
      "100/100 [==============================] - 41s 414ms/step - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 619/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9455\n",
      "Epoch 00619: loss did not improve\n",
      "100/100 [==============================] - 41s 408ms/step - loss: 0.0300 - acc: 0.9456\n",
      "Epoch 620/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9493\n",
      "Epoch 00620: loss did not improve\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.0279 - acc: 0.9492\n",
      "Epoch 621/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9466\n",
      "Epoch 00621: loss did not improve\n",
      "100/100 [==============================] - 40s 402ms/step - loss: 0.0306 - acc: 0.9466\n",
      "Epoch 622/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9469\n",
      "Epoch 00622: loss did not improve\n",
      "100/100 [==============================] - 42s 420ms/step - loss: 0.0298 - acc: 0.9469\n",
      "Epoch 623/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9479\n",
      "Epoch 00623: loss did not improve\n",
      "100/100 [==============================] - 41s 409ms/step - loss: 0.0283 - acc: 0.9479\n",
      "Epoch 624/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9464\n",
      "Epoch 00624: loss did not improve\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.0299 - acc: 0.9464\n",
      "Epoch 625/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9473\n",
      "Epoch 00625: loss did not improve\n",
      "100/100 [==============================] - 41s 405ms/step - loss: 0.0293 - acc: 0.9473\n",
      "Epoch 626/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9462\n",
      "Epoch 00626: loss did not improve\n",
      "100/100 [==============================] - 41s 413ms/step - loss: 0.0299 - acc: 0.9462\n",
      "Epoch 627/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9452\n",
      "Epoch 00627: loss did not improve\n",
      "100/100 [==============================] - 41s 411ms/step - loss: 0.0304 - acc: 0.9452\n",
      "Epoch 628/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9463\n",
      "Epoch 00628: loss did not improve\n",
      "100/100 [==============================] - 40s 403ms/step - loss: 0.0295 - acc: 0.9462\n",
      "Epoch 629/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9471\n",
      "Epoch 00629: loss did not improve\n",
      "100/100 [==============================] - 41s 412ms/step - loss: 0.0291 - acc: 0.9472\n",
      "Epoch 630/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9460\n",
      "Epoch 00630: loss did not improve\n",
      "100/100 [==============================] - 41s 413ms/step - loss: 0.0303 - acc: 0.9460\n",
      "Epoch 631/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9450\n",
      "Epoch 00631: loss did not improve\n",
      "100/100 [==============================] - 42s 419ms/step - loss: 0.0300 - acc: 0.9451\n",
      "Epoch 632/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9463\n",
      "Epoch 00632: loss did not improve\n",
      "100/100 [==============================] - 42s 416ms/step - loss: 0.0299 - acc: 0.9463\n",
      "Epoch 633/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9469\n",
      "Epoch 00633: loss did not improve\n",
      "100/100 [==============================] - 42s 419ms/step - loss: 0.0292 - acc: 0.9468\n",
      "Epoch 634/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9484\n",
      "Epoch 00634: loss did not improve\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.0289 - acc: 0.9486\n",
      "Epoch 635/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9474\n",
      "Epoch 00635: loss did not improve\n",
      "100/100 [==============================] - 42s 418ms/step - loss: 0.0289 - acc: 0.9475\n",
      "Epoch 636/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9468\n",
      "Epoch 00636: loss did not improve\n",
      "100/100 [==============================] - 43s 432ms/step - loss: 0.0292 - acc: 0.9468\n",
      "Epoch 637/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9447\n",
      "Epoch 00637: loss did not improve\n",
      "100/100 [==============================] - 43s 433ms/step - loss: 0.0303 - acc: 0.9447\n",
      "Epoch 638/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9481\n",
      "Epoch 00638: loss did not improve\n",
      "100/100 [==============================] - 43s 433ms/step - loss: 0.0287 - acc: 0.9480\n",
      "Epoch 639/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 00639: loss did not improve\n",
      "100/100 [==============================] - 43s 430ms/step - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 640/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9494\n",
      "Epoch 00640: loss did not improve\n",
      "100/100 [==============================] - 43s 429ms/step - loss: 0.0284 - acc: 0.9495\n",
      "Epoch 641/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9482\n",
      "Epoch 00641: loss did not improve\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.0284 - acc: 0.9482\n",
      "Epoch 642/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9495\n",
      "Epoch 00642: loss did not improve\n",
      "100/100 [==============================] - 43s 425ms/step - loss: 0.0280 - acc: 0.9491\n",
      "Epoch 643/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9477\n",
      "Epoch 00643: loss did not improve\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.0290 - acc: 0.9476\n",
      "Epoch 644/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9447\n",
      "Epoch 00644: loss did not improve\n",
      "100/100 [==============================] - 43s 426ms/step - loss: 0.0299 - acc: 0.9449\n",
      "Epoch 645/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9437\n",
      "Epoch 00645: loss did not improve\n",
      "100/100 [==============================] - 43s 433ms/step - loss: 0.0316 - acc: 0.9438\n",
      "Epoch 646/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9476\n",
      "Epoch 00646: loss did not improve\n",
      "100/100 [==============================] - 43s 430ms/step - loss: 0.0292 - acc: 0.9475\n",
      "Epoch 647/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9469\n",
      "Epoch 00647: loss did not improve\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 648/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9469\n",
      "Epoch 00648: loss did not improve\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0294 - acc: 0.9467\n",
      "Epoch 649/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9431\n",
      "Epoch 00649: loss did not improve\n",
      "100/100 [==============================] - 43s 435ms/step - loss: 0.0308 - acc: 0.9434\n",
      "Epoch 650/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9433\n",
      "Epoch 00650: loss did not improve\n",
      "100/100 [==============================] - 44s 437ms/step - loss: 0.0307 - acc: 0.9435\n",
      "Epoch 651/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9462\n",
      "Epoch 00651: loss did not improve\n",
      "100/100 [==============================] - 45s 448ms/step - loss: 0.0296 - acc: 0.9462\n",
      "Epoch 652/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9460\n",
      "Epoch 00652: loss did not improve\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.0297 - acc: 0.9460\n",
      "Epoch 653/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9440\n",
      "Epoch 00653: loss did not improve\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.0310 - acc: 0.9441\n",
      "Epoch 654/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9462\n",
      "Epoch 00654: loss did not improve\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.0299 - acc: 0.9461\n",
      "Epoch 655/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9450\n",
      "Epoch 00655: loss did not improve\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0297 - acc: 0.9449\n",
      "Epoch 656/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9440\n",
      "Epoch 00656: loss did not improve\n",
      "100/100 [==============================] - 43s 434ms/step - loss: 0.0313 - acc: 0.9441\n",
      "Epoch 657/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9464\n",
      "Epoch 00657: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0293 - acc: 0.9465\n",
      "Epoch 658/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9469\n",
      "Epoch 00658: loss did not improve\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.0297 - acc: 0.9467\n",
      "Epoch 659/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9466\n",
      "Epoch 00659: loss did not improve\n",
      "100/100 [==============================] - 43s 428ms/step - loss: 0.0293 - acc: 0.9466\n",
      "Epoch 660/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9463\n",
      "Epoch 00660: loss did not improve\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.0300 - acc: 0.9459\n",
      "Epoch 661/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9464\n",
      "Epoch 00661: loss did not improve\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.0301 - acc: 0.9456\n",
      "Epoch 662/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9456\n",
      "Epoch 00662: loss did not improve\n",
      "100/100 [==============================] - 44s 436ms/step - loss: 0.0298 - acc: 0.9456\n",
      "Epoch 663/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9466\n",
      "Epoch 00663: loss did not improve\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.0295 - acc: 0.9468\n",
      "Epoch 664/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9471\n",
      "Epoch 00664: loss did not improve\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.0295 - acc: 0.9473\n",
      "Epoch 665/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9454\n",
      "Epoch 00665: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0302 - acc: 0.9450\n",
      "Epoch 666/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9466\n",
      "Epoch 00666: loss did not improve\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.0291 - acc: 0.9467\n",
      "Epoch 667/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9454\n",
      "Epoch 00667: loss did not improve\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.0304 - acc: 0.9453\n",
      "Epoch 668/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9481\n",
      "Epoch 00668: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0290 - acc: 0.9478\n",
      "Epoch 669/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9465\n",
      "Epoch 00669: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0294 - acc: 0.9466\n",
      "Epoch 670/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9439\n",
      "Epoch 00670: loss did not improve\n",
      "100/100 [==============================] - 45s 448ms/step - loss: 0.0314 - acc: 0.9438\n",
      "Epoch 671/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9477\n",
      "Epoch 00671: loss did not improve\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.0288 - acc: 0.9478\n",
      "Epoch 672/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9466\n",
      "Epoch 00672: loss did not improve\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0292 - acc: 0.9468\n",
      "Epoch 673/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9482\n",
      "Epoch 00673: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0288 - acc: 0.9482\n",
      "Epoch 674/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9441\n",
      "Epoch 00674: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0307 - acc: 0.9435\n",
      "Epoch 675/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9409\n",
      "Epoch 00675: loss did not improve\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0323 - acc: 0.9410\n",
      "Epoch 676/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9436\n",
      "Epoch 00676: loss did not improve\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.0310 - acc: 0.9434\n",
      "Epoch 677/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9470\n",
      "Epoch 00677: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0294 - acc: 0.9469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 678/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9477\n",
      "Epoch 00678: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0292 - acc: 0.9477\n",
      "Epoch 679/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9454\n",
      "Epoch 00679: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0299 - acc: 0.9454\n",
      "Epoch 680/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9492\n",
      "Epoch 00680: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0279 - acc: 0.9491\n",
      "Epoch 681/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9462\n",
      "Epoch 00681: loss did not improve\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0297 - acc: 0.9463\n",
      "Epoch 682/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9464\n",
      "Epoch 00682: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0292 - acc: 0.9465\n",
      "Epoch 683/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9439\n",
      "Epoch 00683: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0309 - acc: 0.9438\n",
      "Epoch 684/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9463\n",
      "Epoch 00684: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0294 - acc: 0.9464\n",
      "Epoch 685/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9491\n",
      "Epoch 00685: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0282 - acc: 0.9490\n",
      "Epoch 686/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9479\n",
      "Epoch 00686: loss did not improve\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0287 - acc: 0.9479\n",
      "Epoch 687/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9465\n",
      "Epoch 00687: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0298 - acc: 0.9465\n",
      "Epoch 688/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9469\n",
      "Epoch 00688: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0291 - acc: 0.9470\n",
      "Epoch 689/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9484\n",
      "Epoch 00689: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0284 - acc: 0.9483\n",
      "Epoch 690/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9474\n",
      "Epoch 00690: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0297 - acc: 0.9471\n",
      "Epoch 691/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9460\n",
      "Epoch 00691: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0298 - acc: 0.9462\n",
      "Epoch 692/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9466\n",
      "Epoch 00692: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0298 - acc: 0.9466\n",
      "Epoch 693/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9475\n",
      "Epoch 00693: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0293 - acc: 0.9472\n",
      "Epoch 694/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9502\n",
      "Epoch 00694: loss improved from 0.02776 to 0.02745, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0274 - acc: 0.9500\n",
      "Epoch 695/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9458\n",
      "Epoch 00695: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0300 - acc: 0.9459\n",
      "Epoch 696/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9447\n",
      "Epoch 00696: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0306 - acc: 0.9444\n",
      "Epoch 697/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9427\n",
      "Epoch 00697: loss did not improve\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.0317 - acc: 0.9430\n",
      "Epoch 698/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9460\n",
      "Epoch 00698: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0301 - acc: 0.9459\n",
      "Epoch 699/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9471\n",
      "Epoch 00699: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0286 - acc: 0.9472\n",
      "Epoch 700/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9465\n",
      "Epoch 00700: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0308 - acc: 0.9460\n",
      "Epoch 701/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9469\n",
      "Epoch 00701: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0294 - acc: 0.9469\n",
      "Epoch 702/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9473\n",
      "Epoch 00702: loss did not improve\n",
      "100/100 [==============================] - 46s 455ms/step - loss: 0.0296 - acc: 0.9473\n",
      "Epoch 703/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9454\n",
      "Epoch 00703: loss did not improve\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.0303 - acc: 0.9454\n",
      "Epoch 704/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9458\n",
      "Epoch 00704: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0300 - acc: 0.9456\n",
      "Epoch 705/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9446\n",
      "Epoch 00705: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0308 - acc: 0.9446\n",
      "Epoch 706/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9489\n",
      "Epoch 00706: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0290 - acc: 0.9487\n",
      "Epoch 707/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9463\n",
      "Epoch 00707: loss did not improve\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.0295 - acc: 0.9463\n",
      "Epoch 708/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9513\n",
      "Epoch 00708: loss improved from 0.02745 to 0.02693, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0269 - acc: 0.9512\n",
      "Epoch 709/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9487\n",
      "Epoch 00709: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0284 - acc: 0.9486\n",
      "Epoch 710/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9442\n",
      "Epoch 00710: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0307 - acc: 0.9444\n",
      "Epoch 711/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9490\n",
      "Epoch 00711: loss did not improve\n",
      "100/100 [==============================] - 45s 455ms/step - loss: 0.0285 - acc: 0.9491\n",
      "Epoch 712/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9473\n",
      "Epoch 00712: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0291 - acc: 0.9474\n",
      "Epoch 713/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9497\n",
      "Epoch 00713: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0278 - acc: 0.9494\n",
      "Epoch 714/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9473\n",
      "Epoch 00714: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0291 - acc: 0.9473\n",
      "Epoch 715/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9465\n",
      "Epoch 00715: loss did not improve\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.0296 - acc: 0.9462\n",
      "Epoch 716/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9487\n",
      "Epoch 00716: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0284 - acc: 0.9487\n",
      "Epoch 717/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9482\n",
      "Epoch 00717: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0286 - acc: 0.9483\n",
      "Epoch 718/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9470\n",
      "Epoch 00718: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0292 - acc: 0.9469\n",
      "Epoch 719/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9432\n",
      "Epoch 00719: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0309 - acc: 0.9433\n",
      "Epoch 720/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9476\n",
      "Epoch 00720: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0291 - acc: 0.9476\n",
      "Epoch 721/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9466\n",
      "Epoch 00721: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0294 - acc: 0.9465\n",
      "Epoch 722/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9439\n",
      "Epoch 00722: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0307 - acc: 0.9439\n",
      "Epoch 723/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9461\n",
      "Epoch 00723: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0299 - acc: 0.9461\n",
      "Epoch 724/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9457\n",
      "Epoch 00724: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0301 - acc: 0.9457\n",
      "Epoch 725/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9486\n",
      "Epoch 00725: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0281 - acc: 0.9487\n",
      "Epoch 726/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9514\n",
      "Epoch 00726: loss improved from 0.02693 to 0.02685, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0269 - acc: 0.9514\n",
      "Epoch 727/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9489\n",
      "Epoch 00727: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0284 - acc: 0.9490\n",
      "Epoch 728/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9459\n",
      "Epoch 00728: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0295 - acc: 0.9460\n",
      "Epoch 729/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9453\n",
      "Epoch 00729: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0304 - acc: 0.9453\n",
      "Epoch 730/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9464\n",
      "Epoch 00730: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0294 - acc: 0.9464\n",
      "Epoch 731/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9465\n",
      "Epoch 00731: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0295 - acc: 0.9466\n",
      "Epoch 732/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9480\n",
      "Epoch 00732: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0287 - acc: 0.9480\n",
      "Epoch 733/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9492\n",
      "Epoch 00733: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0279 - acc: 0.9492\n",
      "Epoch 734/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9456\n",
      "Epoch 00734: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0306 - acc: 0.9456\n",
      "Epoch 735/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9458\n",
      "Epoch 00735: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0300 - acc: 0.9459\n",
      "Epoch 736/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9462\n",
      "Epoch 00736: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0299 - acc: 0.9460\n",
      "Epoch 737/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9469\n",
      "Epoch 00737: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0293 - acc: 0.9470\n",
      "Epoch 738/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9454\n",
      "Epoch 00738: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0302 - acc: 0.9452\n",
      "Epoch 739/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9461\n",
      "Epoch 00739: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0302 - acc: 0.9461\n",
      "Epoch 740/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9457\n",
      "Epoch 00740: loss did not improve\n",
      "100/100 [==============================] - 45s 455ms/step - loss: 0.0301 - acc: 0.9459\n",
      "Epoch 741/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9462\n",
      "Epoch 00741: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0297 - acc: 0.9462\n",
      "Epoch 742/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9464\n",
      "Epoch 00742: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0297 - acc: 0.9462\n",
      "Epoch 743/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9509\n",
      "Epoch 00743: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0271 - acc: 0.9509\n",
      "Epoch 744/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9449\n",
      "Epoch 00744: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0310 - acc: 0.9449\n",
      "Epoch 745/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9450\n",
      "Epoch 00745: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0303 - acc: 0.9450\n",
      "Epoch 746/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9456\n",
      "Epoch 00746: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0302 - acc: 0.9453\n",
      "Epoch 747/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9476\n",
      "Epoch 00747: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0295 - acc: 0.9475\n",
      "Epoch 748/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9445\n",
      "Epoch 00748: loss did not improve\n",
      "100/100 [==============================] - 46s 455ms/step - loss: 0.0304 - acc: 0.9444\n",
      "Epoch 749/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9455\n",
      "Epoch 00749: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0301 - acc: 0.9456\n",
      "Epoch 750/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9486\n",
      "Epoch 00750: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0283 - acc: 0.9487\n",
      "Epoch 751/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9456\n",
      "Epoch 00751: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0298 - acc: 0.9457\n",
      "Epoch 752/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9456\n",
      "Epoch 00752: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0303 - acc: 0.9456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 753/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9475\n",
      "Epoch 00753: loss did not improve\n",
      "100/100 [==============================] - 45s 455ms/step - loss: 0.0294 - acc: 0.9474\n",
      "Epoch 754/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9498\n",
      "Epoch 00754: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0280 - acc: 0.9498\n",
      "Epoch 755/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9469\n",
      "Epoch 00755: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0293 - acc: 0.9469\n",
      "Epoch 756/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9467\n",
      "Epoch 00756: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0292 - acc: 0.9468\n",
      "Epoch 757/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9453\n",
      "Epoch 00757: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0303 - acc: 0.9451\n",
      "Epoch 758/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9472\n",
      "Epoch 00758: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0292 - acc: 0.9468\n",
      "Epoch 759/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9448\n",
      "Epoch 00759: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0307 - acc: 0.9449\n",
      "Epoch 760/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9499\n",
      "Epoch 00760: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0278 - acc: 0.9499\n",
      "Epoch 761/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9483\n",
      "Epoch 00761: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0286 - acc: 0.9483\n",
      "Epoch 762/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9485\n",
      "Epoch 00762: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0289 - acc: 0.9485\n",
      "Epoch 763/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9450\n",
      "Epoch 00763: loss did not improve\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.0303 - acc: 0.9449\n",
      "Epoch 764/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9492\n",
      "Epoch 00764: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0278 - acc: 0.9491\n",
      "Epoch 765/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9509\n",
      "Epoch 00765: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0271 - acc: 0.9509\n",
      "Epoch 766/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9464\n",
      "Epoch 00766: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0302 - acc: 0.9465\n",
      "Epoch 767/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 00767: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 768/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9493\n",
      "Epoch 00768: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0287 - acc: 0.9491\n",
      "Epoch 769/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9496\n",
      "Epoch 00769: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0277 - acc: 0.9497\n",
      "Epoch 770/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9465\n",
      "Epoch 00770: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0299 - acc: 0.9466\n",
      "Epoch 771/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9440\n",
      "Epoch 00771: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0312 - acc: 0.9441\n",
      "Epoch 772/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9477\n",
      "Epoch 00772: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0289 - acc: 0.9479\n",
      "Epoch 773/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9454\n",
      "Epoch 00773: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0307 - acc: 0.9455\n",
      "Epoch 774/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9500\n",
      "Epoch 00774: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0277 - acc: 0.9500\n",
      "Epoch 775/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9472\n",
      "Epoch 00775: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0295 - acc: 0.9472\n",
      "Epoch 776/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9467\n",
      "Epoch 00776: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0298 - acc: 0.9468\n",
      "Epoch 777/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9464\n",
      "Epoch 00777: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0296 - acc: 0.9465\n",
      "Epoch 778/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9455\n",
      "Epoch 00778: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0301 - acc: 0.9455\n",
      "Epoch 779/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9467\n",
      "Epoch 00779: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0297 - acc: 0.9467\n",
      "Epoch 780/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9470\n",
      "Epoch 00780: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0291 - acc: 0.9471\n",
      "Epoch 781/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9477\n",
      "Epoch 00781: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0291 - acc: 0.9478\n",
      "Epoch 782/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9467\n",
      "Epoch 00782: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0292 - acc: 0.9468\n",
      "Epoch 783/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9437\n",
      "Epoch 00783: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0312 - acc: 0.9434\n",
      "Epoch 784/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9470\n",
      "Epoch 00784: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0290 - acc: 0.9471\n",
      "Epoch 785/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9482\n",
      "Epoch 00785: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0287 - acc: 0.9481\n",
      "Epoch 786/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9460\n",
      "Epoch 00786: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0301 - acc: 0.9460\n",
      "Epoch 787/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9483\n",
      "Epoch 00787: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0282 - acc: 0.9483\n",
      "Epoch 788/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9499\n",
      "Epoch 00788: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0277 - acc: 0.9500\n",
      "Epoch 789/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9488\n",
      "Epoch 00789: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0283 - acc: 0.9488\n",
      "Epoch 790/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9476\n",
      "Epoch 00790: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0289 - acc: 0.9475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 791/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9474\n",
      "Epoch 00791: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0290 - acc: 0.9474\n",
      "Epoch 792/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9462\n",
      "Epoch 00792: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0294 - acc: 0.9463\n",
      "Epoch 793/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9447\n",
      "Epoch 00793: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0307 - acc: 0.9446\n",
      "Epoch 794/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9442\n",
      "Epoch 00794: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0312 - acc: 0.9440\n",
      "Epoch 795/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9495\n",
      "Epoch 00795: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0279 - acc: 0.9496\n",
      "Epoch 796/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9491\n",
      "Epoch 00796: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0278 - acc: 0.9493\n",
      "Epoch 797/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9489\n",
      "Epoch 00797: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0282 - acc: 0.9489\n",
      "Epoch 798/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9460\n",
      "Epoch 00798: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0300 - acc: 0.9461\n",
      "Epoch 799/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9505\n",
      "Epoch 00799: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0271 - acc: 0.9505\n",
      "Epoch 800/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9485\n",
      "Epoch 00800: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0287 - acc: 0.9484\n",
      "Epoch 801/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9447\n",
      "Epoch 00801: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0308 - acc: 0.9446\n",
      "Epoch 802/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9477\n",
      "Epoch 00802: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0292 - acc: 0.9474\n",
      "Epoch 803/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9472\n",
      "Epoch 00803: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0290 - acc: 0.9473\n",
      "Epoch 804/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9450\n",
      "Epoch 00804: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0301 - acc: 0.9452\n",
      "Epoch 805/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9484\n",
      "Epoch 00805: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0286 - acc: 0.9486\n",
      "Epoch 806/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9444\n",
      "Epoch 00806: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0303 - acc: 0.9444\n",
      "Epoch 807/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9425\n",
      "Epoch 00807: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0318 - acc: 0.9427\n",
      "Epoch 808/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9466\n",
      "Epoch 00808: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0291 - acc: 0.9467\n",
      "Epoch 809/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9446\n",
      "Epoch 00809: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0310 - acc: 0.9446\n",
      "Epoch 810/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9482\n",
      "Epoch 00810: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0282 - acc: 0.9484\n",
      "Epoch 811/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9505\n",
      "Epoch 00811: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0272 - acc: 0.9505\n",
      "Epoch 812/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9492\n",
      "Epoch 00812: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0280 - acc: 0.9492\n",
      "Epoch 813/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9451\n",
      "Epoch 00813: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0304 - acc: 0.9453\n",
      "Epoch 814/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9465\n",
      "Epoch 00814: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0295 - acc: 0.9464\n",
      "Epoch 815/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9475\n",
      "Epoch 00815: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0290 - acc: 0.9476\n",
      "Epoch 816/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9503\n",
      "Epoch 00816: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0277 - acc: 0.9504\n",
      "Epoch 817/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9446\n",
      "Epoch 00817: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0304 - acc: 0.9448\n",
      "Epoch 818/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9457\n",
      "Epoch 00818: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0300 - acc: 0.9458\n",
      "Epoch 819/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9488\n",
      "Epoch 00819: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0282 - acc: 0.9489\n",
      "Epoch 820/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9467\n",
      "Epoch 00820: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0294 - acc: 0.9467\n",
      "Epoch 821/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9485\n",
      "Epoch 00821: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0284 - acc: 0.9487\n",
      "Epoch 822/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9499\n",
      "Epoch 00822: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0280 - acc: 0.9500\n",
      "Epoch 823/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9504\n",
      "Epoch 00823: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0276 - acc: 0.9505\n",
      "Epoch 824/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9471\n",
      "Epoch 00824: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0295 - acc: 0.9472\n",
      "Epoch 825/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9479\n",
      "Epoch 00825: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0287 - acc: 0.9475\n",
      "Epoch 826/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9476\n",
      "Epoch 00826: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0286 - acc: 0.9476\n",
      "Epoch 827/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9477\n",
      "Epoch 00827: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0292 - acc: 0.9477\n",
      "Epoch 828/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9466\n",
      "Epoch 00828: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0296 - acc: 0.9466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 829/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9479\n",
      "Epoch 00829: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0285 - acc: 0.9478\n",
      "Epoch 830/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9504\n",
      "Epoch 00830: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0277 - acc: 0.9502\n",
      "Epoch 831/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9423\n",
      "Epoch 00831: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0323 - acc: 0.9423\n",
      "Epoch 832/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9453\n",
      "Epoch 00832: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0305 - acc: 0.9455\n",
      "Epoch 833/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9493\n",
      "Epoch 00833: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0284 - acc: 0.9490\n",
      "Epoch 834/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9484\n",
      "Epoch 00834: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0283 - acc: 0.9485\n",
      "Epoch 835/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9449\n",
      "Epoch 00835: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0307 - acc: 0.9449\n",
      "Epoch 836/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9503\n",
      "Epoch 00836: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0272 - acc: 0.9505\n",
      "Epoch 837/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9471\n",
      "Epoch 00837: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0296 - acc: 0.9472\n",
      "Epoch 838/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9473\n",
      "Epoch 00838: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0291 - acc: 0.9474\n",
      "Epoch 839/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 00839: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0286 - acc: 0.9480\n",
      "Epoch 840/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9480\n",
      "Epoch 00840: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0288 - acc: 0.9479\n",
      "Epoch 841/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9482\n",
      "Epoch 00841: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0286 - acc: 0.9484\n",
      "Epoch 842/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9464\n",
      "Epoch 00842: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0294 - acc: 0.9465\n",
      "Epoch 843/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9508\n",
      "Epoch 00843: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0273 - acc: 0.9507\n",
      "Epoch 844/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9476\n",
      "Epoch 00844: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0290 - acc: 0.9472\n",
      "Epoch 845/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9458\n",
      "Epoch 00845: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0302 - acc: 0.9460\n",
      "Epoch 846/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9492\n",
      "Epoch 00846: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0281 - acc: 0.9490\n",
      "Epoch 847/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9471\n",
      "Epoch 00847: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0290 - acc: 0.9470\n",
      "Epoch 848/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9474\n",
      "Epoch 00848: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0294 - acc: 0.9471\n",
      "Epoch 849/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9475\n",
      "Epoch 00849: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0293 - acc: 0.9477\n",
      "Epoch 850/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9431\n",
      "Epoch 00850: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0316 - acc: 0.9432\n",
      "Epoch 851/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9479\n",
      "Epoch 00851: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0289 - acc: 0.9479\n",
      "Epoch 852/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9479\n",
      "Epoch 00852: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0288 - acc: 0.9480\n",
      "Epoch 853/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 00853: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0284 - acc: 0.9483\n",
      "Epoch 854/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9469\n",
      "Epoch 00854: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0289 - acc: 0.9470\n",
      "Epoch 855/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9499\n",
      "Epoch 00855: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0280 - acc: 0.9501\n",
      "Epoch 856/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9477\n",
      "Epoch 00856: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0286 - acc: 0.9478\n",
      "Epoch 857/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9497\n",
      "Epoch 00857: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0276 - acc: 0.9495\n",
      "Epoch 858/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9512\n",
      "Epoch 00858: loss improved from 0.02685 to 0.02674, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0267 - acc: 0.9514\n",
      "Epoch 859/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9500\n",
      "Epoch 00859: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0276 - acc: 0.9501\n",
      "Epoch 860/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9467\n",
      "Epoch 00860: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0296 - acc: 0.9467\n",
      "Epoch 861/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9475\n",
      "Epoch 00861: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0294 - acc: 0.9475\n",
      "Epoch 862/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9466\n",
      "Epoch 00862: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0293 - acc: 0.9465\n",
      "Epoch 863/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9456\n",
      "Epoch 00863: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0299 - acc: 0.9456\n",
      "Epoch 864/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9470\n",
      "Epoch 00864: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0295 - acc: 0.9469\n",
      "Epoch 865/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9479\n",
      "Epoch 00865: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0287 - acc: 0.9480\n",
      "Epoch 866/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9463\n",
      "Epoch 00866: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0297 - acc: 0.9464\n",
      "Epoch 867/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9456\n",
      "Epoch 00867: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0297 - acc: 0.9456\n",
      "Epoch 868/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9479\n",
      "Epoch 00868: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0286 - acc: 0.9478\n",
      "Epoch 869/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9470\n",
      "Epoch 00869: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0292 - acc: 0.9469\n",
      "Epoch 870/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9477\n",
      "Epoch 00870: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0286 - acc: 0.9476\n",
      "Epoch 871/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9482\n",
      "Epoch 00871: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0286 - acc: 0.9481\n",
      "Epoch 872/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9496\n",
      "Epoch 00872: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0278 - acc: 0.9494\n",
      "Epoch 873/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9469\n",
      "Epoch 00873: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0293 - acc: 0.9470\n",
      "Epoch 874/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9481\n",
      "Epoch 00874: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0295 - acc: 0.9479\n",
      "Epoch 875/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9489\n",
      "Epoch 00875: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0285 - acc: 0.9491\n",
      "Epoch 876/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9501\n",
      "Epoch 00876: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0276 - acc: 0.9502\n",
      "Epoch 877/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9495\n",
      "Epoch 00877: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0282 - acc: 0.9493\n",
      "Epoch 878/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9492\n",
      "Epoch 00878: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0280 - acc: 0.9493\n",
      "Epoch 879/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9493\n",
      "Epoch 00879: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0280 - acc: 0.9493\n",
      "Epoch 880/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9502\n",
      "Epoch 00880: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0272 - acc: 0.9503\n",
      "Epoch 881/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9493\n",
      "Epoch 00881: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0279 - acc: 0.9494\n",
      "Epoch 882/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9483\n",
      "Epoch 00882: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0283 - acc: 0.9483\n",
      "Epoch 883/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9480\n",
      "Epoch 00883: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0284 - acc: 0.9483\n",
      "Epoch 884/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9486\n",
      "Epoch 00884: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0282 - acc: 0.9487\n",
      "Epoch 885/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9463\n",
      "Epoch 00885: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0295 - acc: 0.9465\n",
      "Epoch 886/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9485\n",
      "Epoch 00886: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0283 - acc: 0.9486\n",
      "Epoch 887/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9476\n",
      "Epoch 00887: loss did not improve\n",
      "100/100 [==============================] - 49s 490ms/step - loss: 0.0292 - acc: 0.9477\n",
      "Epoch 888/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9493\n",
      "Epoch 00888: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0279 - acc: 0.9492\n",
      "Epoch 889/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9471\n",
      "Epoch 00889: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0288 - acc: 0.9473\n",
      "Epoch 890/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9480\n",
      "Epoch 00890: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0290 - acc: 0.9481\n",
      "Epoch 891/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9445\n",
      "Epoch 00891: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0304 - acc: 0.9445\n",
      "Epoch 892/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9455\n",
      "Epoch 00892: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0303 - acc: 0.9456\n",
      "Epoch 893/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9478\n",
      "Epoch 00893: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0286 - acc: 0.9477\n",
      "Epoch 894/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9492\n",
      "Epoch 00894: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0284 - acc: 0.9494\n",
      "Epoch 895/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9479\n",
      "Epoch 00895: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0288 - acc: 0.9478\n",
      "Epoch 896/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9471\n",
      "Epoch 00896: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0297 - acc: 0.9471\n",
      "Epoch 897/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9453\n",
      "Epoch 00897: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0302 - acc: 0.9454\n",
      "Epoch 898/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9443\n",
      "Epoch 00898: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0303 - acc: 0.9443\n",
      "Epoch 899/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9458\n",
      "Epoch 00899: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0297 - acc: 0.9459\n",
      "Epoch 900/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9472\n",
      "Epoch 00900: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0290 - acc: 0.9473\n",
      "Epoch 901/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9476\n",
      "Epoch 00901: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0291 - acc: 0.9473\n",
      "Epoch 902/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9448\n",
      "Epoch 00902: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0300 - acc: 0.9449\n",
      "Epoch 903/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9492\n",
      "Epoch 00903: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0281 - acc: 0.9493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 904/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9456\n",
      "Epoch 00904: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0301 - acc: 0.9456\n",
      "Epoch 905/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9495\n",
      "Epoch 00905: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0276 - acc: 0.9495\n",
      "Epoch 906/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9463\n",
      "Epoch 00906: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0297 - acc: 0.9462\n",
      "Epoch 907/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9479\n",
      "Epoch 00907: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0285 - acc: 0.9478\n",
      "Epoch 908/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9508\n",
      "Epoch 00908: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0270 - acc: 0.9509\n",
      "Epoch 909/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9479\n",
      "Epoch 00909: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0290 - acc: 0.9478\n",
      "Epoch 910/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9463\n",
      "Epoch 00910: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0295 - acc: 0.9462\n",
      "Epoch 911/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9492\n",
      "Epoch 00911: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0280 - acc: 0.9491\n",
      "Epoch 912/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9495\n",
      "Epoch 00912: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0280 - acc: 0.9497\n",
      "Epoch 913/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9446\n",
      "Epoch 00913: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0305 - acc: 0.9447\n",
      "Epoch 914/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9463\n",
      "Epoch 00914: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0292 - acc: 0.9465\n",
      "Epoch 915/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9475\n",
      "Epoch 00915: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0289 - acc: 0.9476\n",
      "Epoch 916/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9467\n",
      "Epoch 00916: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0293 - acc: 0.9467\n",
      "Epoch 917/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9482\n",
      "Epoch 00917: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 918/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9484\n",
      "Epoch 00918: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0283 - acc: 0.9485\n",
      "Epoch 919/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9448\n",
      "Epoch 00919: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0305 - acc: 0.9449\n",
      "Epoch 920/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9495\n",
      "Epoch 00920: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0278 - acc: 0.9494\n",
      "Epoch 921/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9477\n",
      "Epoch 00921: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0292 - acc: 0.9478\n",
      "Epoch 922/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9480\n",
      "Epoch 00922: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0286 - acc: 0.9481\n",
      "Epoch 923/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9504\n",
      "Epoch 00923: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0272 - acc: 0.9501\n",
      "Epoch 924/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9503\n",
      "Epoch 00924: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0277 - acc: 0.9504\n",
      "Epoch 925/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9453\n",
      "Epoch 00925: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0301 - acc: 0.9454\n",
      "Epoch 926/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9444\n",
      "Epoch 00926: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0304 - acc: 0.9445\n",
      "Epoch 927/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9477\n",
      "Epoch 00927: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0288 - acc: 0.9477\n",
      "Epoch 928/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9485\n",
      "Epoch 00928: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0282 - acc: 0.9486\n",
      "Epoch 929/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9452\n",
      "Epoch 00929: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0302 - acc: 0.9452\n",
      "Epoch 930/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9481\n",
      "Epoch 00930: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0284 - acc: 0.9478\n",
      "Epoch 931/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9467\n",
      "Epoch 00931: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0295 - acc: 0.9467\n",
      "Epoch 932/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9471\n",
      "Epoch 00932: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0292 - acc: 0.9472\n",
      "Epoch 933/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9489\n",
      "Epoch 00933: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0278 - acc: 0.9490\n",
      "Epoch 934/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9483\n",
      "Epoch 00934: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0285 - acc: 0.9484\n",
      "Epoch 935/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9474\n",
      "Epoch 00935: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0290 - acc: 0.9475\n",
      "Epoch 936/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9473\n",
      "Epoch 00936: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0292 - acc: 0.9474\n",
      "Epoch 937/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9480\n",
      "Epoch 00937: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0284 - acc: 0.9481\n",
      "Epoch 938/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9504\n",
      "Epoch 00938: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0273 - acc: 0.9504\n",
      "Epoch 939/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9511\n",
      "Epoch 00939: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0273 - acc: 0.9512\n",
      "Epoch 940/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9462\n",
      "Epoch 00940: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0295 - acc: 0.9463\n",
      "Epoch 941/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9475\n",
      "Epoch 00941: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0285 - acc: 0.9475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 942/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9484\n",
      "Epoch 00942: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0282 - acc: 0.9485\n",
      "Epoch 943/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9463\n",
      "Epoch 00943: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0294 - acc: 0.9465\n",
      "Epoch 944/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9483\n",
      "Epoch 00944: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 945/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9502\n",
      "Epoch 00945: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0274 - acc: 0.9502\n",
      "Epoch 946/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9508\n",
      "Epoch 00946: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0270 - acc: 0.9507\n",
      "Epoch 947/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9460\n",
      "Epoch 00947: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0296 - acc: 0.9462\n",
      "Epoch 948/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9488\n",
      "Epoch 00948: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0278 - acc: 0.9489\n",
      "Epoch 949/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9447\n",
      "Epoch 00949: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0300 - acc: 0.9446\n",
      "Epoch 950/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9490\n",
      "Epoch 00950: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0283 - acc: 0.9492\n",
      "Epoch 951/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9488\n",
      "Epoch 00951: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0284 - acc: 0.9489\n",
      "Epoch 952/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9473\n",
      "Epoch 00952: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0294 - acc: 0.9473\n",
      "Epoch 953/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9483\n",
      "Epoch 00953: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0286 - acc: 0.9484\n",
      "Epoch 954/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9477\n",
      "Epoch 00954: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0288 - acc: 0.9478\n",
      "Epoch 955/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9487\n",
      "Epoch 00955: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0285 - acc: 0.9488\n",
      "Epoch 956/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9451\n",
      "Epoch 00956: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0302 - acc: 0.9451\n",
      "Epoch 957/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9501\n",
      "Epoch 00957: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0276 - acc: 0.9501\n",
      "Epoch 958/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9509\n",
      "Epoch 00958: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0274 - acc: 0.9509\n",
      "Epoch 959/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9461\n",
      "Epoch 00959: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0298 - acc: 0.9459\n",
      "Epoch 960/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9514\n",
      "Epoch 00960: loss improved from 0.02674 to 0.02669, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0267 - acc: 0.9514\n",
      "Epoch 961/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9439\n",
      "Epoch 00961: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0311 - acc: 0.9439\n",
      "Epoch 962/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9450\n",
      "Epoch 00962: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0297 - acc: 0.9449\n",
      "Epoch 963/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9459\n",
      "Epoch 00963: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0302 - acc: 0.9461\n",
      "Epoch 964/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9469\n",
      "Epoch 00964: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0293 - acc: 0.9468\n",
      "Epoch 965/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9461\n",
      "Epoch 00965: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0294 - acc: 0.9463\n",
      "Epoch 966/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9495\n",
      "Epoch 00966: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0278 - acc: 0.9495\n",
      "Epoch 967/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9483\n",
      "Epoch 00967: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0285 - acc: 0.9483\n",
      "Epoch 968/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9461\n",
      "Epoch 00968: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0293 - acc: 0.9462\n",
      "Epoch 969/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9476\n",
      "Epoch 00969: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0290 - acc: 0.9476\n",
      "Epoch 970/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9513\n",
      "Epoch 00970: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0273 - acc: 0.9511\n",
      "Epoch 971/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9512\n",
      "Epoch 00971: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0268 - acc: 0.9511\n",
      "Epoch 972/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9502\n",
      "Epoch 00972: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0278 - acc: 0.9502\n",
      "Epoch 973/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9472\n",
      "Epoch 00973: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0290 - acc: 0.9473\n",
      "Epoch 974/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9506\n",
      "Epoch 00974: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0271 - acc: 0.9506\n",
      "Epoch 975/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9503\n",
      "Epoch 00975: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0275 - acc: 0.9504\n",
      "Epoch 976/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9500\n",
      "Epoch 00976: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0277 - acc: 0.9500\n",
      "Epoch 977/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9493\n",
      "Epoch 00977: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0278 - acc: 0.9494\n",
      "Epoch 978/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9432\n",
      "Epoch 00978: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0316 - acc: 0.9433\n",
      "Epoch 979/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9480\n",
      "Epoch 00979: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 980/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9460\n",
      "Epoch 00980: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0295 - acc: 0.9461\n",
      "Epoch 981/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9475\n",
      "Epoch 00981: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0288 - acc: 0.9476\n",
      "Epoch 982/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9499\n",
      "Epoch 00982: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0274 - acc: 0.9496\n",
      "Epoch 983/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9497\n",
      "Epoch 00983: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0277 - acc: 0.9499\n",
      "Epoch 984/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9495\n",
      "Epoch 00984: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0280 - acc: 0.9492\n",
      "Epoch 985/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9504\n",
      "Epoch 00985: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0269 - acc: 0.9503\n",
      "Epoch 986/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9480\n",
      "Epoch 00986: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 987/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9493\n",
      "Epoch 00987: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0282 - acc: 0.9489\n",
      "Epoch 988/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9445\n",
      "Epoch 00988: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0299 - acc: 0.9445\n",
      "Epoch 989/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9469\n",
      "Epoch 00989: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0290 - acc: 0.9471\n",
      "Epoch 990/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9480\n",
      "Epoch 00990: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0286 - acc: 0.9481\n",
      "Epoch 991/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9490\n",
      "Epoch 00991: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0280 - acc: 0.9490\n",
      "Epoch 992/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9455\n",
      "Epoch 00992: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0296 - acc: 0.9456\n",
      "Epoch 993/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9469\n",
      "Epoch 00993: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0294 - acc: 0.9469\n",
      "Epoch 994/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9453\n",
      "Epoch 00994: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0299 - acc: 0.9454\n",
      "Epoch 995/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9446\n",
      "Epoch 00995: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0302 - acc: 0.9448\n",
      "Epoch 996/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9492\n",
      "Epoch 00996: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0279 - acc: 0.9493\n",
      "Epoch 997/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9469\n",
      "Epoch 00997: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0288 - acc: 0.9469\n",
      "Epoch 998/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9489\n",
      "Epoch 00998: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0278 - acc: 0.9491\n",
      "Epoch 999/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9482\n",
      "Epoch 00999: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0286 - acc: 0.9483\n",
      "Epoch 1000/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9485\n",
      "Epoch 01000: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0284 - acc: 0.9486\n",
      "Epoch 1001/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9482\n",
      "Epoch 01001: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0287 - acc: 0.9482\n",
      "Epoch 1002/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9490\n",
      "Epoch 01002: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0281 - acc: 0.9488\n",
      "Epoch 1003/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9489\n",
      "Epoch 01003: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0279 - acc: 0.9490\n",
      "Epoch 1004/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9474\n",
      "Epoch 01004: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0290 - acc: 0.9473\n",
      "Epoch 1005/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9504\n",
      "Epoch 01005: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0274 - acc: 0.9503\n",
      "Epoch 1006/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9498\n",
      "Epoch 01006: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0282 - acc: 0.9496\n",
      "Epoch 1007/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9462\n",
      "Epoch 01007: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0293 - acc: 0.9463\n",
      "Epoch 1008/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9468\n",
      "Epoch 01008: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 1009/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9454\n",
      "Epoch 01009: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0302 - acc: 0.9453\n",
      "Epoch 1010/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9480\n",
      "Epoch 01010: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0285 - acc: 0.9480\n",
      "Epoch 1011/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9509\n",
      "Epoch 01011: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0269 - acc: 0.9509\n",
      "Epoch 1012/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9509\n",
      "Epoch 01012: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0269 - acc: 0.9509\n",
      "Epoch 1013/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9476\n",
      "Epoch 01013: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0284 - acc: 0.9476\n",
      "Epoch 1014/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9477\n",
      "Epoch 01014: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0291 - acc: 0.9478\n",
      "Epoch 1015/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9487\n",
      "Epoch 01015: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0281 - acc: 0.9487\n",
      "Epoch 1016/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9470\n",
      "Epoch 01016: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0290 - acc: 0.9471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1017/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9489\n",
      "Epoch 01017: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0282 - acc: 0.9488\n",
      "Epoch 1018/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9467\n",
      "Epoch 01018: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0297 - acc: 0.9466\n",
      "Epoch 1019/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9484\n",
      "Epoch 01019: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0279 - acc: 0.9485\n",
      "Epoch 1020/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9480\n",
      "Epoch 01020: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0291 - acc: 0.9481\n",
      "Epoch 1021/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9510\n",
      "Epoch 01021: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0270 - acc: 0.9510\n",
      "Epoch 1022/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9495\n",
      "Epoch 01022: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0281 - acc: 0.9494\n",
      "Epoch 1023/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9484\n",
      "Epoch 01023: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0282 - acc: 0.9484\n",
      "Epoch 1024/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9493\n",
      "Epoch 01024: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0283 - acc: 0.9492\n",
      "Epoch 1025/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9466\n",
      "Epoch 01025: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 1026/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9497\n",
      "Epoch 01026: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0279 - acc: 0.9495\n",
      "Epoch 1027/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9494\n",
      "Epoch 01027: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0278 - acc: 0.9495\n",
      "Epoch 1028/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9489\n",
      "Epoch 01028: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0282 - acc: 0.9490\n",
      "Epoch 1029/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9471\n",
      "Epoch 01029: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0294 - acc: 0.9472\n",
      "Epoch 1030/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9478\n",
      "Epoch 01030: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0282 - acc: 0.9479\n",
      "Epoch 1031/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9501\n",
      "Epoch 01031: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0275 - acc: 0.9501\n",
      "Epoch 1032/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9509\n",
      "Epoch 01032: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0273 - acc: 0.9509\n",
      "Epoch 1033/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9476\n",
      "Epoch 01033: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0285 - acc: 0.9477\n",
      "Epoch 1034/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9473\n",
      "Epoch 01034: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0296 - acc: 0.9472\n",
      "Epoch 1035/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9485\n",
      "Epoch 01035: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0285 - acc: 0.9485\n",
      "Epoch 1036/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9479\n",
      "Epoch 01036: loss did not improve\n",
      "100/100 [==============================] - 48s 485ms/step - loss: 0.0287 - acc: 0.9477\n",
      "Epoch 1037/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9503\n",
      "Epoch 01037: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0279 - acc: 0.9503\n",
      "Epoch 1038/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9507\n",
      "Epoch 01038: loss did not improve\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.0274 - acc: 0.9509\n",
      "Epoch 1039/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9492\n",
      "Epoch 01039: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0281 - acc: 0.9492\n",
      "Epoch 1040/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9486\n",
      "Epoch 01040: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0282 - acc: 0.9485\n",
      "Epoch 1041/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9507\n",
      "Epoch 01041: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0270 - acc: 0.9508\n",
      "Epoch 1042/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9492\n",
      "Epoch 01042: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0279 - acc: 0.9493\n",
      "Epoch 1043/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9472\n",
      "Epoch 01043: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0294 - acc: 0.9471\n",
      "Epoch 1044/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9487\n",
      "Epoch 01044: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0282 - acc: 0.9487\n",
      "Epoch 1045/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9482\n",
      "Epoch 01045: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0289 - acc: 0.9483\n",
      "Epoch 1046/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9490\n",
      "Epoch 01046: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0278 - acc: 0.9490\n",
      "Epoch 1047/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9457\n",
      "Epoch 01047: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0301 - acc: 0.9457\n",
      "Epoch 1048/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9513\n",
      "Epoch 01048: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0267 - acc: 0.9513\n",
      "Epoch 1049/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9469\n",
      "Epoch 01049: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0290 - acc: 0.9470\n",
      "Epoch 1050/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9488\n",
      "Epoch 01050: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0284 - acc: 0.9489\n",
      "Epoch 1051/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9491\n",
      "Epoch 01051: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0281 - acc: 0.9492\n",
      "Epoch 1052/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9491\n",
      "Epoch 01052: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0279 - acc: 0.9491\n",
      "Epoch 1053/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9449\n",
      "Epoch 01053: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0303 - acc: 0.9451\n",
      "Epoch 1054/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9471\n",
      "Epoch 01054: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0295 - acc: 0.9469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1055/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9492\n",
      "Epoch 01055: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0281 - acc: 0.9491\n",
      "Epoch 1056/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9477\n",
      "Epoch 01056: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0292 - acc: 0.9476\n",
      "Epoch 1057/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9481\n",
      "Epoch 01057: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0284 - acc: 0.9479\n",
      "Epoch 1058/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9462\n",
      "Epoch 01058: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0297 - acc: 0.9462\n",
      "Epoch 1059/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9484\n",
      "Epoch 01059: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0280 - acc: 0.9484\n",
      "Epoch 1060/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9474\n",
      "Epoch 01060: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0287 - acc: 0.9475\n",
      "Epoch 1061/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9502\n",
      "Epoch 01061: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0272 - acc: 0.9502\n",
      "Epoch 1062/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 01062: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0285 - acc: 0.9483\n",
      "Epoch 1063/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9488\n",
      "Epoch 01063: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0285 - acc: 0.9487\n",
      "Epoch 1064/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9466\n",
      "Epoch 01064: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 1065/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9494\n",
      "Epoch 01065: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0277 - acc: 0.9496\n",
      "Epoch 1066/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9485\n",
      "Epoch 01066: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0287 - acc: 0.9486\n",
      "Epoch 1067/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9495\n",
      "Epoch 01067: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0281 - acc: 0.9495\n",
      "Epoch 1068/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9509\n",
      "Epoch 01068: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0269 - acc: 0.9509\n",
      "Epoch 1069/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9505\n",
      "Epoch 01069: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0271 - acc: 0.9503\n",
      "Epoch 1070/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9489\n",
      "Epoch 01070: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0280 - acc: 0.9489\n",
      "Epoch 1071/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9478\n",
      "Epoch 01071: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0285 - acc: 0.9478\n",
      "Epoch 1072/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9490\n",
      "Epoch 01072: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0284 - acc: 0.9489\n",
      "Epoch 1073/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9487\n",
      "Epoch 01073: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0283 - acc: 0.9487\n",
      "Epoch 1074/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9449\n",
      "Epoch 01074: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0302 - acc: 0.9449\n",
      "Epoch 1075/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9480\n",
      "Epoch 01075: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0288 - acc: 0.9481\n",
      "Epoch 1076/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9482\n",
      "Epoch 01076: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0286 - acc: 0.9484\n",
      "Epoch 1077/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9500\n",
      "Epoch 01077: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0274 - acc: 0.9502\n",
      "Epoch 1078/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9451\n",
      "Epoch 01078: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0299 - acc: 0.9453\n",
      "Epoch 1079/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9469\n",
      "Epoch 01079: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0289 - acc: 0.9470\n",
      "Epoch 1080/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9496\n",
      "Epoch 01080: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0282 - acc: 0.9496\n",
      "Epoch 1081/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9473\n",
      "Epoch 01081: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0289 - acc: 0.9473\n",
      "Epoch 1082/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9482\n",
      "Epoch 01082: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0286 - acc: 0.9483\n",
      "Epoch 1083/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9483\n",
      "Epoch 01083: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0279 - acc: 0.9484\n",
      "Epoch 1084/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9495\n",
      "Epoch 01084: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0275 - acc: 0.9495\n",
      "Epoch 1085/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9500\n",
      "Epoch 01085: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0274 - acc: 0.9500\n",
      "Epoch 1086/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9486\n",
      "Epoch 01086: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0287 - acc: 0.9484\n",
      "Epoch 1087/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9476\n",
      "Epoch 01087: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0284 - acc: 0.9478\n",
      "Epoch 1088/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9461\n",
      "Epoch 01088: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0299 - acc: 0.9462\n",
      "Epoch 1089/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9472\n",
      "Epoch 01089: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0288 - acc: 0.9473\n",
      "Epoch 1090/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9503\n",
      "Epoch 01090: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0275 - acc: 0.9502\n",
      "Epoch 1091/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9491\n",
      "Epoch 01091: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0280 - acc: 0.9491\n",
      "Epoch 1092/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9492\n",
      "Epoch 01092: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0280 - acc: 0.9492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1093/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9475\n",
      "Epoch 01093: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0290 - acc: 0.9474\n",
      "Epoch 1094/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9492\n",
      "Epoch 01094: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0283 - acc: 0.9491\n",
      "Epoch 1095/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9482\n",
      "Epoch 01095: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0286 - acc: 0.9484\n",
      "Epoch 1096/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9511\n",
      "Epoch 01096: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0272 - acc: 0.9512\n",
      "Epoch 1097/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9477\n",
      "Epoch 01097: loss did not improve\n",
      "100/100 [==============================] - 49s 485ms/step - loss: 0.0291 - acc: 0.9477\n",
      "Epoch 1098/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9477\n",
      "Epoch 01098: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0293 - acc: 0.9471\n",
      "Epoch 1099/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9478\n",
      "Epoch 01099: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0284 - acc: 0.9480\n",
      "Epoch 1100/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9512\n",
      "Epoch 01100: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0268 - acc: 0.9512\n",
      "Epoch 1101/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9455\n",
      "Epoch 01101: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0300 - acc: 0.9456\n",
      "Epoch 1102/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9492\n",
      "Epoch 01102: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0280 - acc: 0.9494\n",
      "Epoch 1103/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9483\n",
      "Epoch 01103: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0284 - acc: 0.9483\n",
      "Epoch 1104/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9486\n",
      "Epoch 01104: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0288 - acc: 0.9481\n",
      "Epoch 1105/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9497\n",
      "Epoch 01105: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0277 - acc: 0.9495\n",
      "Epoch 1106/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9497\n",
      "Epoch 01106: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0275 - acc: 0.9497\n",
      "Epoch 1107/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9510\n",
      "Epoch 01107: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0270 - acc: 0.9508\n",
      "Epoch 1108/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9465\n",
      "Epoch 01108: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0296 - acc: 0.9464\n",
      "Epoch 1109/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9472\n",
      "Epoch 01109: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0292 - acc: 0.9470\n",
      "Epoch 1110/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9503\n",
      "Epoch 01110: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0270 - acc: 0.9504\n",
      "Epoch 1111/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9471\n",
      "Epoch 01111: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0292 - acc: 0.9470\n",
      "Epoch 1112/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9508\n",
      "Epoch 01112: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0274 - acc: 0.9508\n",
      "Epoch 1113/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9502\n",
      "Epoch 01113: loss did not improve\n",
      "100/100 [==============================] - 48s 485ms/step - loss: 0.0274 - acc: 0.9502\n",
      "Epoch 1114/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9485\n",
      "Epoch 01114: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0284 - acc: 0.9485\n",
      "Epoch 1115/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9486\n",
      "Epoch 01115: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0286 - acc: 0.9483\n",
      "Epoch 1116/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9469\n",
      "Epoch 01116: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0294 - acc: 0.9469\n",
      "Epoch 1117/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9470\n",
      "Epoch 01117: loss did not improve\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.0289 - acc: 0.9471\n",
      "Epoch 1118/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9487\n",
      "Epoch 01118: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0283 - acc: 0.9488\n",
      "Epoch 1119/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9481\n",
      "Epoch 01119: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0283 - acc: 0.9482\n",
      "Epoch 1120/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9485\n",
      "Epoch 01120: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0282 - acc: 0.9485\n",
      "Epoch 1121/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9468\n",
      "Epoch 01121: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0291 - acc: 0.9468\n",
      "Epoch 1122/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9470\n",
      "Epoch 01122: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0287 - acc: 0.9472\n",
      "Epoch 1123/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9503\n",
      "Epoch 01123: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0275 - acc: 0.9503\n",
      "Epoch 1124/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9476\n",
      "Epoch 01124: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0292 - acc: 0.9477\n",
      "Epoch 1125/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9487\n",
      "Epoch 01125: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0280 - acc: 0.9487\n",
      "Epoch 1126/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9484\n",
      "Epoch 01126: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0283 - acc: 0.9485\n",
      "Epoch 1127/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9476\n",
      "Epoch 01127: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0290 - acc: 0.9476\n",
      "Epoch 1128/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9469\n",
      "Epoch 01128: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0288 - acc: 0.9471\n",
      "Epoch 1129/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9486\n",
      "Epoch 01129: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0280 - acc: 0.9488\n",
      "Epoch 1130/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9471\n",
      "Epoch 01130: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0288 - acc: 0.9473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1131/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9468\n",
      "Epoch 01131: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0288 - acc: 0.9468\n",
      "Epoch 1132/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9485\n",
      "Epoch 01132: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0284 - acc: 0.9486\n",
      "Epoch 1133/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9471\n",
      "Epoch 01133: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0286 - acc: 0.9473\n",
      "Epoch 1134/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9511\n",
      "Epoch 01134: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0267 - acc: 0.9512\n",
      "Epoch 1135/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9497\n",
      "Epoch 01135: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0278 - acc: 0.9497\n",
      "Epoch 1136/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9469\n",
      "Epoch 01136: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0288 - acc: 0.9471\n",
      "Epoch 1137/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9489\n",
      "Epoch 01137: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0279 - acc: 0.9490\n",
      "Epoch 1138/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9471\n",
      "Epoch 01138: loss did not improve\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.0288 - acc: 0.9471\n",
      "Epoch 1139/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9514\n",
      "Epoch 01139: loss did not improve\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.0269 - acc: 0.9514\n",
      "Epoch 1140/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9490\n",
      "Epoch 01140: loss did not improve\n",
      "100/100 [==============================] - 39s 385ms/step - loss: 0.0283 - acc: 0.9489\n",
      "Epoch 1141/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9482\n",
      "Epoch 01141: loss did not improve\n",
      "100/100 [==============================] - 39s 386ms/step - loss: 0.0280 - acc: 0.9484\n",
      "Epoch 1142/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9499\n",
      "Epoch 01142: loss did not improve\n",
      "100/100 [==============================] - 39s 386ms/step - loss: 0.0281 - acc: 0.9498\n",
      "Epoch 1143/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9511\n",
      "Epoch 01143: loss did not improve\n",
      "100/100 [==============================] - 38s 385ms/step - loss: 0.0271 - acc: 0.9512\n",
      "Epoch 1144/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9487\n",
      "Epoch 01144: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0285 - acc: 0.9488\n",
      "Epoch 1145/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9481\n",
      "Epoch 01145: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0284 - acc: 0.9482\n",
      "Epoch 1146/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9509\n",
      "Epoch 01146: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0272 - acc: 0.9510\n",
      "Epoch 1147/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9506\n",
      "Epoch 01147: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0269 - acc: 0.9508\n",
      "Epoch 1148/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9518\n",
      "Epoch 01148: loss improved from 0.02669 to 0.02643, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 39s 389ms/step - loss: 0.0264 - acc: 0.9517\n",
      "Epoch 1149/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9490\n",
      "Epoch 01149: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0281 - acc: 0.9489\n",
      "Epoch 1150/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9504\n",
      "Epoch 01150: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0276 - acc: 0.9499\n",
      "Epoch 1151/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9521\n",
      "Epoch 01151: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0264 - acc: 0.9523\n",
      "Epoch 1152/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9486\n",
      "Epoch 01152: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0283 - acc: 0.9485\n",
      "Epoch 1153/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9493\n",
      "Epoch 01153: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0281 - acc: 0.9494\n",
      "Epoch 1154/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9477\n",
      "Epoch 01154: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0288 - acc: 0.9474\n",
      "Epoch 1155/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9498\n",
      "Epoch 01155: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0276 - acc: 0.9498\n",
      "Epoch 1156/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9486\n",
      "Epoch 01156: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0281 - acc: 0.9484\n",
      "Epoch 1157/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9491\n",
      "Epoch 01157: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0279 - acc: 0.9491\n",
      "Epoch 1158/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9515\n",
      "Epoch 01158: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0269 - acc: 0.9514\n",
      "Epoch 1159/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9484\n",
      "Epoch 01159: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0282 - acc: 0.9483\n",
      "Epoch 1160/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9484\n",
      "Epoch 01160: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0280 - acc: 0.9485\n",
      "Epoch 1161/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9475\n",
      "Epoch 01161: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0291 - acc: 0.9476\n",
      "Epoch 1162/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9478\n",
      "Epoch 01162: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0285 - acc: 0.9479\n",
      "Epoch 1163/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9473\n",
      "Epoch 01163: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0289 - acc: 0.9472\n",
      "Epoch 1164/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9496\n",
      "Epoch 01164: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0277 - acc: 0.9493\n",
      "Epoch 1165/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9487\n",
      "Epoch 01165: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0282 - acc: 0.9487\n",
      "Epoch 1166/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9491\n",
      "Epoch 01166: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0280 - acc: 0.9491\n",
      "Epoch 1167/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9480\n",
      "Epoch 01167: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0288 - acc: 0.9476\n",
      "Epoch 1168/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9520\n",
      "Epoch 01168: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0265 - acc: 0.9520\n",
      "Epoch 1169/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9465\n",
      "Epoch 01169: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0292 - acc: 0.9465\n",
      "Epoch 1170/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9448\n",
      "Epoch 01170: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0302 - acc: 0.9446\n",
      "Epoch 1171/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9481\n",
      "Epoch 01171: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0286 - acc: 0.9482\n",
      "Epoch 1172/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9498\n",
      "Epoch 01172: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0275 - acc: 0.9499\n",
      "Epoch 1173/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9472\n",
      "Epoch 01173: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0289 - acc: 0.9473\n",
      "Epoch 1174/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9478\n",
      "Epoch 01174: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0288 - acc: 0.9475\n",
      "Epoch 1175/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9465\n",
      "Epoch 01175: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0295 - acc: 0.9462\n",
      "Epoch 1176/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9497\n",
      "Epoch 01176: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0276 - acc: 0.9498\n",
      "Epoch 1177/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9482\n",
      "Epoch 01177: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0285 - acc: 0.9483\n",
      "Epoch 1178/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9490\n",
      "Epoch 01178: loss did not improve\n",
      "100/100 [==============================] - 39s 387ms/step - loss: 0.0279 - acc: 0.9491\n",
      "Epoch 1179/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9521\n",
      "Epoch 01179: loss improved from 0.02643 to 0.02604, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 41s 411ms/step - loss: 0.0260 - acc: 0.9521\n",
      "Epoch 1180/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9501\n",
      "Epoch 01180: loss did not improve\n",
      "100/100 [==============================] - 40s 401ms/step - loss: 0.0274 - acc: 0.9502\n",
      "Epoch 1181/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9484\n",
      "Epoch 01181: loss did not improve\n",
      "100/100 [==============================] - 40s 404ms/step - loss: 0.0282 - acc: 0.9483\n",
      "Epoch 1182/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9502\n",
      "Epoch 01182: loss did not improve\n",
      "100/100 [==============================] - 41s 412ms/step - loss: 0.0271 - acc: 0.9501\n",
      "Epoch 1183/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9491\n",
      "Epoch 01183: loss did not improve\n",
      "100/100 [==============================] - 41s 409ms/step - loss: 0.0275 - acc: 0.9491\n",
      "Epoch 1184/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9504\n",
      "Epoch 01184: loss did not improve\n",
      "100/100 [==============================] - 42s 420ms/step - loss: 0.0273 - acc: 0.9504\n",
      "Epoch 1185/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9511\n",
      "Epoch 01185: loss did not improve\n",
      "100/100 [==============================] - 41s 407ms/step - loss: 0.0267 - acc: 0.9513\n",
      "Epoch 1186/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9499\n",
      "Epoch 01186: loss did not improve\n",
      "100/100 [==============================] - 41s 412ms/step - loss: 0.0272 - acc: 0.9500\n",
      "Epoch 1187/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9481\n",
      "Epoch 01187: loss did not improve\n",
      "100/100 [==============================] - 42s 415ms/step - loss: 0.0280 - acc: 0.9481\n",
      "Epoch 1188/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9497\n",
      "Epoch 01188: loss did not improve\n",
      "100/100 [==============================] - 41s 413ms/step - loss: 0.0274 - acc: 0.9496\n",
      "Epoch 1189/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9484\n",
      "Epoch 01189: loss did not improve\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.0281 - acc: 0.9485\n",
      "Epoch 1190/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9498\n",
      "Epoch 01190: loss did not improve\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.0277 - acc: 0.9498\n",
      "Epoch 1191/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9494\n",
      "Epoch 01191: loss did not improve\n",
      "100/100 [==============================] - 43s 428ms/step - loss: 0.0277 - acc: 0.9494\n",
      "Epoch 1192/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9489\n",
      "Epoch 01192: loss did not improve\n",
      "100/100 [==============================] - 42s 423ms/step - loss: 0.0276 - acc: 0.9490\n",
      "Epoch 1193/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9514\n",
      "Epoch 01193: loss did not improve\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.0269 - acc: 0.9514\n",
      "Epoch 1194/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9484\n",
      "Epoch 01194: loss did not improve\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.0284 - acc: 0.9484\n",
      "Epoch 1195/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9488\n",
      "Epoch 01195: loss did not improve\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.0285 - acc: 0.9488\n",
      "Epoch 1196/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9470\n",
      "Epoch 01196: loss did not improve\n",
      "100/100 [==============================] - 42s 419ms/step - loss: 0.0294 - acc: 0.9472\n",
      "Epoch 1197/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9471\n",
      "Epoch 01197: loss did not improve\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 0.0290 - acc: 0.9469\n",
      "Epoch 1198/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9507\n",
      "Epoch 01198: loss did not improve\n",
      "100/100 [==============================] - 42s 423ms/step - loss: 0.0272 - acc: 0.9508\n",
      "Epoch 1199/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9480\n",
      "Epoch 01199: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0279 - acc: 0.9481\n",
      "Epoch 1200/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 01200: loss did not improve\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 1201/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9488\n",
      "Epoch 01201: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0280 - acc: 0.9489\n",
      "Epoch 1202/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9469\n",
      "Epoch 01202: loss did not improve\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0287 - acc: 0.9469\n",
      "Epoch 1203/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9498\n",
      "Epoch 01203: loss did not improve\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.0276 - acc: 0.9498\n",
      "Epoch 1204/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9474\n",
      "Epoch 01204: loss did not improve\n",
      "100/100 [==============================] - 44s 437ms/step - loss: 0.0294 - acc: 0.9475\n",
      "Epoch 1205/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9477\n",
      "Epoch 01205: loss did not improve\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.0291 - acc: 0.9469\n",
      "Epoch 1206/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9495\n",
      "Epoch 01206: loss did not improve\n",
      "100/100 [==============================] - 43s 434ms/step - loss: 0.0276 - acc: 0.9495\n",
      "Epoch 1207/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9481\n",
      "Epoch 01207: loss did not improve\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.0283 - acc: 0.9481\n",
      "Epoch 1208/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9504\n",
      "Epoch 01208: loss did not improve\n",
      "100/100 [==============================] - 44s 436ms/step - loss: 0.0269 - acc: 0.9505\n",
      "Epoch 1209/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9486\n",
      "Epoch 01209: loss did not improve\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.0283 - acc: 0.9487\n",
      "Epoch 1210/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9502\n",
      "Epoch 01210: loss did not improve\n",
      "100/100 [==============================] - 44s 436ms/step - loss: 0.0274 - acc: 0.9501\n",
      "Epoch 1211/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9509\n",
      "Epoch 01211: loss did not improve\n",
      "100/100 [==============================] - 43s 432ms/step - loss: 0.0272 - acc: 0.9509\n",
      "Epoch 1212/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9507\n",
      "Epoch 01212: loss did not improve\n",
      "100/100 [==============================] - 43s 432ms/step - loss: 0.0271 - acc: 0.9506\n",
      "Epoch 1213/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9495\n",
      "Epoch 01213: loss did not improve\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.0274 - acc: 0.9495\n",
      "Epoch 1214/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9470\n",
      "Epoch 01214: loss did not improve\n",
      "100/100 [==============================] - 44s 437ms/step - loss: 0.0294 - acc: 0.9469\n",
      "Epoch 1215/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9498\n",
      "Epoch 01215: loss did not improve\n",
      "100/100 [==============================] - 44s 435ms/step - loss: 0.0279 - acc: 0.9496\n",
      "Epoch 1216/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9475\n",
      "Epoch 01216: loss did not improve\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0291 - acc: 0.9476\n",
      "Epoch 1217/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9508\n",
      "Epoch 01217: loss did not improve\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0268 - acc: 0.9510\n",
      "Epoch 1218/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9496\n",
      "Epoch 01218: loss did not improve\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.0278 - acc: 0.9491\n",
      "Epoch 1219/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9481\n",
      "Epoch 01219: loss did not improve\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.0287 - acc: 0.9482\n",
      "Epoch 1220/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9456\n",
      "Epoch 01220: loss did not improve\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.0300 - acc: 0.9454\n",
      "Epoch 1221/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9483\n",
      "Epoch 01221: loss did not improve\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0280 - acc: 0.9485\n",
      "Epoch 1222/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9468\n",
      "Epoch 01222: loss did not improve\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0291 - acc: 0.9469\n",
      "Epoch 1223/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9482\n",
      "Epoch 01223: loss did not improve\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.0282 - acc: 0.9483\n",
      "Epoch 1224/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9484\n",
      "Epoch 01224: loss did not improve\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.0278 - acc: 0.9485\n",
      "Epoch 1225/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9468\n",
      "Epoch 01225: loss did not improve\n",
      "100/100 [==============================] - 43s 430ms/step - loss: 0.0297 - acc: 0.9465\n",
      "Epoch 1226/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9461\n",
      "Epoch 01226: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0294 - acc: 0.9462\n",
      "Epoch 1227/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9475\n",
      "Epoch 01227: loss did not improve\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0286 - acc: 0.9475\n",
      "Epoch 1228/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9507\n",
      "Epoch 01228: loss did not improve\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.0271 - acc: 0.9507\n",
      "Epoch 1229/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9478\n",
      "Epoch 01229: loss did not improve\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.0283 - acc: 0.9480\n",
      "Epoch 1230/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9472\n",
      "Epoch 01230: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0293 - acc: 0.9473\n",
      "Epoch 1231/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9492\n",
      "Epoch 01231: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0279 - acc: 0.9492\n",
      "Epoch 1232/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9489\n",
      "Epoch 01232: loss did not improve\n",
      "100/100 [==============================] - 44s 436ms/step - loss: 0.0284 - acc: 0.9490\n",
      "Epoch 1233/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9486\n",
      "Epoch 01233: loss did not improve\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.0283 - acc: 0.9486\n",
      "Epoch 1234/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9481\n",
      "Epoch 01234: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0282 - acc: 0.9482\n",
      "Epoch 1235/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9489\n",
      "Epoch 01235: loss did not improve\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0281 - acc: 0.9490\n",
      "Epoch 1236/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9476\n",
      "Epoch 01236: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0290 - acc: 0.9476\n",
      "Epoch 1237/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9491\n",
      "Epoch 01237: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0280 - acc: 0.9492\n",
      "Epoch 1238/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9488\n",
      "Epoch 01238: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0279 - acc: 0.9490\n",
      "Epoch 1239/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9470\n",
      "Epoch 01239: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0292 - acc: 0.9470\n",
      "Epoch 1240/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9495\n",
      "Epoch 01240: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0276 - acc: 0.9495\n",
      "Epoch 1241/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9483\n",
      "Epoch 01241: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0281 - acc: 0.9485\n",
      "Epoch 1242/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9489\n",
      "Epoch 01242: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0281 - acc: 0.9486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1243/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9475\n",
      "Epoch 01243: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0285 - acc: 0.9476\n",
      "Epoch 1244/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9439\n",
      "Epoch 01244: loss did not improve\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.0306 - acc: 0.9441\n",
      "Epoch 1245/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9478\n",
      "Epoch 01245: loss did not improve\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0286 - acc: 0.9477\n",
      "Epoch 1246/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9481\n",
      "Epoch 01246: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0282 - acc: 0.9480\n",
      "Epoch 1247/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9484\n",
      "Epoch 01247: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0285 - acc: 0.9482\n",
      "Epoch 1248/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9486\n",
      "Epoch 01248: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0280 - acc: 0.9486\n",
      "Epoch 1249/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9478\n",
      "Epoch 01249: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0284 - acc: 0.9478\n",
      "Epoch 1250/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9476\n",
      "Epoch 01250: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0286 - acc: 0.9476\n",
      "Epoch 1251/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9481\n",
      "Epoch 01251: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0282 - acc: 0.9482\n",
      "Epoch 1252/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9474\n",
      "Epoch 01252: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0288 - acc: 0.9475\n",
      "Epoch 1253/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9480\n",
      "Epoch 01253: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0282 - acc: 0.9481\n",
      "Epoch 1254/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9501\n",
      "Epoch 01254: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0274 - acc: 0.9499\n",
      "Epoch 1255/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9514\n",
      "Epoch 01255: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0267 - acc: 0.9514\n",
      "Epoch 1256/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9492\n",
      "Epoch 01256: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0282 - acc: 0.9493\n",
      "Epoch 1257/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9492\n",
      "Epoch 01257: loss did not improve\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.0283 - acc: 0.9492\n",
      "Epoch 1258/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9491\n",
      "Epoch 01258: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0280 - acc: 0.9491\n",
      "Epoch 1259/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9493\n",
      "Epoch 01259: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0279 - acc: 0.9494\n",
      "Epoch 1260/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9489\n",
      "Epoch 01260: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0281 - acc: 0.9489\n",
      "Epoch 1261/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9481\n",
      "Epoch 01261: loss did not improve\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.0285 - acc: 0.9482\n",
      "Epoch 1262/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9498\n",
      "Epoch 01262: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0276 - acc: 0.9497\n",
      "Epoch 1263/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9505\n",
      "Epoch 01263: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0270 - acc: 0.9506\n",
      "Epoch 1264/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9511\n",
      "Epoch 01264: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0270 - acc: 0.9511\n",
      "Epoch 1265/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9476\n",
      "Epoch 01265: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0293 - acc: 0.9476\n",
      "Epoch 1266/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9448\n",
      "Epoch 01266: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0306 - acc: 0.9449\n",
      "Epoch 1267/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9466\n",
      "Epoch 01267: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0294 - acc: 0.9467\n",
      "Epoch 1268/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9496\n",
      "Epoch 01268: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0274 - acc: 0.9496\n",
      "Epoch 1269/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9489\n",
      "Epoch 01269: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0285 - acc: 0.9490\n",
      "Epoch 1270/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9477\n",
      "Epoch 01270: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0287 - acc: 0.9477\n",
      "Epoch 1271/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9488\n",
      "Epoch 01271: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0283 - acc: 0.9485\n",
      "Epoch 1272/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9483\n",
      "Epoch 01272: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0286 - acc: 0.9485\n",
      "Epoch 1273/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9473\n",
      "Epoch 01273: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0291 - acc: 0.9471\n",
      "Epoch 1274/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9459\n",
      "Epoch 01274: loss did not improve\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.0297 - acc: 0.9457\n",
      "Epoch 1275/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9489\n",
      "Epoch 01275: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0280 - acc: 0.9490\n",
      "Epoch 1276/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9466\n",
      "Epoch 01276: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0290 - acc: 0.9467\n",
      "Epoch 1277/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9500\n",
      "Epoch 01277: loss did not improve\n",
      "100/100 [==============================] - 45s 455ms/step - loss: 0.0273 - acc: 0.9499\n",
      "Epoch 1278/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9530\n",
      "Epoch 01278: loss improved from 0.02604 to 0.02570, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0257 - acc: 0.9529\n",
      "Epoch 1279/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9485\n",
      "Epoch 01279: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0283 - acc: 0.9485\n",
      "Epoch 1280/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9474\n",
      "Epoch 01280: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0289 - acc: 0.9474\n",
      "Epoch 1281/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9496\n",
      "Epoch 01281: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0281 - acc: 0.9496\n",
      "Epoch 1282/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9513\n",
      "Epoch 01282: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0265 - acc: 0.9515\n",
      "Epoch 1283/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9479\n",
      "Epoch 01283: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0288 - acc: 0.9477\n",
      "Epoch 1284/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9490\n",
      "Epoch 01284: loss did not improve\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.0274 - acc: 0.9489\n",
      "Epoch 1285/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9500\n",
      "Epoch 01285: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0274 - acc: 0.9499\n",
      "Epoch 1286/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9478\n",
      "Epoch 01286: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0285 - acc: 0.9479\n",
      "Epoch 1287/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9513\n",
      "Epoch 01287: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0266 - acc: 0.9512\n",
      "Epoch 1288/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9505\n",
      "Epoch 01288: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0269 - acc: 0.9504\n",
      "Epoch 1289/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9465\n",
      "Epoch 01289: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0294 - acc: 0.9466\n",
      "Epoch 1290/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9503\n",
      "Epoch 01290: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0274 - acc: 0.9503\n",
      "Epoch 1291/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9480\n",
      "Epoch 01291: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0286 - acc: 0.9480\n",
      "Epoch 1292/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9479\n",
      "Epoch 01292: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0287 - acc: 0.9478\n",
      "Epoch 1293/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9483\n",
      "Epoch 01293: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0282 - acc: 0.9484\n",
      "Epoch 1294/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9500\n",
      "Epoch 01294: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0272 - acc: 0.9501\n",
      "Epoch 1295/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9501\n",
      "Epoch 01295: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0271 - acc: 0.9503\n",
      "Epoch 1296/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9537\n",
      "Epoch 01296: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0258 - acc: 0.9533\n",
      "Epoch 1297/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9480\n",
      "Epoch 01297: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0286 - acc: 0.9481\n",
      "Epoch 1298/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9493\n",
      "Epoch 01298: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0279 - acc: 0.9494\n",
      "Epoch 1299/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9489\n",
      "Epoch 01299: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0282 - acc: 0.9491\n",
      "Epoch 1300/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9494\n",
      "Epoch 01300: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0277 - acc: 0.9494\n",
      "Epoch 1301/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9487\n",
      "Epoch 01301: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0282 - acc: 0.9488\n",
      "Epoch 1302/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9521\n",
      "Epoch 01302: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0262 - acc: 0.9519\n",
      "Epoch 1303/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9468\n",
      "Epoch 01303: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0292 - acc: 0.9468\n",
      "Epoch 1304/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9492\n",
      "Epoch 01304: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0279 - acc: 0.9491\n",
      "Epoch 1305/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9466\n",
      "Epoch 01305: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 1306/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9494\n",
      "Epoch 01306: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0279 - acc: 0.9492\n",
      "Epoch 1307/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9490\n",
      "Epoch 01307: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0282 - acc: 0.9489\n",
      "Epoch 1308/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9497\n",
      "Epoch 01308: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0278 - acc: 0.9495\n",
      "Epoch 1309/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9461\n",
      "Epoch 01309: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0301 - acc: 0.9460\n",
      "Epoch 1310/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9507\n",
      "Epoch 01310: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0273 - acc: 0.9508\n",
      "Epoch 1311/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9504\n",
      "Epoch 01311: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0278 - acc: 0.9503\n",
      "Epoch 1312/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9518\n",
      "Epoch 01312: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0261 - acc: 0.9520\n",
      "Epoch 1313/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9496\n",
      "Epoch 01313: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0277 - acc: 0.9496\n",
      "Epoch 1314/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9472\n",
      "Epoch 01314: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0291 - acc: 0.9472\n",
      "Epoch 1315/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9497\n",
      "Epoch 01315: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0277 - acc: 0.9497\n",
      "Epoch 1316/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9480\n",
      "Epoch 01316: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0288 - acc: 0.9480\n",
      "Epoch 1317/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9483\n",
      "Epoch 01317: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0286 - acc: 0.9482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1318/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9469\n",
      "Epoch 01318: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0289 - acc: 0.9466\n",
      "Epoch 1319/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9494\n",
      "Epoch 01319: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0278 - acc: 0.9492\n",
      "Epoch 1320/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9487\n",
      "Epoch 01320: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0276 - acc: 0.9488\n",
      "Epoch 1321/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9489\n",
      "Epoch 01321: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0279 - acc: 0.9489\n",
      "Epoch 1322/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9476\n",
      "Epoch 01322: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0287 - acc: 0.9478\n",
      "Epoch 1323/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9494\n",
      "Epoch 01323: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0273 - acc: 0.9496\n",
      "Epoch 1324/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9453\n",
      "Epoch 01324: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0302 - acc: 0.9453\n",
      "Epoch 1325/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9474\n",
      "Epoch 01325: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0284 - acc: 0.9474\n",
      "Epoch 1326/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9483\n",
      "Epoch 01326: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0281 - acc: 0.9485\n",
      "Epoch 1327/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9472\n",
      "Epoch 01327: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0292 - acc: 0.9472\n",
      "Epoch 1328/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9482\n",
      "Epoch 01328: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0286 - acc: 0.9484\n",
      "Epoch 1329/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9494\n",
      "Epoch 01329: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0278 - acc: 0.9494\n",
      "Epoch 1330/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9506\n",
      "Epoch 01330: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0269 - acc: 0.9506\n",
      "Epoch 1331/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9489\n",
      "Epoch 01331: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0283 - acc: 0.9489\n",
      "Epoch 1332/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9482\n",
      "Epoch 01332: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0286 - acc: 0.9484\n",
      "Epoch 1333/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9519\n",
      "Epoch 01333: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0264 - acc: 0.9519\n",
      "Epoch 1334/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9511\n",
      "Epoch 01334: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0272 - acc: 0.9510\n",
      "Epoch 1335/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9474\n",
      "Epoch 01335: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0290 - acc: 0.9475\n",
      "Epoch 1336/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9502\n",
      "Epoch 01336: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0273 - acc: 0.9501\n",
      "Epoch 1337/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9485\n",
      "Epoch 01337: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0283 - acc: 0.9485\n",
      "Epoch 1338/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9512\n",
      "Epoch 01338: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0267 - acc: 0.9511\n",
      "Epoch 1339/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9470\n",
      "Epoch 01339: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 1340/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9481\n",
      "Epoch 01340: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0288 - acc: 0.9478\n",
      "Epoch 1341/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9460\n",
      "Epoch 01341: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0289 - acc: 0.9461\n",
      "Epoch 1342/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9483\n",
      "Epoch 01342: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0287 - acc: 0.9482\n",
      "Epoch 1343/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9510\n",
      "Epoch 01343: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0273 - acc: 0.9510\n",
      "Epoch 1344/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9482\n",
      "Epoch 01344: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0289 - acc: 0.9482\n",
      "Epoch 1345/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9453\n",
      "Epoch 01345: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0296 - acc: 0.9451\n",
      "Epoch 1346/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9482\n",
      "Epoch 01346: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0289 - acc: 0.9483\n",
      "Epoch 1347/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9476\n",
      "Epoch 01347: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0290 - acc: 0.9476\n",
      "Epoch 1348/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9511\n",
      "Epoch 01348: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0269 - acc: 0.9511\n",
      "Epoch 1349/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9464\n",
      "Epoch 01349: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0295 - acc: 0.9466\n",
      "Epoch 1350/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9494\n",
      "Epoch 01350: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0274 - acc: 0.9495\n",
      "Epoch 1351/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9496\n",
      "Epoch 01351: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0279 - acc: 0.9496\n",
      "Epoch 1352/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9495\n",
      "Epoch 01352: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0278 - acc: 0.9496\n",
      "Epoch 1353/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9458\n",
      "Epoch 01353: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0299 - acc: 0.9460\n",
      "Epoch 1354/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9495\n",
      "Epoch 01354: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0279 - acc: 0.9494\n",
      "Epoch 1355/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9474\n",
      "Epoch 01355: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0291 - acc: 0.9474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1356/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9496\n",
      "Epoch 01356: loss did not improve\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.0277 - acc: 0.9498\n",
      "Epoch 1357/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9516\n",
      "Epoch 01357: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0265 - acc: 0.9515\n",
      "Epoch 1358/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9487\n",
      "Epoch 01358: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0277 - acc: 0.9487\n",
      "Epoch 1359/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9509\n",
      "Epoch 01359: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0265 - acc: 0.9507\n",
      "Epoch 1360/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9486\n",
      "Epoch 01360: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0280 - acc: 0.9487\n",
      "Epoch 1361/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9493\n",
      "Epoch 01361: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0277 - acc: 0.9493\n",
      "Epoch 1362/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9493\n",
      "Epoch 01362: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0282 - acc: 0.9494\n",
      "Epoch 1363/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9476\n",
      "Epoch 01363: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0287 - acc: 0.9475\n",
      "Epoch 1364/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9506\n",
      "Epoch 01364: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0274 - acc: 0.9505\n",
      "Epoch 1365/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9511\n",
      "Epoch 01365: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0270 - acc: 0.9511\n",
      "Epoch 1366/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9504\n",
      "Epoch 01366: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0272 - acc: 0.9504\n",
      "Epoch 1367/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9480\n",
      "Epoch 01367: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0282 - acc: 0.9480\n",
      "Epoch 1368/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9504\n",
      "Epoch 01368: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0274 - acc: 0.9503\n",
      "Epoch 1369/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9492\n",
      "Epoch 01369: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0278 - acc: 0.9493\n",
      "Epoch 1370/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9477\n",
      "Epoch 01370: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0286 - acc: 0.9478\n",
      "Epoch 1371/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9488\n",
      "Epoch 01371: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0280 - acc: 0.9489\n",
      "Epoch 1372/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9513\n",
      "Epoch 01372: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0269 - acc: 0.9513\n",
      "Epoch 1373/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9468\n",
      "Epoch 01373: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0295 - acc: 0.9468\n",
      "Epoch 1374/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9481\n",
      "Epoch 01374: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0284 - acc: 0.9479\n",
      "Epoch 1375/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9489\n",
      "Epoch 01375: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0279 - acc: 0.9491\n",
      "Epoch 1376/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9445\n",
      "Epoch 01376: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0300 - acc: 0.9446\n",
      "Epoch 1377/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9506\n",
      "Epoch 01377: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0271 - acc: 0.9506\n",
      "Epoch 1378/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9482\n",
      "Epoch 01378: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0288 - acc: 0.9481\n",
      "Epoch 1379/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9480\n",
      "Epoch 01379: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0281 - acc: 0.9481\n",
      "Epoch 1380/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9525\n",
      "Epoch 01380: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0259 - acc: 0.9527\n",
      "Epoch 1381/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9501\n",
      "Epoch 01381: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0270 - acc: 0.9502\n",
      "Epoch 1382/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9473\n",
      "Epoch 01382: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0292 - acc: 0.9473\n",
      "Epoch 1383/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9478\n",
      "Epoch 01383: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0284 - acc: 0.9479\n",
      "Epoch 1384/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9487\n",
      "Epoch 01384: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0280 - acc: 0.9487\n",
      "Epoch 1385/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9500\n",
      "Epoch 01385: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0276 - acc: 0.9501\n",
      "Epoch 1386/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9466\n",
      "Epoch 01386: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0292 - acc: 0.9466\n",
      "Epoch 1387/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9475\n",
      "Epoch 01387: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0289 - acc: 0.9476\n",
      "Epoch 1388/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9484\n",
      "Epoch 01388: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0283 - acc: 0.9485\n",
      "Epoch 1389/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9489\n",
      "Epoch 01389: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0280 - acc: 0.9489\n",
      "Epoch 1390/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9487\n",
      "Epoch 01390: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0277 - acc: 0.9487\n",
      "Epoch 1391/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9518\n",
      "Epoch 01391: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0265 - acc: 0.9520\n",
      "Epoch 1392/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9514\n",
      "Epoch 01392: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0268 - acc: 0.9516\n",
      "Epoch 1393/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9516\n",
      "Epoch 01393: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0268 - acc: 0.9513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1394/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9488\n",
      "Epoch 01394: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0282 - acc: 0.9485\n",
      "Epoch 1395/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9495\n",
      "Epoch 01395: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0278 - acc: 0.9496\n",
      "Epoch 1396/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9520\n",
      "Epoch 01396: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0264 - acc: 0.9520\n",
      "Epoch 1397/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9492\n",
      "Epoch 01397: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0283 - acc: 0.9491\n",
      "Epoch 1398/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9495\n",
      "Epoch 01398: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0275 - acc: 0.9496\n",
      "Epoch 1399/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9489\n",
      "Epoch 01399: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0283 - acc: 0.9485\n",
      "Epoch 1400/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9477\n",
      "Epoch 01400: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0288 - acc: 0.9478\n",
      "Epoch 1401/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9484\n",
      "Epoch 01401: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0285 - acc: 0.9484\n",
      "Epoch 1402/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9497\n",
      "Epoch 01402: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0280 - acc: 0.9496\n",
      "Epoch 1403/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9485\n",
      "Epoch 01403: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0280 - acc: 0.9485\n",
      "Epoch 1404/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9466\n",
      "Epoch 01404: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0290 - acc: 0.9468\n",
      "Epoch 1405/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9494\n",
      "Epoch 01405: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0278 - acc: 0.9495\n",
      "Epoch 1406/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9489\n",
      "Epoch 01406: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0275 - acc: 0.9491\n",
      "Epoch 1407/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9490\n",
      "Epoch 01407: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0278 - acc: 0.9489\n",
      "Epoch 1408/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9510\n",
      "Epoch 01408: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0268 - acc: 0.9511\n",
      "Epoch 1409/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9488\n",
      "Epoch 01409: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0283 - acc: 0.9489\n",
      "Epoch 1410/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9484\n",
      "Epoch 01410: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0281 - acc: 0.9486\n",
      "Epoch 1411/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9475\n",
      "Epoch 01411: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0286 - acc: 0.9475\n",
      "Epoch 1412/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9516\n",
      "Epoch 01412: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0265 - acc: 0.9517\n",
      "Epoch 1413/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9501\n",
      "Epoch 01413: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0276 - acc: 0.9502\n",
      "Epoch 1414/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9471\n",
      "Epoch 01414: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0293 - acc: 0.9472\n",
      "Epoch 1415/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9497\n",
      "Epoch 01415: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0276 - acc: 0.9494\n",
      "Epoch 1416/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9485\n",
      "Epoch 01416: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0285 - acc: 0.9485\n",
      "Epoch 1417/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9498\n",
      "Epoch 01417: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0274 - acc: 0.9495\n",
      "Epoch 1418/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9442\n",
      "Epoch 01418: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0305 - acc: 0.9444\n",
      "Epoch 1419/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 01419: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 1420/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9477\n",
      "Epoch 01420: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0288 - acc: 0.9474\n",
      "Epoch 1421/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9498\n",
      "Epoch 01421: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0275 - acc: 0.9498\n",
      "Epoch 1422/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9503\n",
      "Epoch 01422: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0270 - acc: 0.9504\n",
      "Epoch 1423/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9482\n",
      "Epoch 01423: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0282 - acc: 0.9483\n",
      "Epoch 1424/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9517\n",
      "Epoch 01424: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0263 - acc: 0.9518\n",
      "Epoch 1425/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9496\n",
      "Epoch 01425: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0272 - acc: 0.9497\n",
      "Epoch 1426/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9506\n",
      "Epoch 01426: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0271 - acc: 0.9506\n",
      "Epoch 1427/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9510\n",
      "Epoch 01427: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0265 - acc: 0.9509\n",
      "Epoch 1428/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9521\n",
      "Epoch 01428: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0262 - acc: 0.9521\n",
      "Epoch 1429/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9477\n",
      "Epoch 01429: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0287 - acc: 0.9478\n",
      "Epoch 1430/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9476\n",
      "Epoch 01430: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0289 - acc: 0.9476\n",
      "Epoch 1431/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9505\n",
      "Epoch 01431: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0272 - acc: 0.9505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1432/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9496\n",
      "Epoch 01432: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0275 - acc: 0.9497\n",
      "Epoch 1433/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9457\n",
      "Epoch 01433: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0295 - acc: 0.9458\n",
      "Epoch 1434/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9470\n",
      "Epoch 01434: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0290 - acc: 0.9470\n",
      "Epoch 1435/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9511\n",
      "Epoch 01435: loss did not improve\n",
      "100/100 [==============================] - 45s 455ms/step - loss: 0.0268 - acc: 0.9512\n",
      "Epoch 1436/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9468\n",
      "Epoch 01436: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0294 - acc: 0.9467\n",
      "Epoch 1437/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9498\n",
      "Epoch 01437: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0272 - acc: 0.9499\n",
      "Epoch 1438/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9507\n",
      "Epoch 01438: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0272 - acc: 0.9507\n",
      "Epoch 1439/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9492\n",
      "Epoch 01439: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0277 - acc: 0.9492\n",
      "Epoch 1440/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9460\n",
      "Epoch 01440: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0296 - acc: 0.9460\n",
      "Epoch 1441/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9516\n",
      "Epoch 01441: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0269 - acc: 0.9516\n",
      "Epoch 1442/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9489\n",
      "Epoch 01442: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0280 - acc: 0.9490\n",
      "Epoch 1443/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9482\n",
      "Epoch 01443: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0283 - acc: 0.9481\n",
      "Epoch 1444/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9499\n",
      "Epoch 01444: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0275 - acc: 0.9500\n",
      "Epoch 1445/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9531\n",
      "Epoch 01445: loss improved from 0.02570 to 0.02564, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0256 - acc: 0.9528\n",
      "Epoch 1446/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9516\n",
      "Epoch 01446: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0268 - acc: 0.9515\n",
      "Epoch 1447/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9510\n",
      "Epoch 01447: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0272 - acc: 0.9509\n",
      "Epoch 1448/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9502\n",
      "Epoch 01448: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0273 - acc: 0.9502\n",
      "Epoch 1449/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9523\n",
      "Epoch 01449: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0263 - acc: 0.9524\n",
      "Epoch 1450/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9508\n",
      "Epoch 01450: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0266 - acc: 0.9507\n",
      "Epoch 1451/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9506\n",
      "Epoch 01451: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0270 - acc: 0.9507\n",
      "Epoch 1452/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 01452: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0285 - acc: 0.9480\n",
      "Epoch 1453/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9497\n",
      "Epoch 01453: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0272 - acc: 0.9499\n",
      "Epoch 1454/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9486\n",
      "Epoch 01454: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0278 - acc: 0.9488\n",
      "Epoch 1455/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9508\n",
      "Epoch 01455: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0274 - acc: 0.9508\n",
      "Epoch 1456/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9494\n",
      "Epoch 01456: loss did not improve\n",
      "100/100 [==============================] - 49s 489ms/step - loss: 0.0275 - acc: 0.9492\n",
      "Epoch 1457/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9482\n",
      "Epoch 01457: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0280 - acc: 0.9484\n",
      "Epoch 1458/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9459\n",
      "Epoch 01458: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0293 - acc: 0.9458\n",
      "Epoch 1459/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9530\n",
      "Epoch 01459: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0265 - acc: 0.9528\n",
      "Epoch 1460/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9493\n",
      "Epoch 01460: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0281 - acc: 0.9493\n",
      "Epoch 1461/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9462\n",
      "Epoch 01461: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0294 - acc: 0.9464\n",
      "Epoch 1462/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9496\n",
      "Epoch 01462: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0275 - acc: 0.9496\n",
      "Epoch 1463/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9497\n",
      "Epoch 01463: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0274 - acc: 0.9496\n",
      "Epoch 1464/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9499\n",
      "Epoch 01464: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0275 - acc: 0.9501\n",
      "Epoch 1465/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9497\n",
      "Epoch 01465: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0276 - acc: 0.9498\n",
      "Epoch 1466/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9474\n",
      "Epoch 01466: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0290 - acc: 0.9474\n",
      "Epoch 1467/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9469\n",
      "Epoch 01467: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0287 - acc: 0.9470\n",
      "Epoch 1468/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9473\n",
      "Epoch 01468: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0289 - acc: 0.9472\n",
      "Epoch 1469/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9488\n",
      "Epoch 01469: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0280 - acc: 0.9488\n",
      "Epoch 1470/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9489\n",
      "Epoch 01470: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0278 - acc: 0.9488\n",
      "Epoch 1471/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9473\n",
      "Epoch 01471: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0290 - acc: 0.9474\n",
      "Epoch 1472/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9494\n",
      "Epoch 01472: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0276 - acc: 0.9495\n",
      "Epoch 1473/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9478\n",
      "Epoch 01473: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0291 - acc: 0.9477\n",
      "Epoch 1474/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9507\n",
      "Epoch 01474: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0269 - acc: 0.9506\n",
      "Epoch 1475/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9501\n",
      "Epoch 01475: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0274 - acc: 0.9500\n",
      "Epoch 1476/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9485\n",
      "Epoch 01476: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0277 - acc: 0.9485\n",
      "Epoch 1477/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9518\n",
      "Epoch 01477: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0262 - acc: 0.9518\n",
      "Epoch 1478/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9489\n",
      "Epoch 01478: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0278 - acc: 0.9490\n",
      "Epoch 1479/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9496\n",
      "Epoch 01479: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0277 - acc: 0.9497\n",
      "Epoch 1480/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9499\n",
      "Epoch 01480: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0274 - acc: 0.9501\n",
      "Epoch 1481/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9484\n",
      "Epoch 01481: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0282 - acc: 0.9485\n",
      "Epoch 1482/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9491\n",
      "Epoch 01482: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0280 - acc: 0.9490\n",
      "Epoch 1483/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9484\n",
      "Epoch 01483: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0284 - acc: 0.9484\n",
      "Epoch 1484/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9501\n",
      "Epoch 01484: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0275 - acc: 0.9500\n",
      "Epoch 1485/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9502\n",
      "Epoch 01485: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0272 - acc: 0.9504\n",
      "Epoch 1486/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9491\n",
      "Epoch 01486: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0279 - acc: 0.9491\n",
      "Epoch 1487/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9491\n",
      "Epoch 01487: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0280 - acc: 0.9491\n",
      "Epoch 1488/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9488\n",
      "Epoch 01488: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0280 - acc: 0.9489\n",
      "Epoch 1489/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9481\n",
      "Epoch 01489: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0281 - acc: 0.9479\n",
      "Epoch 1490/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9492\n",
      "Epoch 01490: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0279 - acc: 0.9491\n",
      "Epoch 1491/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9500\n",
      "Epoch 01491: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0273 - acc: 0.9501\n",
      "Epoch 1492/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9519\n",
      "Epoch 01492: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0266 - acc: 0.9516\n",
      "Epoch 1493/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9512\n",
      "Epoch 01493: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0265 - acc: 0.9512\n",
      "Epoch 1494/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9496\n",
      "Epoch 01494: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0275 - acc: 0.9495\n",
      "Epoch 1495/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9467\n",
      "Epoch 01495: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0291 - acc: 0.9468\n",
      "Epoch 1496/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9500\n",
      "Epoch 01496: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0277 - acc: 0.9500\n",
      "Epoch 1497/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9508\n",
      "Epoch 01497: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0269 - acc: 0.9509\n",
      "Epoch 1498/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9487\n",
      "Epoch 01498: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0286 - acc: 0.9484\n",
      "Epoch 1499/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9491\n",
      "Epoch 01499: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0275 - acc: 0.9492\n",
      "Epoch 1500/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9494\n",
      "Epoch 01500: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0277 - acc: 0.9494\n",
      "Epoch 1501/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9507\n",
      "Epoch 01501: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0271 - acc: 0.9508\n",
      "Epoch 1502/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9514\n",
      "Epoch 01502: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0268 - acc: 0.9514\n",
      "Epoch 1503/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9468\n",
      "Epoch 01503: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0285 - acc: 0.9470\n",
      "Epoch 1504/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9497\n",
      "Epoch 01504: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0274 - acc: 0.9497\n",
      "Epoch 1505/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9477\n",
      "Epoch 01505: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0290 - acc: 0.9476\n",
      "Epoch 1506/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9503\n",
      "Epoch 01506: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0271 - acc: 0.9504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1507/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9507\n",
      "Epoch 01507: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0265 - acc: 0.9508\n",
      "Epoch 1508/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9499\n",
      "Epoch 01508: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0277 - acc: 0.9501\n",
      "Epoch 1509/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9493\n",
      "Epoch 01509: loss did not improve\n",
      "100/100 [==============================] - 49s 490ms/step - loss: 0.0279 - acc: 0.9493\n",
      "Epoch 1510/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9502\n",
      "Epoch 01510: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0273 - acc: 0.9502\n",
      "Epoch 1511/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9505\n",
      "Epoch 01511: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0268 - acc: 0.9506\n",
      "Epoch 1512/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9471\n",
      "Epoch 01512: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0290 - acc: 0.9468\n",
      "Epoch 1513/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9484\n",
      "Epoch 01513: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0280 - acc: 0.9485\n",
      "Epoch 1514/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9510\n",
      "Epoch 01514: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0264 - acc: 0.9510\n",
      "Epoch 1515/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9506\n",
      "Epoch 01515: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0266 - acc: 0.9507\n",
      "Epoch 1516/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9514\n",
      "Epoch 01516: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0268 - acc: 0.9516\n",
      "Epoch 1517/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9478\n",
      "Epoch 01517: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0284 - acc: 0.9478\n",
      "Epoch 1518/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9496\n",
      "Epoch 01518: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0273 - acc: 0.9496\n",
      "Epoch 1519/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9446\n",
      "Epoch 01519: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0302 - acc: 0.9448\n",
      "Epoch 1520/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9498\n",
      "Epoch 01520: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0278 - acc: 0.9498\n",
      "Epoch 1521/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9517\n",
      "Epoch 01521: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0268 - acc: 0.9517\n",
      "Epoch 1522/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9504\n",
      "Epoch 01522: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0277 - acc: 0.9505\n",
      "Epoch 1523/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9480\n",
      "Epoch 01523: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 1524/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9493\n",
      "Epoch 01524: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0283 - acc: 0.9493\n",
      "Epoch 1525/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9473\n",
      "Epoch 01525: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0286 - acc: 0.9472\n",
      "Epoch 1526/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9510\n",
      "Epoch 01526: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0268 - acc: 0.9511\n",
      "Epoch 1527/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9525\n",
      "Epoch 01527: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0262 - acc: 0.9526\n",
      "Epoch 1528/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9466\n",
      "Epoch 01528: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0291 - acc: 0.9464\n",
      "Epoch 1529/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9510\n",
      "Epoch 01529: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0265 - acc: 0.9511\n",
      "Epoch 1530/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9498\n",
      "Epoch 01530: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0274 - acc: 0.9498\n",
      "Epoch 1531/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9467\n",
      "Epoch 01531: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0290 - acc: 0.9468\n",
      "Epoch 1532/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9479\n",
      "Epoch 01532: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0286 - acc: 0.9480\n",
      "Epoch 1533/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9505\n",
      "Epoch 01533: loss did not improve\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.0272 - acc: 0.9505\n",
      "Epoch 1534/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9480\n",
      "Epoch 01534: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0283 - acc: 0.9479\n",
      "Epoch 1535/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9497\n",
      "Epoch 01535: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0274 - acc: 0.9498\n",
      "Epoch 1536/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9491\n",
      "Epoch 01536: loss did not improve\n",
      "100/100 [==============================] - 49s 485ms/step - loss: 0.0278 - acc: 0.9492\n",
      "Epoch 1537/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9466\n",
      "Epoch 01537: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0295 - acc: 0.9464\n",
      "Epoch 1538/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9482\n",
      "Epoch 01538: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0285 - acc: 0.9480\n",
      "Epoch 1539/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9497\n",
      "Epoch 01539: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0278 - acc: 0.9497\n",
      "Epoch 1540/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9535\n",
      "Epoch 01540: loss improved from 0.02564 to 0.02548, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0255 - acc: 0.9535\n",
      "Epoch 1541/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9526\n",
      "Epoch 01541: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0262 - acc: 0.9527\n",
      "Epoch 1542/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9498\n",
      "Epoch 01542: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0274 - acc: 0.9499\n",
      "Epoch 1543/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9519\n",
      "Epoch 01543: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0264 - acc: 0.9519\n",
      "Epoch 1544/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9514\n",
      "Epoch 01544: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0266 - acc: 0.9513\n",
      "Epoch 1545/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9495\n",
      "Epoch 01545: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0275 - acc: 0.9496\n",
      "Epoch 1546/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9511\n",
      "Epoch 01546: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0264 - acc: 0.9512\n",
      "Epoch 1547/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9499\n",
      "Epoch 01547: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0282 - acc: 0.9492\n",
      "Epoch 1548/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9481\n",
      "Epoch 01548: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0287 - acc: 0.9482\n",
      "Epoch 1549/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9501\n",
      "Epoch 01549: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0272 - acc: 0.9503\n",
      "Epoch 1550/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9496\n",
      "Epoch 01550: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0276 - acc: 0.9497\n",
      "Epoch 1551/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9522\n",
      "Epoch 01551: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0260 - acc: 0.9521\n",
      "Epoch 1552/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9510\n",
      "Epoch 01552: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0265 - acc: 0.9511\n",
      "Epoch 1553/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9506\n",
      "Epoch 01553: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0272 - acc: 0.9505\n",
      "Epoch 1554/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9520\n",
      "Epoch 01554: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0263 - acc: 0.9521\n",
      "Epoch 1555/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9499\n",
      "Epoch 01555: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0277 - acc: 0.9499\n",
      "Epoch 1556/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9497\n",
      "Epoch 01556: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0276 - acc: 0.9497\n",
      "Epoch 1557/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 01557: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0284 - acc: 0.9482\n",
      "Epoch 1558/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9495\n",
      "Epoch 01558: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0275 - acc: 0.9496\n",
      "Epoch 1559/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9480\n",
      "Epoch 01559: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0287 - acc: 0.9480\n",
      "Epoch 1560/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9513\n",
      "Epoch 01560: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0269 - acc: 0.9513\n",
      "Epoch 1561/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9484\n",
      "Epoch 01561: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0279 - acc: 0.9485\n",
      "Epoch 1562/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9487\n",
      "Epoch 01562: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0282 - acc: 0.9484\n",
      "Epoch 1563/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9490\n",
      "Epoch 01563: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0276 - acc: 0.9492\n",
      "Epoch 1564/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9481\n",
      "Epoch 01564: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0287 - acc: 0.9480\n",
      "Epoch 1565/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9514\n",
      "Epoch 01565: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0265 - acc: 0.9513\n",
      "Epoch 1566/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9501\n",
      "Epoch 01566: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0273 - acc: 0.9502\n",
      "Epoch 1567/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9504\n",
      "Epoch 01567: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0273 - acc: 0.9502\n",
      "Epoch 1568/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9502\n",
      "Epoch 01568: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0270 - acc: 0.9502\n",
      "Epoch 1569/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9516\n",
      "Epoch 01569: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0265 - acc: 0.9514\n",
      "Epoch 1570/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9487\n",
      "Epoch 01570: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0286 - acc: 0.9486\n",
      "Epoch 1571/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9506\n",
      "Epoch 01571: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0270 - acc: 0.9505\n",
      "Epoch 1572/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9501\n",
      "Epoch 01572: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0277 - acc: 0.9501\n",
      "Epoch 1573/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9489\n",
      "Epoch 01573: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0280 - acc: 0.9488\n",
      "Epoch 1574/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9534\n",
      "Epoch 01574: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0256 - acc: 0.9534\n",
      "Epoch 1575/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9515\n",
      "Epoch 01575: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0269 - acc: 0.9514\n",
      "Epoch 1576/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9497\n",
      "Epoch 01576: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0274 - acc: 0.9495\n",
      "Epoch 1577/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9473\n",
      "Epoch 01577: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0289 - acc: 0.9473\n",
      "Epoch 1578/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9479\n",
      "Epoch 01578: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0297 - acc: 0.9476\n",
      "Epoch 1579/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9475\n",
      "Epoch 01579: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0286 - acc: 0.9476\n",
      "Epoch 1580/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9504\n",
      "Epoch 01580: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0266 - acc: 0.9505\n",
      "Epoch 1581/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9509\n",
      "Epoch 01581: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0266 - acc: 0.9508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1582/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9511\n",
      "Epoch 01582: loss did not improve\n",
      "100/100 [==============================] - 48s 485ms/step - loss: 0.0266 - acc: 0.9511\n",
      "Epoch 1583/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9482\n",
      "Epoch 01583: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0282 - acc: 0.9482\n",
      "Epoch 1584/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9506\n",
      "Epoch 01584: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0271 - acc: 0.9506\n",
      "Epoch 1585/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9494\n",
      "Epoch 01585: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0279 - acc: 0.9493\n",
      "Epoch 1586/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9514\n",
      "Epoch 01586: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0264 - acc: 0.9515\n",
      "Epoch 1587/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9496\n",
      "Epoch 01587: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0274 - acc: 0.9495\n",
      "Epoch 1588/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9513\n",
      "Epoch 01588: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0268 - acc: 0.9512\n",
      "Epoch 1589/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9483\n",
      "Epoch 01589: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0288 - acc: 0.9483\n",
      "Epoch 1590/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9526\n",
      "Epoch 01590: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0258 - acc: 0.9528\n",
      "Epoch 1591/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9501\n",
      "Epoch 01591: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0274 - acc: 0.9499\n",
      "Epoch 1592/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9494\n",
      "Epoch 01592: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0278 - acc: 0.9493\n",
      "Epoch 1593/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9514\n",
      "Epoch 01593: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0266 - acc: 0.9515\n",
      "Epoch 1594/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9493\n",
      "Epoch 01594: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0278 - acc: 0.9492\n",
      "Epoch 1595/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9536\n",
      "Epoch 01595: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0258 - acc: 0.9533\n",
      "Epoch 1596/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9497\n",
      "Epoch 01596: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0274 - acc: 0.9499\n",
      "Epoch 1597/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9494\n",
      "Epoch 01597: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0277 - acc: 0.9494\n",
      "Epoch 1598/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9466\n",
      "Epoch 01598: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0293 - acc: 0.9467\n",
      "Epoch 1599/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9483\n",
      "Epoch 01599: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0277 - acc: 0.9484\n",
      "Epoch 1600/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9497\n",
      "Epoch 01600: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0274 - acc: 0.9496\n",
      "Epoch 1601/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9521\n",
      "Epoch 01601: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0265 - acc: 0.9521\n",
      "Epoch 1602/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9499\n",
      "Epoch 01602: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0274 - acc: 0.9497\n",
      "Epoch 1603/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9480\n",
      "Epoch 01603: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0285 - acc: 0.9480\n",
      "Epoch 1604/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9506\n",
      "Epoch 01604: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0271 - acc: 0.9506\n",
      "Epoch 1605/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9503\n",
      "Epoch 01605: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0274 - acc: 0.9502\n",
      "Epoch 1606/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9537\n",
      "Epoch 01606: loss did not improve\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.0262 - acc: 0.9533\n",
      "Epoch 1607/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9496\n",
      "Epoch 01607: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0277 - acc: 0.9496\n",
      "Epoch 1608/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9515\n",
      "Epoch 01608: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0265 - acc: 0.9516\n",
      "Epoch 1609/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9503\n",
      "Epoch 01609: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0270 - acc: 0.9500\n",
      "Epoch 1610/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9514\n",
      "Epoch 01610: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0266 - acc: 0.9516\n",
      "Epoch 1611/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9516\n",
      "Epoch 01611: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0268 - acc: 0.9516\n",
      "Epoch 1612/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9480\n",
      "Epoch 01612: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0283 - acc: 0.9483\n",
      "Epoch 1613/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9485\n",
      "Epoch 01613: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0283 - acc: 0.9486\n",
      "Epoch 1614/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9496\n",
      "Epoch 01614: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0277 - acc: 0.9496\n",
      "Epoch 1615/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9503\n",
      "Epoch 01615: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0269 - acc: 0.9505\n",
      "Epoch 1616/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9494\n",
      "Epoch 01616: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0279 - acc: 0.9495\n",
      "Epoch 1617/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9501\n",
      "Epoch 01617: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0273 - acc: 0.9501\n",
      "Epoch 1618/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9486\n",
      "Epoch 01618: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0276 - acc: 0.9487\n",
      "Epoch 1619/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9501\n",
      "Epoch 01619: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0275 - acc: 0.9502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1620/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9500\n",
      "Epoch 01620: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0277 - acc: 0.9498\n",
      "Epoch 1621/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9501\n",
      "Epoch 01621: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0270 - acc: 0.9503\n",
      "Epoch 1622/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9468\n",
      "Epoch 01622: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0291 - acc: 0.9468\n",
      "Epoch 1623/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9489\n",
      "Epoch 01623: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0281 - acc: 0.9488\n",
      "Epoch 1624/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9489\n",
      "Epoch 01624: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0275 - acc: 0.9489\n",
      "Epoch 1625/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9507\n",
      "Epoch 01625: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0270 - acc: 0.9506\n",
      "Epoch 1626/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9489\n",
      "Epoch 01626: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0281 - acc: 0.9484\n",
      "Epoch 1627/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9487\n",
      "Epoch 01627: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0280 - acc: 0.9488\n",
      "Epoch 1628/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9506\n",
      "Epoch 01628: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0271 - acc: 0.9505\n",
      "Epoch 1629/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9482\n",
      "Epoch 01629: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0280 - acc: 0.9484\n",
      "Epoch 1630/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9496\n",
      "Epoch 01630: loss did not improve\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.0277 - acc: 0.9496\n",
      "Epoch 1631/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9487\n",
      "Epoch 01631: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0278 - acc: 0.9488\n",
      "Epoch 1632/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9485\n",
      "Epoch 01632: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0285 - acc: 0.9486\n",
      "Epoch 1633/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9466\n",
      "Epoch 01633: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0293 - acc: 0.9466\n",
      "Epoch 1634/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9475\n",
      "Epoch 01634: loss did not improve\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.0282 - acc: 0.9475\n",
      "Epoch 1635/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9503\n",
      "Epoch 01635: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0271 - acc: 0.9503\n",
      "Epoch 1636/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9506\n",
      "Epoch 01636: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0273 - acc: 0.9506\n",
      "Epoch 1637/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9495\n",
      "Epoch 01637: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0272 - acc: 0.9496\n",
      "Epoch 1638/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9527\n",
      "Epoch 01638: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0261 - acc: 0.9527\n",
      "Epoch 1639/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9517\n",
      "Epoch 01639: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0264 - acc: 0.9518\n",
      "Epoch 1640/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9525\n",
      "Epoch 01640: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0259 - acc: 0.9526\n",
      "Epoch 1641/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9510\n",
      "Epoch 01641: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0270 - acc: 0.9512\n",
      "Epoch 1642/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9514\n",
      "Epoch 01642: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0264 - acc: 0.9515\n",
      "Epoch 1643/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9468\n",
      "Epoch 01643: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0290 - acc: 0.9468\n",
      "Epoch 1644/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9489\n",
      "Epoch 01644: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0283 - acc: 0.9490\n",
      "Epoch 1645/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9488\n",
      "Epoch 01645: loss did not improve\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.0279 - acc: 0.9487\n",
      "Epoch 1646/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9505\n",
      "Epoch 01646: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0273 - acc: 0.9502\n",
      "Epoch 1647/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9498\n",
      "Epoch 01647: loss did not improve\n",
      "100/100 [==============================] - 49s 487ms/step - loss: 0.0273 - acc: 0.9499\n",
      "Epoch 1648/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9480\n",
      "Epoch 01648: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0283 - acc: 0.9481\n",
      "Epoch 1649/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9514\n",
      "Epoch 01649: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0268 - acc: 0.9515\n",
      "Epoch 1650/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9506\n",
      "Epoch 01650: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0268 - acc: 0.9507\n",
      "Epoch 1651/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9473\n",
      "Epoch 01651: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0290 - acc: 0.9471\n",
      "Epoch 1652/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9492\n",
      "Epoch 01652: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0277 - acc: 0.9494\n",
      "Epoch 1653/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9514\n",
      "Epoch 01653: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0263 - acc: 0.9514\n",
      "Epoch 1654/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9505\n",
      "Epoch 01654: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0272 - acc: 0.9506\n",
      "Epoch 1655/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9501\n",
      "Epoch 01655: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0275 - acc: 0.9501\n",
      "Epoch 1656/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9492\n",
      "Epoch 01656: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0282 - acc: 0.9493\n",
      "Epoch 1657/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9480\n",
      "Epoch 01657: loss did not improve\n",
      "100/100 [==============================] - 48s 481ms/step - loss: 0.0281 - acc: 0.9482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1658/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9499\n",
      "Epoch 01658: loss did not improve\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.0275 - acc: 0.9499\n",
      "Epoch 1659/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9492\n",
      "Epoch 01659: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0276 - acc: 0.9493\n",
      "Epoch 1660/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9510\n",
      "Epoch 01660: loss did not improve\n",
      "100/100 [==============================] - 49s 494ms/step - loss: 0.0269 - acc: 0.9508\n",
      "Epoch 1661/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9499\n",
      "Epoch 01661: loss did not improve\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.0272 - acc: 0.9498\n",
      "Epoch 1662/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9460\n",
      "Epoch 01662: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0293 - acc: 0.9461\n",
      "Epoch 1663/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9500\n",
      "Epoch 01663: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0272 - acc: 0.9500\n",
      "Epoch 1664/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9486\n",
      "Epoch 01664: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0281 - acc: 0.9488\n",
      "Epoch 1665/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9501\n",
      "Epoch 01665: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0272 - acc: 0.9501\n",
      "Epoch 1666/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9502\n",
      "Epoch 01666: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0274 - acc: 0.9503\n",
      "Epoch 1667/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9508\n",
      "Epoch 01667: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0268 - acc: 0.9510\n",
      "Epoch 1668/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9472\n",
      "Epoch 01668: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0287 - acc: 0.9473\n",
      "Epoch 1669/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9520\n",
      "Epoch 01669: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0263 - acc: 0.9521\n",
      "Epoch 1670/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9498\n",
      "Epoch 01670: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0276 - acc: 0.9497\n",
      "Epoch 1671/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9506\n",
      "Epoch 01671: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0275 - acc: 0.9507\n",
      "Epoch 1672/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9495\n",
      "Epoch 01672: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0277 - acc: 0.9496\n",
      "Epoch 1673/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9499\n",
      "Epoch 01673: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0273 - acc: 0.9499\n",
      "Epoch 1674/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9504\n",
      "Epoch 01674: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0272 - acc: 0.9503\n",
      "Epoch 1675/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9501\n",
      "Epoch 01675: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0272 - acc: 0.9502\n",
      "Epoch 1676/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9510\n",
      "Epoch 01676: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0267 - acc: 0.9510\n",
      "Epoch 1677/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9478\n",
      "Epoch 01677: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0289 - acc: 0.9474\n",
      "Epoch 1678/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9481\n",
      "Epoch 01678: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0285 - acc: 0.9482\n",
      "Epoch 1679/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9525\n",
      "Epoch 01679: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0260 - acc: 0.9525\n",
      "Epoch 1680/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9499\n",
      "Epoch 01680: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0278 - acc: 0.9498\n",
      "Epoch 1681/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9494\n",
      "Epoch 01681: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0277 - acc: 0.9493\n",
      "Epoch 1682/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9535\n",
      "Epoch 01682: loss improved from 0.02548 to 0.02548, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.0255 - acc: 0.9535\n",
      "Epoch 1683/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9514\n",
      "Epoch 01683: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0264 - acc: 0.9515\n",
      "Epoch 1684/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9480\n",
      "Epoch 01684: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0289 - acc: 0.9481\n",
      "Epoch 1685/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9485\n",
      "Epoch 01685: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0285 - acc: 0.9483\n",
      "Epoch 1686/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9510\n",
      "Epoch 01686: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0266 - acc: 0.9510\n",
      "Epoch 1687/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9509\n",
      "Epoch 01687: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0271 - acc: 0.9510\n",
      "Epoch 1688/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9487\n",
      "Epoch 01688: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0282 - acc: 0.9485\n",
      "Epoch 1689/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9507\n",
      "Epoch 01689: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0269 - acc: 0.9508\n",
      "Epoch 1690/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9484\n",
      "Epoch 01690: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0284 - acc: 0.9481\n",
      "Epoch 1691/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9491\n",
      "Epoch 01691: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0278 - acc: 0.9492\n",
      "Epoch 1692/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9511\n",
      "Epoch 01692: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0269 - acc: 0.9509\n",
      "Epoch 1693/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9484\n",
      "Epoch 01693: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0283 - acc: 0.9485\n",
      "Epoch 1694/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9514\n",
      "Epoch 01694: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0265 - acc: 0.9515\n",
      "Epoch 1695/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9497\n",
      "Epoch 01695: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0273 - acc: 0.9496\n",
      "Epoch 1696/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9497\n",
      "Epoch 01696: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0277 - acc: 0.9497\n",
      "Epoch 1697/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9499\n",
      "Epoch 01697: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0271 - acc: 0.9500\n",
      "Epoch 1698/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9506\n",
      "Epoch 01698: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0270 - acc: 0.9503\n",
      "Epoch 1699/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9498\n",
      "Epoch 01699: loss did not improve\n",
      "100/100 [==============================] - 48s 479ms/step - loss: 0.0275 - acc: 0.9498\n",
      "Epoch 1700/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9498\n",
      "Epoch 01700: loss did not improve\n",
      "100/100 [==============================] - 48s 482ms/step - loss: 0.0272 - acc: 0.9496\n",
      "Epoch 1701/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9486\n",
      "Epoch 01701: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0279 - acc: 0.9487\n",
      "Epoch 1702/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9497\n",
      "Epoch 01702: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0273 - acc: 0.9498\n",
      "Epoch 1703/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9508\n",
      "Epoch 01703: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0265 - acc: 0.9508\n",
      "Epoch 1704/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9511\n",
      "Epoch 01704: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0263 - acc: 0.9511\n",
      "Epoch 1705/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9483\n",
      "Epoch 01705: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0286 - acc: 0.9480\n",
      "Epoch 1706/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9501\n",
      "Epoch 01706: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0270 - acc: 0.9500\n",
      "Epoch 1707/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9485\n",
      "Epoch 01707: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0281 - acc: 0.9485\n",
      "Epoch 1708/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9516\n",
      "Epoch 01708: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0267 - acc: 0.9516\n",
      "Epoch 1709/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9530\n",
      "Epoch 01709: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0259 - acc: 0.9529\n",
      "Epoch 1710/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9503\n",
      "Epoch 01710: loss did not improve\n",
      "100/100 [==============================] - 39s 385ms/step - loss: 0.0271 - acc: 0.9504\n",
      "Epoch 1711/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9497\n",
      "Epoch 01711: loss did not improve\n",
      "100/100 [==============================] - 38s 385ms/step - loss: 0.0275 - acc: 0.9498\n",
      "Epoch 1712/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9493\n",
      "Epoch 01712: loss did not improve\n",
      "100/100 [==============================] - 38s 385ms/step - loss: 0.0276 - acc: 0.9495\n",
      "Epoch 1713/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9514\n",
      "Epoch 01713: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0268 - acc: 0.9515\n",
      "Epoch 1714/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9509\n",
      "Epoch 01714: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0271 - acc: 0.9510\n",
      "Epoch 1715/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9496\n",
      "Epoch 01715: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0278 - acc: 0.9498\n",
      "Epoch 1716/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9511\n",
      "Epoch 01716: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0265 - acc: 0.9510\n",
      "Epoch 1717/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9518\n",
      "Epoch 01717: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0263 - acc: 0.9519\n",
      "Epoch 1718/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9510\n",
      "Epoch 01718: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0263 - acc: 0.9511\n",
      "Epoch 1719/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9509\n",
      "Epoch 01719: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0268 - acc: 0.9509\n",
      "Epoch 1720/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9516\n",
      "Epoch 01720: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0264 - acc: 0.9517\n",
      "Epoch 1721/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9521\n",
      "Epoch 01721: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0261 - acc: 0.9522\n",
      "Epoch 1722/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9521\n",
      "Epoch 01722: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0260 - acc: 0.9521\n",
      "Epoch 1723/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9492\n",
      "Epoch 01723: loss did not improve\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.0279 - acc: 0.9490\n",
      "Epoch 1724/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 01724: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0284 - acc: 0.9482\n",
      "Epoch 1725/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9509\n",
      "Epoch 01725: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0267 - acc: 0.9509\n",
      "Epoch 1726/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9512\n",
      "Epoch 01726: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0270 - acc: 0.9508\n",
      "Epoch 1727/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9510\n",
      "Epoch 01727: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0266 - acc: 0.9511\n",
      "Epoch 1728/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9495\n",
      "Epoch 01728: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0276 - acc: 0.9493\n",
      "Epoch 1729/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9522\n",
      "Epoch 01729: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0259 - acc: 0.9523\n",
      "Epoch 1730/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9480\n",
      "Epoch 01730: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0282 - acc: 0.9480\n",
      "Epoch 1731/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9494\n",
      "Epoch 01731: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0272 - acc: 0.9493\n",
      "Epoch 1732/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9478\n",
      "Epoch 01732: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0284 - acc: 0.9480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1733/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9478\n",
      "Epoch 01733: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0282 - acc: 0.9478\n",
      "Epoch 1734/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9504\n",
      "Epoch 01734: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0270 - acc: 0.9506\n",
      "Epoch 1735/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9510\n",
      "Epoch 01735: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0269 - acc: 0.9511\n",
      "Epoch 1736/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9475\n",
      "Epoch 01736: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0288 - acc: 0.9475\n",
      "Epoch 1737/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9522\n",
      "Epoch 01737: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0260 - acc: 0.9523\n",
      "Epoch 1738/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9490\n",
      "Epoch 01738: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0276 - acc: 0.9492\n",
      "Epoch 1739/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9498\n",
      "Epoch 01739: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0272 - acc: 0.9500\n",
      "Epoch 1740/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9479\n",
      "Epoch 01740: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0287 - acc: 0.9480\n",
      "Epoch 1741/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9489\n",
      "Epoch 01741: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0273 - acc: 0.9491\n",
      "Epoch 1742/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9497\n",
      "Epoch 01742: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0274 - acc: 0.9498\n",
      "Epoch 1743/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9484\n",
      "Epoch 01743: loss did not improve\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.0281 - acc: 0.9485\n",
      "Epoch 1744/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9478\n",
      "Epoch 01744: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0283 - acc: 0.9480\n",
      "Epoch 1745/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9479\n",
      "Epoch 01745: loss did not improve\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.0284 - acc: 0.9479\n",
      "Epoch 1746/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9504\n",
      "Epoch 01746: loss did not improve\n",
      "100/100 [==============================] - 40s 397ms/step - loss: 0.0274 - acc: 0.9505\n",
      "Epoch 1747/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9510\n",
      "Epoch 01747: loss did not improve\n",
      "100/100 [==============================] - 40s 401ms/step - loss: 0.0263 - acc: 0.9510\n",
      "Epoch 1748/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9521\n",
      "Epoch 01748: loss did not improve\n",
      "100/100 [==============================] - 42s 419ms/step - loss: 0.0260 - acc: 0.9520\n",
      "Epoch 1749/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9526\n",
      "Epoch 01749: loss did not improve\n",
      "100/100 [==============================] - 41s 411ms/step - loss: 0.0258 - acc: 0.9528\n",
      "Epoch 1750/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9510\n",
      "Epoch 01750: loss did not improve\n",
      "100/100 [==============================] - 41s 407ms/step - loss: 0.0270 - acc: 0.9511\n",
      "Epoch 1751/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9497\n",
      "Epoch 01751: loss did not improve\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.0273 - acc: 0.9500\n",
      "Epoch 1752/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9511\n",
      "Epoch 01752: loss did not improve\n",
      "100/100 [==============================] - 41s 413ms/step - loss: 0.0265 - acc: 0.9508\n",
      "Epoch 1753/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9520\n",
      "Epoch 01753: loss did not improve\n",
      "100/100 [==============================] - 42s 423ms/step - loss: 0.0261 - acc: 0.9520\n",
      "Epoch 1754/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9515\n",
      "Epoch 01754: loss did not improve\n",
      "100/100 [==============================] - 41s 413ms/step - loss: 0.0266 - acc: 0.9515\n",
      "Epoch 1755/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9518\n",
      "Epoch 01755: loss did not improve\n",
      "100/100 [==============================] - 42s 418ms/step - loss: 0.0264 - acc: 0.9518\n",
      "Epoch 1756/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9505\n",
      "Epoch 01756: loss did not improve\n",
      "100/100 [==============================] - 42s 416ms/step - loss: 0.0268 - acc: 0.9505\n",
      "Epoch 1757/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9508\n",
      "Epoch 01757: loss did not improve\n",
      "100/100 [==============================] - 42s 424ms/step - loss: 0.0265 - acc: 0.9509\n",
      "Epoch 1758/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9498\n",
      "Epoch 01758: loss did not improve\n",
      "100/100 [==============================] - 43s 433ms/step - loss: 0.0273 - acc: 0.9497\n",
      "Epoch 1759/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9516\n",
      "Epoch 01759: loss did not improve\n",
      "100/100 [==============================] - 44s 436ms/step - loss: 0.0264 - acc: 0.9515\n",
      "Epoch 1760/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9506\n",
      "Epoch 01760: loss did not improve\n",
      "100/100 [==============================] - 43s 425ms/step - loss: 0.0272 - acc: 0.9505\n",
      "Epoch 1761/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9502\n",
      "Epoch 01761: loss did not improve\n",
      "100/100 [==============================] - 43s 432ms/step - loss: 0.0272 - acc: 0.9498\n",
      "Epoch 1762/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9519\n",
      "Epoch 01762: loss did not improve\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.0263 - acc: 0.9518\n",
      "Epoch 1763/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9488\n",
      "Epoch 01763: loss did not improve\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.0276 - acc: 0.9490\n",
      "Epoch 1764/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9486\n",
      "Epoch 01764: loss did not improve\n",
      "100/100 [==============================] - 44s 436ms/step - loss: 0.0282 - acc: 0.9486\n",
      "Epoch 1765/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9510\n",
      "Epoch 01765: loss did not improve\n",
      "100/100 [==============================] - 43s 427ms/step - loss: 0.0262 - acc: 0.9512\n",
      "Epoch 1766/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9467\n",
      "Epoch 01766: loss did not improve\n",
      "100/100 [==============================] - 43s 433ms/step - loss: 0.0289 - acc: 0.9467\n",
      "Epoch 1767/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9495\n",
      "Epoch 01767: loss did not improve\n",
      "100/100 [==============================] - 43s 432ms/step - loss: 0.0272 - acc: 0.9495\n",
      "Epoch 1768/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9526\n",
      "Epoch 01768: loss did not improve\n",
      "100/100 [==============================] - 43s 433ms/step - loss: 0.0259 - acc: 0.9526\n",
      "Epoch 1769/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9473\n",
      "Epoch 01769: loss did not improve\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 0.0287 - acc: 0.9470\n",
      "Epoch 1770/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9503\n",
      "Epoch 01770: loss did not improve\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.0269 - acc: 0.9504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1771/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9478\n",
      "Epoch 01771: loss did not improve\n",
      "100/100 [==============================] - 43s 431ms/step - loss: 0.0281 - acc: 0.9480\n",
      "Epoch 1772/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9524\n",
      "Epoch 01772: loss did not improve\n",
      "100/100 [==============================] - 44s 443ms/step - loss: 0.0264 - acc: 0.9523\n",
      "Epoch 1773/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9512\n",
      "Epoch 01773: loss did not improve\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.0265 - acc: 0.9513\n",
      "Epoch 1774/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9499\n",
      "Epoch 01774: loss did not improve\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0269 - acc: 0.9500\n",
      "Epoch 1775/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9496\n",
      "Epoch 01775: loss did not improve\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.0269 - acc: 0.9496\n",
      "Epoch 1776/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9486\n",
      "Epoch 01776: loss did not improve\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.0285 - acc: 0.9487\n",
      "Epoch 1777/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9496\n",
      "Epoch 01777: loss did not improve\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0275 - acc: 0.9498\n",
      "Epoch 1778/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9501\n",
      "Epoch 01778: loss did not improve\n",
      "100/100 [==============================] - 44s 445ms/step - loss: 0.0271 - acc: 0.9499\n",
      "Epoch 1779/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9515\n",
      "Epoch 01779: loss did not improve\n",
      "100/100 [==============================] - 45s 446ms/step - loss: 0.0263 - acc: 0.9516\n",
      "Epoch 1780/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9495\n",
      "Epoch 01780: loss did not improve\n",
      "100/100 [==============================] - 44s 439ms/step - loss: 0.0274 - acc: 0.9496\n",
      "Epoch 1781/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9535\n",
      "Epoch 01781: loss did not improve\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.0256 - acc: 0.9533\n",
      "Epoch 1782/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9509\n",
      "Epoch 01782: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0267 - acc: 0.9509\n",
      "Epoch 1783/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9463\n",
      "Epoch 01783: loss did not improve\n",
      "100/100 [==============================] - 45s 445ms/step - loss: 0.0292 - acc: 0.9464\n",
      "Epoch 1784/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9474\n",
      "Epoch 01784: loss did not improve\n",
      "100/100 [==============================] - 44s 438ms/step - loss: 0.0293 - acc: 0.9471\n",
      "Epoch 1785/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9490\n",
      "Epoch 01785: loss did not improve\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.0279 - acc: 0.9489\n",
      "Epoch 1786/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9517\n",
      "Epoch 01786: loss did not improve\n",
      "100/100 [==============================] - 44s 444ms/step - loss: 0.0261 - acc: 0.9518\n",
      "Epoch 1787/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9502\n",
      "Epoch 01787: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0273 - acc: 0.9503\n",
      "Epoch 1788/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9487\n",
      "Epoch 01788: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0279 - acc: 0.9488\n",
      "Epoch 1789/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9471\n",
      "Epoch 01789: loss did not improve\n",
      "100/100 [==============================] - 44s 442ms/step - loss: 0.0290 - acc: 0.9472\n",
      "Epoch 1790/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9485\n",
      "Epoch 01790: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0276 - acc: 0.9485\n",
      "Epoch 1791/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9486\n",
      "Epoch 01791: loss did not improve\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.0282 - acc: 0.9485\n",
      "Epoch 1792/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9482\n",
      "Epoch 01792: loss did not improve\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.0280 - acc: 0.9483\n",
      "Epoch 1793/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9479\n",
      "Epoch 01793: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0287 - acc: 0.9479\n",
      "Epoch 1794/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9477\n",
      "Epoch 01794: loss did not improve\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.0284 - acc: 0.9474\n",
      "Epoch 1795/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9491\n",
      "Epoch 01795: loss did not improve\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 0.0278 - acc: 0.9490\n",
      "Epoch 1796/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9504\n",
      "Epoch 01796: loss did not improve\n",
      "100/100 [==============================] - 45s 455ms/step - loss: 0.0273 - acc: 0.9503\n",
      "Epoch 1797/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9520\n",
      "Epoch 01797: loss did not improve\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.0260 - acc: 0.9521\n",
      "Epoch 1798/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9509\n",
      "Epoch 01798: loss did not improve\n",
      "100/100 [==============================] - 43s 434ms/step - loss: 0.0268 - acc: 0.9509\n",
      "Epoch 1799/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9458\n",
      "Epoch 01799: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0288 - acc: 0.9460\n",
      "Epoch 1800/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9505\n",
      "Epoch 01800: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0269 - acc: 0.9506\n",
      "Epoch 1801/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9511\n",
      "Epoch 01801: loss did not improve\n",
      "100/100 [==============================] - 44s 440ms/step - loss: 0.0269 - acc: 0.9511\n",
      "Epoch 1802/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9506\n",
      "Epoch 01802: loss did not improve\n",
      "100/100 [==============================] - 45s 447ms/step - loss: 0.0273 - acc: 0.9505\n",
      "Epoch 1803/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9499\n",
      "Epoch 01803: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0270 - acc: 0.9498\n",
      "Epoch 1804/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9485\n",
      "Epoch 01804: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0281 - acc: 0.9484\n",
      "Epoch 1805/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9486\n",
      "Epoch 01805: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0280 - acc: 0.9487\n",
      "Epoch 1806/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9494\n",
      "Epoch 01806: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0274 - acc: 0.9496\n",
      "Epoch 1807/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9491\n",
      "Epoch 01807: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0277 - acc: 0.9494\n",
      "Epoch 1808/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9503\n",
      "Epoch 01808: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0270 - acc: 0.9502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1809/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9513\n",
      "Epoch 01809: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0267 - acc: 0.9513\n",
      "Epoch 1810/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9496\n",
      "Epoch 01810: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0278 - acc: 0.9496\n",
      "Epoch 1811/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9504\n",
      "Epoch 01811: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0271 - acc: 0.9500\n",
      "Epoch 1812/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9485\n",
      "Epoch 01812: loss did not improve\n",
      "100/100 [==============================] - 46s 455ms/step - loss: 0.0280 - acc: 0.9484\n",
      "Epoch 1813/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9484\n",
      "Epoch 01813: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0284 - acc: 0.9484\n",
      "Epoch 1814/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9491\n",
      "Epoch 01814: loss did not improve\n",
      "100/100 [==============================] - 45s 450ms/step - loss: 0.0275 - acc: 0.9491\n",
      "Epoch 1815/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9486\n",
      "Epoch 01815: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0281 - acc: 0.9487\n",
      "Epoch 1816/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9496\n",
      "Epoch 01816: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0275 - acc: 0.9497\n",
      "Epoch 1817/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9513\n",
      "Epoch 01817: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0265 - acc: 0.9514\n",
      "Epoch 1818/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9498\n",
      "Epoch 01818: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0274 - acc: 0.9499\n",
      "Epoch 1819/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9502\n",
      "Epoch 01819: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0269 - acc: 0.9503\n",
      "Epoch 1820/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9513\n",
      "Epoch 01820: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0264 - acc: 0.9513\n",
      "Epoch 1821/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9484\n",
      "Epoch 01821: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0277 - acc: 0.9485\n",
      "Epoch 1822/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9463\n",
      "Epoch 01822: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0294 - acc: 0.9463\n",
      "Epoch 1823/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9505\n",
      "Epoch 01823: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0266 - acc: 0.9505\n",
      "Epoch 1824/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9530\n",
      "Epoch 01824: loss did not improve\n",
      "100/100 [==============================] - 48s 484ms/step - loss: 0.0257 - acc: 0.9530\n",
      "Epoch 1825/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9500\n",
      "Epoch 01825: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0277 - acc: 0.9499\n",
      "Epoch 1826/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9486\n",
      "Epoch 01826: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0280 - acc: 0.9486\n",
      "Epoch 1827/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9507\n",
      "Epoch 01827: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0265 - acc: 0.9507\n",
      "Epoch 1828/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9509\n",
      "Epoch 01828: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0268 - acc: 0.9509\n",
      "Epoch 1829/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9490\n",
      "Epoch 01829: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0281 - acc: 0.9490\n",
      "Epoch 1830/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9488\n",
      "Epoch 01830: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0282 - acc: 0.9489\n",
      "Epoch 1831/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9484\n",
      "Epoch 01831: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0285 - acc: 0.9484\n",
      "Epoch 1832/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9530\n",
      "Epoch 01832: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0257 - acc: 0.9530\n",
      "Epoch 1833/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9510\n",
      "Epoch 01833: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0269 - acc: 0.9509\n",
      "Epoch 1834/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9497\n",
      "Epoch 01834: loss did not improve\n",
      "100/100 [==============================] - 46s 462ms/step - loss: 0.0276 - acc: 0.9497\n",
      "Epoch 1835/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9492\n",
      "Epoch 01835: loss did not improve\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.0277 - acc: 0.9492\n",
      "Epoch 1836/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9469\n",
      "Epoch 01836: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0292 - acc: 0.9471\n",
      "Epoch 1837/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9509\n",
      "Epoch 01837: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0270 - acc: 0.9507\n",
      "Epoch 1838/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9498\n",
      "Epoch 01838: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0272 - acc: 0.9494\n",
      "Epoch 1839/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9511\n",
      "Epoch 01839: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0269 - acc: 0.9512\n",
      "Epoch 1840/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9506\n",
      "Epoch 01840: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0271 - acc: 0.9504\n",
      "Epoch 1841/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9476\n",
      "Epoch 01841: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0285 - acc: 0.9478\n",
      "Epoch 1842/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9496\n",
      "Epoch 01842: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0273 - acc: 0.9497\n",
      "Epoch 1843/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9462\n",
      "Epoch 01843: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0295 - acc: 0.9463\n",
      "Epoch 1844/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9487\n",
      "Epoch 01844: loss did not improve\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 0.0280 - acc: 0.9487\n",
      "Epoch 1845/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9481\n",
      "Epoch 01845: loss did not improve\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.0286 - acc: 0.9478\n",
      "Epoch 1846/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9524\n",
      "Epoch 01846: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0256 - acc: 0.9525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1847/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9543\n",
      "Epoch 01847: loss improved from 0.02548 to 0.02474, saving model to net_segnet_aug_custom metric.hdf5\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0247 - acc: 0.9545\n",
      "Epoch 1848/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9518\n",
      "Epoch 01848: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0264 - acc: 0.9519\n",
      "Epoch 1849/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9463\n",
      "Epoch 01849: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0293 - acc: 0.9465\n",
      "Epoch 1850/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9515\n",
      "Epoch 01850: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0266 - acc: 0.9514\n",
      "Epoch 1851/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9515\n",
      "Epoch 01851: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0267 - acc: 0.9515\n",
      "Epoch 1852/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9507\n",
      "Epoch 01852: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0270 - acc: 0.9507\n",
      "Epoch 1853/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9519\n",
      "Epoch 01853: loss did not improve\n",
      "100/100 [==============================] - 46s 461ms/step - loss: 0.0262 - acc: 0.9520\n",
      "Epoch 1854/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9513\n",
      "Epoch 01854: loss did not improve\n",
      "100/100 [==============================] - 45s 449ms/step - loss: 0.0269 - acc: 0.9513\n",
      "Epoch 1855/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9491\n",
      "Epoch 01855: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0280 - acc: 0.9490\n",
      "Epoch 1856/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9528\n",
      "Epoch 01856: loss did not improve\n",
      "100/100 [==============================] - 46s 456ms/step - loss: 0.0256 - acc: 0.9528\n",
      "Epoch 1857/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9506\n",
      "Epoch 01857: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0267 - acc: 0.9507\n",
      "Epoch 1858/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9475\n",
      "Epoch 01858: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0283 - acc: 0.9477\n",
      "Epoch 1859/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9500\n",
      "Epoch 01859: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0271 - acc: 0.9501\n",
      "Epoch 1860/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9532\n",
      "Epoch 01860: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0255 - acc: 0.9531\n",
      "Epoch 1861/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9481\n",
      "Epoch 01861: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0285 - acc: 0.9482\n",
      "Epoch 1862/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9481\n",
      "Epoch 01862: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0286 - acc: 0.9480\n",
      "Epoch 1863/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9507\n",
      "Epoch 01863: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0268 - acc: 0.9508\n",
      "Epoch 1864/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9507\n",
      "Epoch 01864: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0265 - acc: 0.9509\n",
      "Epoch 1865/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9528\n",
      "Epoch 01865: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0259 - acc: 0.9530\n",
      "Epoch 1866/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9511\n",
      "Epoch 01866: loss did not improve\n",
      "100/100 [==============================] - 46s 465ms/step - loss: 0.0266 - acc: 0.9510\n",
      "Epoch 1867/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9524\n",
      "Epoch 01867: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0261 - acc: 0.9522\n",
      "Epoch 1868/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9497\n",
      "Epoch 01868: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0280 - acc: 0.9498\n",
      "Epoch 1869/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9483\n",
      "Epoch 01869: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0281 - acc: 0.9480\n",
      "Epoch 1870/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9501\n",
      "Epoch 01870: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0274 - acc: 0.9502\n",
      "Epoch 1871/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9519\n",
      "Epoch 01871: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0263 - acc: 0.9520\n",
      "Epoch 1872/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9505\n",
      "Epoch 01872: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0273 - acc: 0.9500\n",
      "Epoch 1873/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9506\n",
      "Epoch 01873: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0270 - acc: 0.9506\n",
      "Epoch 1874/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9501\n",
      "Epoch 01874: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0275 - acc: 0.9500\n",
      "Epoch 1875/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9475\n",
      "Epoch 01875: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0285 - acc: 0.9476\n",
      "Epoch 1876/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9483\n",
      "Epoch 01876: loss did not improve\n",
      "100/100 [==============================] - 46s 455ms/step - loss: 0.0281 - acc: 0.9482\n",
      "Epoch 1877/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9504\n",
      "Epoch 01877: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0272 - acc: 0.9502\n",
      "Epoch 1878/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9440\n",
      "Epoch 01878: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0309 - acc: 0.9441\n",
      "Epoch 1879/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9500\n",
      "Epoch 01879: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0273 - acc: 0.9501\n",
      "Epoch 1880/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9503\n",
      "Epoch 01880: loss did not improve\n",
      "100/100 [==============================] - 46s 457ms/step - loss: 0.0270 - acc: 0.9503\n",
      "Epoch 1881/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9534\n",
      "Epoch 01881: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0254 - acc: 0.9533\n",
      "Epoch 1882/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9535\n",
      "Epoch 01882: loss did not improve\n",
      "100/100 [==============================] - 46s 460ms/step - loss: 0.0256 - acc: 0.9533\n",
      "Epoch 1883/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9503\n",
      "Epoch 01883: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0276 - acc: 0.9501\n",
      "Epoch 1884/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9503\n",
      "Epoch 01884: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0271 - acc: 0.9502\n",
      "Epoch 1885/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9484\n",
      "Epoch 01885: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0283 - acc: 0.9484\n",
      "Epoch 1886/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9507\n",
      "Epoch 01886: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0268 - acc: 0.9508\n",
      "Epoch 1887/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9489\n",
      "Epoch 01887: loss did not improve\n",
      "100/100 [==============================] - 47s 465ms/step - loss: 0.0277 - acc: 0.9490\n",
      "Epoch 1888/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9485\n",
      "Epoch 01888: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0277 - acc: 0.9486\n",
      "Epoch 1889/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9521\n",
      "Epoch 01889: loss did not improve\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.0261 - acc: 0.9522\n",
      "Epoch 1890/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9490\n",
      "Epoch 01890: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0277 - acc: 0.9489\n",
      "Epoch 1891/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9491\n",
      "Epoch 01891: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0275 - acc: 0.9491\n",
      "Epoch 1892/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9484\n",
      "Epoch 01892: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0283 - acc: 0.9483\n",
      "Epoch 1893/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9510\n",
      "Epoch 01893: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0268 - acc: 0.9509\n",
      "Epoch 1894/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9496\n",
      "Epoch 01894: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0272 - acc: 0.9494\n",
      "Epoch 1895/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9494\n",
      "Epoch 01895: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0275 - acc: 0.9496\n",
      "Epoch 1896/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9477\n",
      "Epoch 01896: loss did not improve\n",
      "100/100 [==============================] - 48s 477ms/step - loss: 0.0284 - acc: 0.9474\n",
      "Epoch 1897/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9505\n",
      "Epoch 01897: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0274 - acc: 0.9507\n",
      "Epoch 1898/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9474\n",
      "Epoch 01898: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0286 - acc: 0.9476\n",
      "Epoch 1899/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9514\n",
      "Epoch 01899: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0265 - acc: 0.9514\n",
      "Epoch 1900/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9505\n",
      "Epoch 01900: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0268 - acc: 0.9506\n",
      "Epoch 1901/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9484\n",
      "Epoch 01901: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0281 - acc: 0.9485\n",
      "Epoch 1902/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9510\n",
      "Epoch 01902: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0267 - acc: 0.9510\n",
      "Epoch 1903/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9517\n",
      "Epoch 01903: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0263 - acc: 0.9517\n",
      "Epoch 1904/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9508\n",
      "Epoch 01904: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0273 - acc: 0.9509\n",
      "Epoch 1905/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9483\n",
      "Epoch 01905: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0281 - acc: 0.9484\n",
      "Epoch 1906/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9504\n",
      "Epoch 01906: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0267 - acc: 0.9505\n",
      "Epoch 1907/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9527\n",
      "Epoch 01907: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0262 - acc: 0.9527\n",
      "Epoch 1908/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9484\n",
      "Epoch 01908: loss did not improve\n",
      "100/100 [==============================] - 47s 470ms/step - loss: 0.0280 - acc: 0.9486\n",
      "Epoch 1909/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9477\n",
      "Epoch 01909: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0285 - acc: 0.9478\n",
      "Epoch 1910/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9484\n",
      "Epoch 01910: loss did not improve\n",
      "100/100 [==============================] - 46s 464ms/step - loss: 0.0281 - acc: 0.9485\n",
      "Epoch 1911/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9521\n",
      "Epoch 01911: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0262 - acc: 0.9519\n",
      "Epoch 1912/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9498\n",
      "Epoch 01912: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0278 - acc: 0.9499\n",
      "Epoch 1913/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9511\n",
      "Epoch 01913: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0271 - acc: 0.9509\n",
      "Epoch 1914/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9497\n",
      "Epoch 01914: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0274 - acc: 0.9498\n",
      "Epoch 1915/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9503\n",
      "Epoch 01915: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0272 - acc: 0.9502\n",
      "Epoch 1916/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9486\n",
      "Epoch 01916: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0281 - acc: 0.9488\n",
      "Epoch 1917/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9504\n",
      "Epoch 01917: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0269 - acc: 0.9506\n",
      "Epoch 1918/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9519\n",
      "Epoch 01918: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0266 - acc: 0.9519\n",
      "Epoch 1919/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9496\n",
      "Epoch 01919: loss did not improve\n",
      "100/100 [==============================] - 47s 466ms/step - loss: 0.0273 - acc: 0.9495\n",
      "Epoch 1920/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9509\n",
      "Epoch 01920: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0266 - acc: 0.9509\n",
      "Epoch 1921/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9514\n",
      "Epoch 01921: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0267 - acc: 0.9514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1922/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9483\n",
      "Epoch 01922: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0282 - acc: 0.9481\n",
      "Epoch 1923/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9508\n",
      "Epoch 01923: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0267 - acc: 0.9509\n",
      "Epoch 1924/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9465\n",
      "Epoch 01924: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0293 - acc: 0.9465\n",
      "Epoch 1925/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9506\n",
      "Epoch 01925: loss did not improve\n",
      "100/100 [==============================] - 48s 475ms/step - loss: 0.0271 - acc: 0.9506\n",
      "Epoch 1926/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0255 - acc: 0.9533\n",
      "Epoch 01926: loss did not improve\n",
      "100/100 [==============================] - 48s 480ms/step - loss: 0.0255 - acc: 0.9533\n",
      "Epoch 1927/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9519\n",
      "Epoch 01927: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0261 - acc: 0.9518\n",
      "Epoch 1928/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9524\n",
      "Epoch 01928: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0260 - acc: 0.9523\n",
      "Epoch 1929/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9507\n",
      "Epoch 01929: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0268 - acc: 0.9507\n",
      "Epoch 1930/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9518\n",
      "Epoch 01930: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0266 - acc: 0.9518\n",
      "Epoch 1931/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9512\n",
      "Epoch 01931: loss did not improve\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.0267 - acc: 0.9512\n",
      "Epoch 1932/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9477\n",
      "Epoch 01932: loss did not improve\n",
      "100/100 [==============================] - 47s 469ms/step - loss: 0.0283 - acc: 0.9478\n",
      "Epoch 1933/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9480\n",
      "Epoch 01933: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0283 - acc: 0.9481\n",
      "Epoch 1934/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9508\n",
      "Epoch 01934: loss did not improve\n",
      "100/100 [==============================] - 47s 467ms/step - loss: 0.0268 - acc: 0.9507\n",
      "Epoch 1935/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9519\n",
      "Epoch 01935: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0264 - acc: 0.9520\n",
      "Epoch 1936/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9499\n",
      "Epoch 01936: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0276 - acc: 0.9497\n",
      "Epoch 1937/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9528\n",
      "Epoch 01937: loss did not improve\n",
      "100/100 [==============================] - 47s 473ms/step - loss: 0.0257 - acc: 0.9529\n",
      "Epoch 1938/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9528\n",
      "Epoch 01938: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0256 - acc: 0.9529\n",
      "Epoch 1939/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9501\n",
      "Epoch 01939: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0276 - acc: 0.9499\n",
      "Epoch 1940/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9487\n",
      "Epoch 01940: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0280 - acc: 0.9483\n",
      "Epoch 1941/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9515\n",
      "Epoch 01941: loss did not improve\n",
      "100/100 [==============================] - 46s 463ms/step - loss: 0.0267 - acc: 0.9515\n",
      "Epoch 1942/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9496\n",
      "Epoch 01942: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0274 - acc: 0.9496\n",
      "Epoch 1943/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0279 - acc: 0.9487\n",
      "Epoch 01943: loss did not improve\n",
      "100/100 [==============================] - 47s 475ms/step - loss: 0.0279 - acc: 0.9488\n",
      "Epoch 1944/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9505\n",
      "Epoch 01944: loss did not improve\n",
      "100/100 [==============================] - 47s 468ms/step - loss: 0.0267 - acc: 0.9507\n",
      "Epoch 1945/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9479\n",
      "Epoch 01945: loss did not improve\n",
      "100/100 [==============================] - 47s 471ms/step - loss: 0.0284 - acc: 0.9478\n",
      "Epoch 1946/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9486\n",
      "Epoch 01946: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0277 - acc: 0.9485\n",
      "Epoch 1947/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9507\n",
      "Epoch 01947: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0267 - acc: 0.9507\n",
      "Epoch 1948/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9516\n",
      "Epoch 01948: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0268 - acc: 0.9513\n",
      "Epoch 1949/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9531\n",
      "Epoch 01949: loss did not improve\n",
      "100/100 [==============================] - 47s 474ms/step - loss: 0.0257 - acc: 0.9531\n",
      "Epoch 1950/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9538\n",
      "Epoch 01950: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0252 - acc: 0.9538\n",
      "Epoch 1951/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9499\n",
      "Epoch 01951: loss did not improve\n",
      "100/100 [==============================] - 48s 476ms/step - loss: 0.0273 - acc: 0.9500\n",
      "Epoch 1952/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9492\n",
      "Epoch 01952: loss did not improve\n",
      "100/100 [==============================] - 47s 472ms/step - loss: 0.0276 - acc: 0.9492\n",
      "Epoch 1953/10000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9519\n",
      "Epoch 01953: loss did not improve\n",
      "100/100 [==============================] - 48s 478ms/step - loss: 0.0264 - acc: 0.9520\n",
      "Epoch 1954/10000\n",
      " 18/100 [====>.........................] - ETA: 38s - loss: 0.0275 - acc: 0.9502"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-327-541b1a776f78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheck2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logs1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistogram_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_grads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fitting model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheck2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1254\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/drc/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/drc/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/drc/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/drc/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/drc/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_checkpoint = ModelCheckpoint('net_segnet_aug_custom metric.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "check2 = TensorBoard(log_dir='logs1', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "print('Fitting model...')\n",
    "autoencoder.fit_generator(train, nb_epoch=10000,steps_per_epoch=100, verbose=1, shuffle=True, callbacks=[model_checkpoint,check2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv1 = Conv2D(4, 3, activation = 'relu', padding = 'same',data_format='channels_last')(inputs)\n",
    "conv1 = Conv2D(4, 3, activation = 'relu', padding = 'same',data_format='channels_last')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2),data_format='channels_last')(conv1)\n",
    "\n",
    "conv2 = Conv2D(16, 3, activation = 'relu', padding = 'same',data_format='channels_last')(pool1)\n",
    "conv2 = Conv2D(16, 3, activation = 'relu', padding = 'same',data_format='channels_last')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2),data_format='channels_last')(conv2)\n",
    "\n",
    "up8 = Conv2D(8, 2, activation = 'relu', padding = 'same',data_format='channels_last')(UpSampling2D(size = (2,2),data_format='channels_last')(pool2))\n",
    "conv8 = Conv2D(8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(up8)\n",
    "\n",
    "up9 = Conv2D(4, 2, activation = 'relu', padding = 'same', data_format='channels_last')(UpSampling2D(size = (2,2),data_format='channels_last')(conv8))\n",
    "conv9 = Conv2D(4, 3, activation = 'relu', padding = 'same',data_format='channels_last')(up9)\n",
    "conv10 = Conv2D(1, 1, activation = 'sigmoid',data_format='channels_last')(conv9)\n",
    "#crop = Cropping2D(cropping=((0, 0), (0, 0)), data_format='channels_last')\n",
    "\n",
    "\n",
    "model = Model(input = inputs, output = conv10)\n",
    "print model.summary()\n",
    "model.compile(optimizer = optimizers.Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-336-26da3f23aa60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocallyConnected2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'channels_last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocallyConnected2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'channels_last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'channels_last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocallyConnected2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'channels_last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/layers/local.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                                 self.data_format)\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mlocal_conv2d\u001b[0;34m(inputs, kernel, kernel_size, strides, output_shape, data_format)\u001b[0m\n\u001b[1;32m   4136\u001b[0m                                   (1, -1, feature_dim)))\n\u001b[1;32m   4137\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 xs.append(reshape(inputs[:, slice_row, slice_col, :],\n\u001b[0m\u001b[1;32m   4139\u001b[0m                                   (1, -1, feature_dim)))\n\u001b[1;32m   4140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/drc/.local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36m_SliceHelper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/drc/.local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/drc/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m   5415\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mellipsis_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5416\u001b[0m     \u001b[0mellipsis_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5417\u001b[0;31m   \u001b[0mellipsis_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ellipsis_mask\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5418\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnew_axis_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5419\u001b[0m     \u001b[0mnew_axis_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/drc/.local/lib/python2.7/site-packages/tensorflow/python/eager/execute.pyc\u001b[0m in \u001b[0;36mmake_int\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     raise TypeError(\"Expected int for argument '%s' not %s.\" %\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "conv1 = LocallyConnected2D(4, 3, activation = 'relu', padding = 'valid',data_format='channels_last')(inputs)\n",
    "conv1 = LocallyConnected2D(4, 3, activation = 'relu', padding = 'valid',data_format='channels_last')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2),data_format='channels_last')(conv1)\n",
    "\n",
    "conv2 = LocallyConnected2D(16, 3, activation = 'relu', padding = 'valid',data_format='channels_last')(pool1)\n",
    "conv2 = LocallyConnected2D(16, 3, activation = 'relu', padding = 'valid',data_format='channels_last')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2),data_format='channels_last')(conv2)\n",
    "\n",
    "up8 = LocallyConnected2D(8, 2, activation = 'relu', padding = 'valid',data_format='channels_last')(UpSampling2D(size = (2,2),data_format='channels_last')(pool2))\n",
    "conv8 = LocallyConnected2D(8, 3, activation = 'relu', padding = 'valid', kernel_initializer = 'he_normal',data_format='channels_last')(conv8)\n",
    "\n",
    "up9 = LocallyConnected2D(4, 2, activation = 'relu', padding = 'valid', data_format='channels_last')(UpSampling2D(size = (2,2),data_format='channels_last')(conv8))\n",
    "conv9 = LocallyConnected2D(4, 3, activation = 'relu', padding = 'valid',data_format='channels_last')(conv9)\n",
    "conv10 = LocallyConnected2D(1, 1, activation = 'sigmoid',data_format='channels_last')(conv9)\n",
    "#crop = Cropping2D(cropping=((0, 0), (0, 0)), data_format='channels_last')\n",
    "\n",
    "\n",
    "model = Model(input = inputs, output = conv10)\n",
    "print model.summary()\n",
    "model.compile(optimizer = optimizers.Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, 400, 400, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 400, 400, 4)  40          input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 400, 400, 4)  148         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling2D) (None, 200, 200, 4)  0           conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 200, 200, 8)  296         max_pooling2d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 200, 200, 8)  584         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling2D) (None, 100, 100, 8)  0           conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 100, 100, 32) 2336        max_pooling2d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 100, 100, 32) 9248        conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling2D) (None, 50, 50, 32)   0           conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 50, 50, 64)   18496       max_pooling2d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 50, 50, 64)   36928       conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_24 (UpSampling2D) (None, 100, 100, 64) 0           conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 100, 100, 32) 8224        up_sampling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "merge_24 (Merge)                (None, 100, 100, 64) 0           conv2d_173[0][0]                 \n",
      "                                                                 conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 100, 100, 32) 18464       merge_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 100, 100, 32) 9248        conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_25 (UpSampling2D) (None, 200, 200, 32) 0           conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 200, 200, 8)  1032        up_sampling2d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "merge_25 (Merge)                (None, 200, 200, 16) 0           conv2d_171[0][0]                 \n",
      "                                                                 conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 200, 200, 8)  1160        merge_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 200, 200, 8)  584         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_26 (UpSampling2D) (None, 400, 400, 8)  0           conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 400, 400, 4)  132         up_sampling2d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "merge_26 (Merge)                (None, 400, 400, 8)  0           conv2d_169[0][0]                 \n",
      "                                                                 conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 400, 400, 4)  292         merge_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 400, 400, 4)  148         conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 400, 400, 2)  74          conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 400, 400, 1)  3           conv2d_185[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 107,437\n",
      "Trainable params: 107,437\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:19: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:24: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:29: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:37: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"co..., inputs=Tensor(\"in...)`\n"
     ]
    }
   ],
   "source": [
    "conv1 = Conv2D(4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(inputs)\n",
    "conv1 = Conv2D(4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2),data_format='channels_last')(conv1)\n",
    "\n",
    "conv2 = Conv2D(8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(pool1)\n",
    "conv2 = Conv2D(8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2),data_format='channels_last')(conv2)\n",
    "\n",
    "conv3 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(pool2)\n",
    "conv3 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(conv3)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2),data_format='channels_last')(conv3)\n",
    "\n",
    "\n",
    "conv5 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(pool3)\n",
    "conv5 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(conv5)\n",
    "drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "up7 = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(UpSampling2D(size = (2,2),data_format='channels_last')(conv5))\n",
    "merge7 = merge([conv3,up7], mode = 'concat', concat_axis = 3)\n",
    "conv7 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(merge7)\n",
    "conv7 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(conv7)\n",
    "\n",
    "up8 = Conv2D(8, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(UpSampling2D(size = (2,2),data_format='channels_last')(conv7))\n",
    "merge8 = merge([conv2,up8], mode = 'concat', concat_axis = 3)\n",
    "conv8 = Conv2D(8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(merge8)\n",
    "conv8 = Conv2D(8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(conv8)\n",
    "\n",
    "up9 = Conv2D(4, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(UpSampling2D(size = (2,2),data_format='channels_last')(conv8))\n",
    "merge9 = merge([conv1,up9], mode = 'concat', concat_axis = 3)\n",
    "conv9 = Conv2D(4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(merge9)\n",
    "conv9 = Conv2D(4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(conv9)\n",
    "conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',data_format='channels_last')(conv9)\n",
    "conv10 = Conv2D(1, 1, activation = 'sigmoid',data_format='channels_last')(conv9)\n",
    "#crop = Cropping2D(cropping=((0, 0), (0, 0)), data_format='channels_last')\n",
    "\n",
    "\n",
    "model = Model(input = inputs, output = conv10)\n",
    "print model.summary()\n",
    "model.compile(optimizer = optimizers.Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(input_image_tensor):\n",
    "    def resnet_block_1(input_features, nb_features=16, nb_kernel_rows=3, nb_kernel_cols=3):\n",
    "\n",
    "        y = Conv2D(nb_features, nb_kernel_cols, padding='same',data_format='channels_last')(input_features)\n",
    "        y = Activation('relu')(y)\n",
    "        y = Conv2D(nb_features, nb_kernel_rows, padding='same',data_format='channels_last')(y)\n",
    "\n",
    "        y = add([input_features, y])\n",
    "        return Activation('relu')(y)\n",
    "\n",
    "    x = Conv2D(4, 3, padding='same', activation='relu',data_format='channels_last')(input_image_tensor)\n",
    "    x = Conv2D(16, 3, padding='same', activation='relu',data_format='channels_last')(input_image_tensor)\n",
    "\n",
    "    for _ in range(4):\n",
    "        x = resnet_block_1(x)\n",
    "        \n",
    "    x = Conv2D(1, 1, padding='same', activation='sigmoid',data_format='channels_last')(x)\n",
    "\n",
    "    \n",
    "    model = Model(input = inputs, output = x)\n",
    "    print model.summary()\n",
    "    model.compile(optimizer = optimizers.Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 480, 640, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 480, 640, 16) 160         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 480, 640, 16) 2320        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 480, 640, 16) 0           conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 480, 640, 16) 2320        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 480, 640, 16) 0           conv2d_72[0][0]                  \n",
      "                                                                 conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 480, 640, 16) 0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 480, 640, 16) 2320        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 480, 640, 16) 0           conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 480, 640, 16) 2320        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 480, 640, 16) 0           activation_47[0][0]              \n",
      "                                                                 conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 480, 640, 16) 0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 480, 640, 16) 2320        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 480, 640, 16) 0           conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 480, 640, 16) 2320        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 480, 640, 16) 0           activation_49[0][0]              \n",
      "                                                                 conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 480, 640, 16) 0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 480, 640, 16) 2320        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 480, 640, 16) 0           conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 480, 640, 16) 2320        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 480, 640, 16) 0           activation_51[0][0]              \n",
      "                                                                 conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 480, 640, 16) 0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 480, 640, 1)  17          activation_53[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 18,737\n",
      "Trainable params: 18,737\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"co..., inputs=Tensor(\"in...)`\n"
     ]
    }
   ],
   "source": [
    "model = network(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "class Image_Saver(Callback):\n",
    "    def on_train_begin(self,logs={}):\n",
    "        self.var_y_pred = tf.placeholder(tf.float32, [FLAGS.batch_size, 480,640,1])\n",
    "        #self.var_y_pred = tf.Variable(0., validate_shape=False)\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "            #callback = TensorBoard(log_path)\n",
    "            #callback.set_model(model)\n",
    "            \n",
    "            print type(K.eval(self.model.outputs[0]))\n",
    "            x=self.model.predict_on_batch(batch)\n",
    "            writer = tf.summary.FileWriter('./logs1')\n",
    "            writer.add_summary(tf.summary.image('input', x[0]))\n",
    "save = Image_Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., shuffle=True, verbose=1, steps_per_epoch=100, epochs=1000, callbacks=[<keras.ca...)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6483 - acc: 0.9118\n",
      "Epoch 00001: loss improved from inf to 0.64731, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 85s 845ms/step - loss: 0.6473 - acc: 0.9119\n",
      "Epoch 2/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3856 - acc: 0.9158\n",
      "Epoch 00002: loss improved from 0.64731 to 0.38418, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 84s 840ms/step - loss: 0.3842 - acc: 0.9161\n",
      "Epoch 3/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.9148\n",
      "Epoch 00003: loss improved from 0.38418 to 0.32339, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 83s 826ms/step - loss: 0.3234 - acc: 0.9149\n",
      "Epoch 4/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.9166\n",
      "Epoch 00004: loss improved from 0.32339 to 0.30753, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 81s 812ms/step - loss: 0.3075 - acc: 0.9168\n",
      "Epoch 5/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3001 - acc: 0.9148\n",
      "Epoch 00005: loss improved from 0.30753 to 0.29947, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 85s 845ms/step - loss: 0.2995 - acc: 0.9151\n",
      "Epoch 6/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2914 - acc: 0.9154\n",
      "Epoch 00006: loss improved from 0.29947 to 0.29147, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 82s 818ms/step - loss: 0.2915 - acc: 0.9153\n",
      "Epoch 7/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9183\n",
      "Epoch 00007: loss improved from 0.29147 to 0.28169, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 81s 810ms/step - loss: 0.2817 - acc: 0.9182\n",
      "Epoch 8/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9144\n",
      "Epoch 00008: loss did not improve\n",
      "100/100 [==============================] - 83s 827ms/step - loss: 0.2885 - acc: 0.9144\n",
      "Epoch 9/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.9170\n",
      "Epoch 00009: loss improved from 0.28169 to 0.28059, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 83s 829ms/step - loss: 0.2806 - acc: 0.9171\n",
      "Epoch 10/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9152\n",
      "Epoch 00010: loss did not improve\n",
      "100/100 [==============================] - 84s 838ms/step - loss: 0.2831 - acc: 0.9152\n",
      "Epoch 11/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2819 - acc: 0.9151\n",
      "Epoch 00011: loss did not improve\n",
      "100/100 [==============================] - 83s 829ms/step - loss: 0.2820 - acc: 0.9151\n",
      "Epoch 12/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2748 - acc: 0.9168\n",
      "Epoch 00012: loss improved from 0.28059 to 0.27449, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 83s 835ms/step - loss: 0.2745 - acc: 0.9170\n",
      "Epoch 13/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2760 - acc: 0.9159\n",
      "Epoch 00013: loss did not improve\n",
      "100/100 [==============================] - 83s 827ms/step - loss: 0.2758 - acc: 0.9159\n",
      "Epoch 14/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9152\n",
      "Epoch 00014: loss improved from 0.27449 to 0.27235, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 85s 849ms/step - loss: 0.2724 - acc: 0.9154\n",
      "Epoch 15/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9159\n",
      "Epoch 00015: loss improved from 0.27235 to 0.26893, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 84s 842ms/step - loss: 0.2689 - acc: 0.9162\n",
      "Epoch 16/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2636 - acc: 0.9171\n",
      "Epoch 00016: loss improved from 0.26893 to 0.26405, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 84s 839ms/step - loss: 0.2640 - acc: 0.9169\n",
      "Epoch 17/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9165\n",
      "Epoch 00017: loss improved from 0.26405 to 0.26080, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 84s 845ms/step - loss: 0.2608 - acc: 0.9164\n",
      "Epoch 18/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2639 - acc: 0.9132\n",
      "Epoch 00018: loss did not improve\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.2642 - acc: 0.9131\n",
      "Epoch 19/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2563 - acc: 0.9158\n",
      "Epoch 00019: loss improved from 0.26080 to 0.25636, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 85s 852ms/step - loss: 0.2564 - acc: 0.9158\n",
      "Epoch 20/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9185\n",
      "Epoch 00020: loss improved from 0.25636 to 0.24637, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2464 - acc: 0.9184\n",
      "Epoch 21/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2560 - acc: 0.9145\n",
      "Epoch 00021: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2558 - acc: 0.9146\n",
      "Epoch 22/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9163\n",
      "Epoch 00022: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2489 - acc: 0.9164\n",
      "Epoch 23/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.9146\n",
      "Epoch 00023: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2523 - acc: 0.9145\n",
      "Epoch 24/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2513 - acc: 0.9156\n",
      "Epoch 00024: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2524 - acc: 0.9151\n",
      "Epoch 25/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2603 - acc: 0.9113\n",
      "Epoch 00025: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2601 - acc: 0.9112\n",
      "Epoch 26/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2516 - acc: 0.9143\n",
      "Epoch 00026: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2516 - acc: 0.9143\n",
      "Epoch 27/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2475 - acc: 0.9160\n",
      "Epoch 00027: loss did not improve\n",
      "100/100 [==============================] - 90s 901ms/step - loss: 0.2472 - acc: 0.9162\n",
      "Epoch 28/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2449 - acc: 0.9167\n",
      "Epoch 00028: loss improved from 0.24637 to 0.24476, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 89s 893ms/step - loss: 0.2448 - acc: 0.9168\n",
      "Epoch 29/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2436 - acc: 0.9174\n",
      "Epoch 00029: loss improved from 0.24476 to 0.24457, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2446 - acc: 0.9171\n",
      "Epoch 30/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2496 - acc: 0.9147\n",
      "Epoch 00030: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2497 - acc: 0.9147\n",
      "Epoch 31/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2481 - acc: 0.9150\n",
      "Epoch 00031: loss did not improve\n",
      "100/100 [==============================] - 88s 885ms/step - loss: 0.2481 - acc: 0.9151\n",
      "Epoch 32/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9162\n",
      "Epoch 00032: loss improved from 0.24457 to 0.24371, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2437 - acc: 0.9162\n",
      "Epoch 33/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9155\n",
      "Epoch 00033: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2444 - acc: 0.9156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2522 - acc: 0.9144\n",
      "Epoch 00034: loss did not improve\n",
      "100/100 [==============================] - 89s 889ms/step - loss: 0.2517 - acc: 0.9146\n",
      "Epoch 35/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9155\n",
      "Epoch 00035: loss did not improve\n",
      "100/100 [==============================] - 89s 886ms/step - loss: 0.2458 - acc: 0.9154\n",
      "Epoch 36/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.9131\n",
      "Epoch 00036: loss did not improve\n",
      "100/100 [==============================] - 88s 884ms/step - loss: 0.2513 - acc: 0.9132\n",
      "Epoch 37/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2430 - acc: 0.9157\n",
      "Epoch 00037: loss improved from 0.24371 to 0.24255, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2426 - acc: 0.9159\n",
      "Epoch 38/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2419 - acc: 0.9169\n",
      "Epoch 00038: loss improved from 0.24255 to 0.24175, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 89s 888ms/step - loss: 0.2418 - acc: 0.9169\n",
      "Epoch 39/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9139\n",
      "Epoch 00039: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2527 - acc: 0.9138\n",
      "Epoch 40/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2527 - acc: 0.9127\n",
      "Epoch 00040: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2526 - acc: 0.9127\n",
      "Epoch 41/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9153\n",
      "Epoch 00041: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2428 - acc: 0.9153\n",
      "Epoch 42/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.9164\n",
      "Epoch 00042: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2419 - acc: 0.9165\n",
      "Epoch 43/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2442 - acc: 0.9156\n",
      "Epoch 00043: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2445 - acc: 0.9154\n",
      "Epoch 44/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9166\n",
      "Epoch 00044: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2434 - acc: 0.9164\n",
      "Epoch 45/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2427 - acc: 0.9161\n",
      "Epoch 00045: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2424 - acc: 0.9162\n",
      "Epoch 46/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2442 - acc: 0.9157\n",
      "Epoch 00046: loss did not improve\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.2439 - acc: 0.9157\n",
      "Epoch 47/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2455 - acc: 0.9147\n",
      "Epoch 00047: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2455 - acc: 0.9146\n",
      "Epoch 48/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9171\n",
      "Epoch 00048: loss improved from 0.24175 to 0.23909, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2391 - acc: 0.9170\n",
      "Epoch 49/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.9155\n",
      "Epoch 00049: loss did not improve\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.2421 - acc: 0.9155\n",
      "Epoch 50/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.9162\n",
      "Epoch 00050: loss did not improve\n",
      "100/100 [==============================] - 85s 852ms/step - loss: 0.2442 - acc: 0.9163\n",
      "Epoch 51/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9174\n",
      "Epoch 00051: loss improved from 0.23909 to 0.23715, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.2371 - acc: 0.9176\n",
      "Epoch 52/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2441 - acc: 0.9146\n",
      "Epoch 00052: loss did not improve\n",
      "100/100 [==============================] - 85s 847ms/step - loss: 0.2440 - acc: 0.9146\n",
      "Epoch 53/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2424 - acc: 0.9169\n",
      "Epoch 00053: loss did not improve\n",
      "100/100 [==============================] - 85s 852ms/step - loss: 0.2424 - acc: 0.9169\n",
      "Epoch 54/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9144\n",
      "Epoch 00054: loss did not improve\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.2478 - acc: 0.9144\n",
      "Epoch 55/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.9158\n",
      "Epoch 00055: loss did not improve\n",
      "100/100 [==============================] - 85s 850ms/step - loss: 0.2443 - acc: 0.9159\n",
      "Epoch 56/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9187\n",
      "Epoch 00056: loss improved from 0.23715 to 0.23700, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2370 - acc: 0.9188\n",
      "Epoch 57/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9160\n",
      "Epoch 00057: loss did not improve\n",
      "100/100 [==============================] - 85s 854ms/step - loss: 0.2412 - acc: 0.9160\n",
      "Epoch 58/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2378 - acc: 0.9180\n",
      "Epoch 00058: loss did not improve\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.2380 - acc: 0.9180\n",
      "Epoch 59/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2379 - acc: 0.9168\n",
      "Epoch 00059: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2375 - acc: 0.9169\n",
      "Epoch 60/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2406 - acc: 0.9167\n",
      "Epoch 00060: loss did not improve\n",
      "100/100 [==============================] - 86s 855ms/step - loss: 0.2411 - acc: 0.9166\n",
      "Epoch 61/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2485 - acc: 0.9143\n",
      "Epoch 00061: loss did not improve\n",
      "100/100 [==============================] - 85s 847ms/step - loss: 0.2487 - acc: 0.9141\n",
      "Epoch 62/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9167\n",
      "Epoch 00062: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2406 - acc: 0.9168\n",
      "Epoch 63/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9152\n",
      "Epoch 00063: loss did not improve\n",
      "100/100 [==============================] - 86s 855ms/step - loss: 0.2441 - acc: 0.9151\n",
      "Epoch 64/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9178\n",
      "Epoch 00064: loss improved from 0.23700 to 0.23574, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2357 - acc: 0.9179\n",
      "Epoch 65/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9151\n",
      "Epoch 00065: loss did not improve\n",
      "100/100 [==============================] - 84s 842ms/step - loss: 0.2464 - acc: 0.9151\n",
      "Epoch 66/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2534 - acc: 0.9131\n",
      "Epoch 00066: loss did not improve\n",
      "100/100 [==============================] - 85s 846ms/step - loss: 0.2527 - acc: 0.9134\n",
      "Epoch 67/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9170\n",
      "Epoch 00067: loss did not improve\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.2392 - acc: 0.9171\n",
      "Epoch 68/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9165\n",
      "Epoch 00068: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2402 - acc: 0.9165\n",
      "Epoch 69/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2349 - acc: 0.9199\n",
      "Epoch 00069: loss improved from 0.23574 to 0.23486, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2349 - acc: 0.9199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2491 - acc: 0.9141\n",
      "Epoch 00070: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2493 - acc: 0.9140\n",
      "Epoch 71/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2470 - acc: 0.9144\n",
      "Epoch 00071: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2469 - acc: 0.9144\n",
      "Epoch 72/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9176\n",
      "Epoch 00072: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2407 - acc: 0.9175\n",
      "Epoch 73/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9161\n",
      "Epoch 00073: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2441 - acc: 0.9160\n",
      "Epoch 74/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9181\n",
      "Epoch 00074: loss did not improve\n",
      "100/100 [==============================] - 86s 855ms/step - loss: 0.2362 - acc: 0.9180\n",
      "Epoch 75/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9148\n",
      "Epoch 00075: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2436 - acc: 0.9151\n",
      "Epoch 76/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2476 - acc: 0.9151\n",
      "Epoch 00076: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2476 - acc: 0.9150\n",
      "Epoch 77/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9159\n",
      "Epoch 00077: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2403 - acc: 0.9160\n",
      "Epoch 78/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9190\n",
      "Epoch 00078: loss improved from 0.23486 to 0.23357, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2336 - acc: 0.9190\n",
      "Epoch 79/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9166\n",
      "Epoch 00079: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2397 - acc: 0.9167\n",
      "Epoch 80/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2431 - acc: 0.9163\n",
      "Epoch 00080: loss did not improve\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.2434 - acc: 0.9162\n",
      "Epoch 81/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.9154\n",
      "Epoch 00081: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2453 - acc: 0.9153\n",
      "Epoch 82/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2435 - acc: 0.9165\n",
      "Epoch 00082: loss did not improve\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.2431 - acc: 0.9166\n",
      "Epoch 83/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.9176\n",
      "Epoch 00083: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2381 - acc: 0.9177\n",
      "Epoch 84/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.9145\n",
      "Epoch 00084: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2470 - acc: 0.9146\n",
      "Epoch 85/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9158\n",
      "Epoch 00085: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2438 - acc: 0.9161\n",
      "Epoch 86/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9164\n",
      "Epoch 00086: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2408 - acc: 0.9164\n",
      "Epoch 87/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2372 - acc: 0.9186\n",
      "Epoch 00087: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2373 - acc: 0.9185\n",
      "Epoch 88/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9168\n",
      "Epoch 00088: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2390 - acc: 0.9170\n",
      "Epoch 89/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9195\n",
      "Epoch 00089: loss improved from 0.23357 to 0.23263, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.2326 - acc: 0.9197\n",
      "Epoch 90/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.9167\n",
      "Epoch 00090: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2407 - acc: 0.9167\n",
      "Epoch 91/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2500 - acc: 0.9138\n",
      "Epoch 00091: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2496 - acc: 0.9140\n",
      "Epoch 92/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9180\n",
      "Epoch 00092: loss did not improve\n",
      "100/100 [==============================] - 85s 850ms/step - loss: 0.2366 - acc: 0.9182\n",
      "Epoch 93/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9155\n",
      "Epoch 00093: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2426 - acc: 0.9154\n",
      "Epoch 94/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2460 - acc: 0.9152\n",
      "Epoch 00094: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2469 - acc: 0.9149\n",
      "Epoch 95/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9161\n",
      "Epoch 00095: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2431 - acc: 0.9161\n",
      "Epoch 96/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9170\n",
      "Epoch 00096: loss did not improve\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.2402 - acc: 0.9170\n",
      "Epoch 97/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9180\n",
      "Epoch 00097: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2396 - acc: 0.9180\n",
      "Epoch 98/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9184\n",
      "Epoch 00098: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2380 - acc: 0.9184\n",
      "Epoch 99/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9186\n",
      "Epoch 00099: loss did not improve\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.2341 - acc: 0.9186\n",
      "Epoch 100/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9180\n",
      "Epoch 00100: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2356 - acc: 0.9181\n",
      "Epoch 101/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9197\n",
      "Epoch 00101: loss improved from 0.23263 to 0.23255, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2326 - acc: 0.9196\n",
      "Epoch 102/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2356 - acc: 0.9191\n",
      "Epoch 00102: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2356 - acc: 0.9191\n",
      "Epoch 103/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9186\n",
      "Epoch 00103: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2367 - acc: 0.9187\n",
      "Epoch 104/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9197\n",
      "Epoch 00104: loss improved from 0.23255 to 0.23253, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 89s 888ms/step - loss: 0.2325 - acc: 0.9196\n",
      "Epoch 105/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9178\n",
      "Epoch 00105: loss did not improve\n",
      "100/100 [==============================] - 86s 865ms/step - loss: 0.2355 - acc: 0.9180\n",
      "Epoch 106/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9167\n",
      "Epoch 00106: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2417 - acc: 0.9169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9196\n",
      "Epoch 00107: loss improved from 0.23253 to 0.23209, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2321 - acc: 0.9194\n",
      "Epoch 108/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2404 - acc: 0.9165\n",
      "Epoch 00108: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2401 - acc: 0.9166\n",
      "Epoch 109/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9182\n",
      "Epoch 00109: loss did not improve\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.2391 - acc: 0.9182\n",
      "Epoch 110/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9177\n",
      "Epoch 00110: loss did not improve\n",
      "100/100 [==============================] - 85s 855ms/step - loss: 0.2369 - acc: 0.9176\n",
      "Epoch 111/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2389 - acc: 0.9166\n",
      "Epoch 00111: loss did not improve\n",
      "100/100 [==============================] - 88s 884ms/step - loss: 0.2389 - acc: 0.9166\n",
      "Epoch 112/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9198\n",
      "Epoch 00112: loss did not improve\n",
      "100/100 [==============================] - 86s 855ms/step - loss: 0.2322 - acc: 0.9198\n",
      "Epoch 113/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9197\n",
      "Epoch 00113: loss did not improve\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.2336 - acc: 0.9197\n",
      "Epoch 114/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2398 - acc: 0.9174\n",
      "Epoch 00114: loss did not improve\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.2395 - acc: 0.9175\n",
      "Epoch 115/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9149\n",
      "Epoch 00115: loss did not improve\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.2474 - acc: 0.9149\n",
      "Epoch 116/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9188\n",
      "Epoch 00116: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2341 - acc: 0.9189\n",
      "Epoch 117/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9183\n",
      "Epoch 00117: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2352 - acc: 0.9182\n",
      "Epoch 118/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9189\n",
      "Epoch 00118: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2346 - acc: 0.9189\n",
      "Epoch 119/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9209\n",
      "Epoch 00119: loss improved from 0.23209 to 0.23111, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2311 - acc: 0.9208\n",
      "Epoch 120/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9191\n",
      "Epoch 00120: loss did not improve\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.2350 - acc: 0.9191\n",
      "Epoch 121/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.9149\n",
      "Epoch 00121: loss did not improve\n",
      "100/100 [==============================] - 86s 855ms/step - loss: 0.2438 - acc: 0.9151\n",
      "Epoch 122/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2388 - acc: 0.9176\n",
      "Epoch 00122: loss did not improve\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.2388 - acc: 0.9177\n",
      "Epoch 123/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9153\n",
      "Epoch 00123: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2460 - acc: 0.9154\n",
      "Epoch 124/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9172\n",
      "Epoch 00124: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2409 - acc: 0.9169\n",
      "Epoch 125/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9196\n",
      "Epoch 00125: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2333 - acc: 0.9198\n",
      "Epoch 126/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9177\n",
      "Epoch 00126: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2375 - acc: 0.9177\n",
      "Epoch 127/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9189\n",
      "Epoch 00127: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2369 - acc: 0.9189\n",
      "Epoch 128/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9185\n",
      "Epoch 00128: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2378 - acc: 0.9186\n",
      "Epoch 129/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2425 - acc: 0.9169\n",
      "Epoch 00129: loss did not improve\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.2424 - acc: 0.9170\n",
      "Epoch 130/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9169\n",
      "Epoch 00130: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2417 - acc: 0.9172\n",
      "Epoch 131/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9193\n",
      "Epoch 00131: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2355 - acc: 0.9192\n",
      "Epoch 132/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9183\n",
      "Epoch 00132: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2380 - acc: 0.9181\n",
      "Epoch 133/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9170\n",
      "Epoch 00133: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2397 - acc: 0.9171\n",
      "Epoch 134/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9191\n",
      "Epoch 00134: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2376 - acc: 0.9191\n",
      "Epoch 135/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9192\n",
      "Epoch 00135: loss did not improve\n",
      "100/100 [==============================] - 85s 854ms/step - loss: 0.2354 - acc: 0.9191\n",
      "Epoch 136/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2414 - acc: 0.9172\n",
      "Epoch 00136: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2412 - acc: 0.9173\n",
      "Epoch 137/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9196\n",
      "Epoch 00137: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2339 - acc: 0.9197\n",
      "Epoch 138/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9195\n",
      "Epoch 00138: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2352 - acc: 0.9190\n",
      "Epoch 139/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9187\n",
      "Epoch 00139: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2366 - acc: 0.9187\n",
      "Epoch 140/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2337 - acc: 0.9190\n",
      "Epoch 00140: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2337 - acc: 0.9190\n",
      "Epoch 141/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9191\n",
      "Epoch 00141: loss did not improve\n",
      "100/100 [==============================] - 85s 852ms/step - loss: 0.2333 - acc: 0.9191\n",
      "Epoch 142/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9191\n",
      "Epoch 00142: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2331 - acc: 0.9191\n",
      "Epoch 143/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9172\n",
      "Epoch 00143: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2391 - acc: 0.9172\n",
      "Epoch 144/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9198\n",
      "Epoch 00144: loss improved from 0.23111 to 0.23071, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2307 - acc: 0.9197\n",
      "Epoch 145/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2401 - acc: 0.9174\n",
      "Epoch 00145: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2396 - acc: 0.9176\n",
      "Epoch 146/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9188\n",
      "Epoch 00146: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2366 - acc: 0.9185\n",
      "Epoch 147/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9184\n",
      "Epoch 00147: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2344 - acc: 0.9184\n",
      "Epoch 148/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2447 - acc: 0.9148\n",
      "Epoch 00148: loss did not improve\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.2448 - acc: 0.9147\n",
      "Epoch 149/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9185\n",
      "Epoch 00149: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2364 - acc: 0.9184\n",
      "Epoch 150/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9189\n",
      "Epoch 00150: loss did not improve\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.2321 - acc: 0.9189\n",
      "Epoch 151/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2388 - acc: 0.9182\n",
      "Epoch 00151: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2387 - acc: 0.9182\n",
      "Epoch 152/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9185\n",
      "Epoch 00152: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2372 - acc: 0.9185\n",
      "Epoch 153/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9184\n",
      "Epoch 00153: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2368 - acc: 0.9184\n",
      "Epoch 154/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9194\n",
      "Epoch 00154: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2334 - acc: 0.9195\n",
      "Epoch 155/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9195\n",
      "Epoch 00155: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2324 - acc: 0.9195\n",
      "Epoch 156/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9196\n",
      "Epoch 00156: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2335 - acc: 0.9197\n",
      "Epoch 157/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9204\n",
      "Epoch 00157: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2319 - acc: 0.9205\n",
      "Epoch 158/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9208\n",
      "Epoch 00158: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2331 - acc: 0.9207\n",
      "Epoch 159/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9194\n",
      "Epoch 00159: loss did not improve\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.2330 - acc: 0.9195\n",
      "Epoch 160/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9196\n",
      "Epoch 00160: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2342 - acc: 0.9195\n",
      "Epoch 161/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9168\n",
      "Epoch 00161: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2417 - acc: 0.9167\n",
      "Epoch 162/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9198\n",
      "Epoch 00162: loss improved from 0.23071 to 0.22919, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2292 - acc: 0.9199\n",
      "Epoch 163/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9204\n",
      "Epoch 00163: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2320 - acc: 0.9205\n",
      "Epoch 164/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9182\n",
      "Epoch 00164: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2348 - acc: 0.9184\n",
      "Epoch 165/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9198\n",
      "Epoch 00165: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2340 - acc: 0.9199\n",
      "Epoch 166/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9199\n",
      "Epoch 00166: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2330 - acc: 0.9198\n",
      "Epoch 167/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9182\n",
      "Epoch 00167: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2343 - acc: 0.9184\n",
      "Epoch 168/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2374 - acc: 0.9185\n",
      "Epoch 00168: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2373 - acc: 0.9185\n",
      "Epoch 169/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2395 - acc: 0.9175\n",
      "Epoch 00169: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2393 - acc: 0.9175\n",
      "Epoch 170/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2381 - acc: 0.9178\n",
      "Epoch 00170: loss did not improve\n",
      "100/100 [==============================] - 85s 846ms/step - loss: 0.2377 - acc: 0.9180\n",
      "Epoch 171/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9196\n",
      "Epoch 00171: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2311 - acc: 0.9198\n",
      "Epoch 172/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9198\n",
      "Epoch 00172: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2331 - acc: 0.9197\n",
      "Epoch 173/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9200\n",
      "Epoch 00173: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2332 - acc: 0.9199\n",
      "Epoch 174/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9192\n",
      "Epoch 00174: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2321 - acc: 0.9193\n",
      "Epoch 175/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2445 - acc: 0.9158\n",
      "Epoch 00175: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2446 - acc: 0.9157\n",
      "Epoch 176/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9191\n",
      "Epoch 00176: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2345 - acc: 0.9190\n",
      "Epoch 177/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.9186\n",
      "Epoch 00177: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2375 - acc: 0.9184\n",
      "Epoch 178/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2323 - acc: 0.9204\n",
      "Epoch 00178: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2329 - acc: 0.9203\n",
      "Epoch 179/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9188\n",
      "Epoch 00179: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2360 - acc: 0.9189\n",
      "Epoch 180/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9196\n",
      "Epoch 00180: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2335 - acc: 0.9193\n",
      "Epoch 181/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2309 - acc: 0.9203\n",
      "Epoch 00181: loss did not improve\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.2311 - acc: 0.9202\n",
      "Epoch 182/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9192\n",
      "Epoch 00182: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2346 - acc: 0.9191\n",
      "Epoch 183/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9190\n",
      "Epoch 00183: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2345 - acc: 0.9191\n",
      "Epoch 184/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9206\n",
      "Epoch 00184: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2316 - acc: 0.9206\n",
      "Epoch 185/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9189\n",
      "Epoch 00185: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2337 - acc: 0.9188\n",
      "Epoch 186/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9180\n",
      "Epoch 00186: loss did not improve\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.2369 - acc: 0.9180\n",
      "Epoch 187/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9204\n",
      "Epoch 00187: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2323 - acc: 0.9202\n",
      "Epoch 188/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9199\n",
      "Epoch 00188: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2321 - acc: 0.9199\n",
      "Epoch 189/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9202\n",
      "Epoch 00189: loss did not improve\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.2320 - acc: 0.9201\n",
      "Epoch 190/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2322 - acc: 0.9196\n",
      "Epoch 00190: loss did not improve\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.2317 - acc: 0.9198\n",
      "Epoch 191/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2345 - acc: 0.9188\n",
      "Epoch 00191: loss did not improve\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.2345 - acc: 0.9188\n",
      "Epoch 192/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9177\n",
      "Epoch 00192: loss did not improve\n",
      "100/100 [==============================] - 85s 850ms/step - loss: 0.2365 - acc: 0.9179\n",
      "Epoch 193/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9220\n",
      "Epoch 00193: loss improved from 0.22919 to 0.22706, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2271 - acc: 0.9219\n",
      "Epoch 194/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9216\n",
      "Epoch 00194: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2292 - acc: 0.9217\n",
      "Epoch 195/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9182\n",
      "Epoch 00195: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2365 - acc: 0.9181\n",
      "Epoch 196/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2442 - acc: 0.9162\n",
      "Epoch 00196: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2437 - acc: 0.9164\n",
      "Epoch 197/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9231\n",
      "Epoch 00197: loss improved from 0.22706 to 0.22310, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2231 - acc: 0.9229\n",
      "Epoch 198/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9194\n",
      "Epoch 00198: loss did not improve\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.2325 - acc: 0.9194\n",
      "Epoch 199/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9217\n",
      "Epoch 00199: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2270 - acc: 0.9216\n",
      "Epoch 200/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9162\n",
      "Epoch 00200: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2442 - acc: 0.9161\n",
      "Epoch 201/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9186\n",
      "Epoch 00201: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2369 - acc: 0.9185\n",
      "Epoch 202/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9172\n",
      "Epoch 00202: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2405 - acc: 0.9172\n",
      "Epoch 203/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9186\n",
      "Epoch 00203: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2347 - acc: 0.9186\n",
      "Epoch 204/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9211\n",
      "Epoch 00204: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2294 - acc: 0.9212\n",
      "Epoch 205/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9213\n",
      "Epoch 00205: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2299 - acc: 0.9214\n",
      "Epoch 206/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9197\n",
      "Epoch 00206: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2324 - acc: 0.9200\n",
      "Epoch 207/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9170\n",
      "Epoch 00207: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2397 - acc: 0.9171\n",
      "Epoch 208/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2394 - acc: 0.9181\n",
      "Epoch 00208: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2400 - acc: 0.9179\n",
      "Epoch 209/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9215\n",
      "Epoch 00209: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2294 - acc: 0.9211\n",
      "Epoch 210/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2413 - acc: 0.9164\n",
      "Epoch 00210: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2405 - acc: 0.9167\n",
      "Epoch 211/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9218\n",
      "Epoch 00211: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2276 - acc: 0.9216\n",
      "Epoch 212/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9197\n",
      "Epoch 00212: loss did not improve\n",
      "100/100 [==============================] - 86s 855ms/step - loss: 0.2327 - acc: 0.9197\n",
      "Epoch 213/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9193\n",
      "Epoch 00213: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2341 - acc: 0.9195\n",
      "Epoch 214/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9199\n",
      "Epoch 00214: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2324 - acc: 0.9199\n",
      "Epoch 215/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9206\n",
      "Epoch 00215: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2333 - acc: 0.9204\n",
      "Epoch 216/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9210\n",
      "Epoch 00216: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2287 - acc: 0.9210\n",
      "Epoch 217/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9207\n",
      "Epoch 00217: loss did not improve\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.2308 - acc: 0.9207\n",
      "Epoch 218/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2383 - acc: 0.9176\n",
      "Epoch 00218: loss did not improve\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.2383 - acc: 0.9176\n",
      "Epoch 219/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9205\n",
      "Epoch 00219: loss did not improve\n",
      "100/100 [==============================] - 86s 865ms/step - loss: 0.2335 - acc: 0.9204\n",
      "Epoch 220/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9190\n",
      "Epoch 00220: loss did not improve\n",
      "100/100 [==============================] - 85s 851ms/step - loss: 0.2346 - acc: 0.9189\n",
      "Epoch 221/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9197\n",
      "Epoch 00221: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2339 - acc: 0.9197\n",
      "Epoch 222/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9207\n",
      "Epoch 00222: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2298 - acc: 0.9208\n",
      "Epoch 223/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2362 - acc: 0.9190\n",
      "Epoch 00223: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2362 - acc: 0.9190\n",
      "Epoch 224/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9190\n",
      "Epoch 00224: loss did not improve\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.2344 - acc: 0.9191\n",
      "Epoch 225/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2387 - acc: 0.9178\n",
      "Epoch 00225: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2379 - acc: 0.9181\n",
      "Epoch 226/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.9187\n",
      "Epoch 00226: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2332 - acc: 0.9190\n",
      "Epoch 227/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9196\n",
      "Epoch 00227: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2317 - acc: 0.9194\n",
      "Epoch 228/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2432 - acc: 0.9163\n",
      "Epoch 00228: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2432 - acc: 0.9162\n",
      "Epoch 229/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9195\n",
      "Epoch 00229: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2329 - acc: 0.9195\n",
      "Epoch 230/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9193\n",
      "Epoch 00230: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2336 - acc: 0.9192\n",
      "Epoch 231/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9215\n",
      "Epoch 00231: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2260 - acc: 0.9215\n",
      "Epoch 232/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2356 - acc: 0.9178\n",
      "Epoch 00232: loss did not improve\n",
      "100/100 [==============================] - 89s 886ms/step - loss: 0.2353 - acc: 0.9179\n",
      "Epoch 233/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9194\n",
      "Epoch 00233: loss did not improve\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.2333 - acc: 0.9193\n",
      "Epoch 234/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2279 - acc: 0.9207\n",
      "Epoch 00234: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2288 - acc: 0.9204\n",
      "Epoch 235/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9177\n",
      "Epoch 00235: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2356 - acc: 0.9178\n",
      "Epoch 236/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9207\n",
      "Epoch 00236: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2294 - acc: 0.9206\n",
      "Epoch 237/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9190\n",
      "Epoch 00237: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2334 - acc: 0.9191\n",
      "Epoch 238/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9198\n",
      "Epoch 00238: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2333 - acc: 0.9198\n",
      "Epoch 239/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9203\n",
      "Epoch 00239: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2298 - acc: 0.9205\n",
      "Epoch 240/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9200\n",
      "Epoch 00240: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2339 - acc: 0.9199\n",
      "Epoch 241/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9219\n",
      "Epoch 00241: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2266 - acc: 0.9219\n",
      "Epoch 242/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9196\n",
      "Epoch 00242: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2320 - acc: 0.9195\n",
      "Epoch 243/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9189\n",
      "Epoch 00243: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2353 - acc: 0.9188\n",
      "Epoch 244/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9209\n",
      "Epoch 00244: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2289 - acc: 0.9210\n",
      "Epoch 245/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2374 - acc: 0.9176\n",
      "Epoch 00245: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2374 - acc: 0.9175\n",
      "Epoch 246/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9184\n",
      "Epoch 00246: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2364 - acc: 0.9184\n",
      "Epoch 247/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9198\n",
      "Epoch 00247: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2327 - acc: 0.9198\n",
      "Epoch 248/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9212\n",
      "Epoch 00248: loss did not improve\n",
      "100/100 [==============================] - 85s 850ms/step - loss: 0.2283 - acc: 0.9213\n",
      "Epoch 249/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9195\n",
      "Epoch 00249: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2329 - acc: 0.9196\n",
      "Epoch 250/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2382 - acc: 0.9181\n",
      "Epoch 00250: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2381 - acc: 0.9181\n",
      "Epoch 251/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9209\n",
      "Epoch 00251: loss did not improve\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.2292 - acc: 0.9210\n",
      "Epoch 252/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9189\n",
      "Epoch 00252: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2316 - acc: 0.9189\n",
      "Epoch 253/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2366 - acc: 0.9184\n",
      "Epoch 00253: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2366 - acc: 0.9185\n",
      "Epoch 254/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9197\n",
      "Epoch 00254: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2342 - acc: 0.9196\n",
      "Epoch 255/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9195\n",
      "Epoch 00255: loss did not improve\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.2321 - acc: 0.9195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9183\n",
      "Epoch 00256: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2341 - acc: 0.9186\n",
      "Epoch 257/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9212\n",
      "Epoch 00257: loss did not improve\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.2262 - acc: 0.9214\n",
      "Epoch 258/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9210\n",
      "Epoch 00258: loss did not improve\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.2293 - acc: 0.9210\n",
      "Epoch 259/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9225\n",
      "Epoch 00259: loss did not improve\n",
      "100/100 [==============================] - 85s 854ms/step - loss: 0.2249 - acc: 0.9224\n",
      "Epoch 260/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9212\n",
      "Epoch 00260: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2287 - acc: 0.9211\n",
      "Epoch 261/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9192\n",
      "Epoch 00261: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2329 - acc: 0.9193\n",
      "Epoch 262/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2373 - acc: 0.9182\n",
      "Epoch 00262: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2374 - acc: 0.9182\n",
      "Epoch 263/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9204\n",
      "Epoch 00263: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2321 - acc: 0.9205\n",
      "Epoch 264/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9208\n",
      "Epoch 00264: loss did not improve\n",
      "100/100 [==============================] - 85s 854ms/step - loss: 0.2294 - acc: 0.9209\n",
      "Epoch 265/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9197\n",
      "Epoch 00265: loss did not improve\n",
      "100/100 [==============================] - 86s 865ms/step - loss: 0.2308 - acc: 0.9197\n",
      "Epoch 266/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2400 - acc: 0.9176\n",
      "Epoch 00266: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2399 - acc: 0.9177\n",
      "Epoch 267/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9211\n",
      "Epoch 00267: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2283 - acc: 0.9210\n",
      "Epoch 268/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9214\n",
      "Epoch 00268: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2264 - acc: 0.9210\n",
      "Epoch 269/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9184\n",
      "Epoch 00269: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2357 - acc: 0.9184\n",
      "Epoch 270/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9200\n",
      "Epoch 00270: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2313 - acc: 0.9200\n",
      "Epoch 271/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9206\n",
      "Epoch 00271: loss did not improve\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.2287 - acc: 0.9206\n",
      "Epoch 272/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9202\n",
      "Epoch 00272: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2316 - acc: 0.9201\n",
      "Epoch 273/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9197\n",
      "Epoch 00273: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2311 - acc: 0.9197\n",
      "Epoch 274/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9186\n",
      "Epoch 00274: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2342 - acc: 0.9186\n",
      "Epoch 275/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9220\n",
      "Epoch 00275: loss did not improve\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.2267 - acc: 0.9220\n",
      "Epoch 276/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9218\n",
      "Epoch 00276: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2270 - acc: 0.9220\n",
      "Epoch 277/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9203\n",
      "Epoch 00277: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2312 - acc: 0.9203\n",
      "Epoch 278/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9186\n",
      "Epoch 00278: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2349 - acc: 0.9188\n",
      "Epoch 279/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9193\n",
      "Epoch 00279: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2319 - acc: 0.9194\n",
      "Epoch 280/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9198\n",
      "Epoch 00280: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2328 - acc: 0.9197\n",
      "Epoch 281/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.9192\n",
      "Epoch 00281: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2318 - acc: 0.9192\n",
      "Epoch 282/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9228\n",
      "Epoch 00282: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2279 - acc: 0.9227\n",
      "Epoch 283/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9191\n",
      "Epoch 00283: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2307 - acc: 0.9190\n",
      "Epoch 284/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9195\n",
      "Epoch 00284: loss did not improve\n",
      "100/100 [==============================] - 89s 890ms/step - loss: 0.2334 - acc: 0.9195\n",
      "Epoch 285/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9190\n",
      "Epoch 00285: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2331 - acc: 0.9188\n",
      "Epoch 286/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.9193\n",
      "Epoch 00286: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2327 - acc: 0.9193\n",
      "Epoch 287/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9216\n",
      "Epoch 00287: loss did not improve\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.2270 - acc: 0.9215\n",
      "Epoch 288/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9197\n",
      "Epoch 00288: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2337 - acc: 0.9198\n",
      "Epoch 289/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9203\n",
      "Epoch 00289: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2309 - acc: 0.9205\n",
      "Epoch 290/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9194\n",
      "Epoch 00290: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2348 - acc: 0.9192\n",
      "Epoch 291/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9199\n",
      "Epoch 00291: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2332 - acc: 0.9201\n",
      "Epoch 292/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9235\n",
      "Epoch 00292: loss improved from 0.22310 to 0.22026, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2203 - acc: 0.9235\n",
      "Epoch 293/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2318 - acc: 0.9194\n",
      "Epoch 00293: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2319 - acc: 0.9193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9204\n",
      "Epoch 00294: loss did not improve\n",
      "100/100 [==============================] - 89s 887ms/step - loss: 0.2318 - acc: 0.9205\n",
      "Epoch 295/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9195\n",
      "Epoch 00295: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2298 - acc: 0.9196\n",
      "Epoch 296/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9203\n",
      "Epoch 00296: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2301 - acc: 0.9202\n",
      "Epoch 297/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2356 - acc: 0.9189\n",
      "Epoch 00297: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2354 - acc: 0.9189\n",
      "Epoch 298/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9219\n",
      "Epoch 00298: loss did not improve\n",
      "100/100 [==============================] - 88s 885ms/step - loss: 0.2260 - acc: 0.9221\n",
      "Epoch 299/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9201\n",
      "Epoch 00299: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2315 - acc: 0.9201\n",
      "Epoch 300/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9223\n",
      "Epoch 00300: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2248 - acc: 0.9224\n",
      "Epoch 301/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9217\n",
      "Epoch 00301: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2272 - acc: 0.9215\n",
      "Epoch 302/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9210\n",
      "Epoch 00302: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2288 - acc: 0.9209\n",
      "Epoch 303/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9174\n",
      "Epoch 00303: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2379 - acc: 0.9173\n",
      "Epoch 304/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9225\n",
      "Epoch 00304: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2258 - acc: 0.9224\n",
      "Epoch 305/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9220\n",
      "Epoch 00305: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2253 - acc: 0.9221\n",
      "Epoch 306/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9201\n",
      "Epoch 00306: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2337 - acc: 0.9204\n",
      "Epoch 307/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9189\n",
      "Epoch 00307: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2343 - acc: 0.9190\n",
      "Epoch 308/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9205\n",
      "Epoch 00308: loss did not improve\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.2310 - acc: 0.9203\n",
      "Epoch 309/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9194\n",
      "Epoch 00309: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2354 - acc: 0.9193\n",
      "Epoch 310/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9176\n",
      "Epoch 00310: loss did not improve\n",
      "100/100 [==============================] - 86s 865ms/step - loss: 0.2397 - acc: 0.9177\n",
      "Epoch 311/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9206\n",
      "Epoch 00311: loss did not improve\n",
      "100/100 [==============================] - 89s 892ms/step - loss: 0.2285 - acc: 0.9207\n",
      "Epoch 312/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9197\n",
      "Epoch 00312: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2339 - acc: 0.9195\n",
      "Epoch 313/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9220\n",
      "Epoch 00313: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2281 - acc: 0.9215\n",
      "Epoch 314/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9214\n",
      "Epoch 00314: loss did not improve\n",
      "100/100 [==============================] - 86s 865ms/step - loss: 0.2280 - acc: 0.9213\n",
      "Epoch 315/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9186\n",
      "Epoch 00315: loss did not improve\n",
      "100/100 [==============================] - 88s 884ms/step - loss: 0.2347 - acc: 0.9187\n",
      "Epoch 316/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9212\n",
      "Epoch 00316: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2286 - acc: 0.9211\n",
      "Epoch 317/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9211\n",
      "Epoch 00317: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2282 - acc: 0.9211\n",
      "Epoch 318/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9197\n",
      "Epoch 00318: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2306 - acc: 0.9195\n",
      "Epoch 319/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9201\n",
      "Epoch 00319: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2327 - acc: 0.9200\n",
      "Epoch 320/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.9193\n",
      "Epoch 00320: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2340 - acc: 0.9193\n",
      "Epoch 321/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9188\n",
      "Epoch 00321: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2350 - acc: 0.9188\n",
      "Epoch 322/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9223\n",
      "Epoch 00322: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2254 - acc: 0.9223\n",
      "Epoch 323/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9200\n",
      "Epoch 00323: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2326 - acc: 0.9198\n",
      "Epoch 324/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9174\n",
      "Epoch 00324: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2390 - acc: 0.9176\n",
      "Epoch 325/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9179\n",
      "Epoch 00325: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2360 - acc: 0.9180\n",
      "Epoch 326/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9213\n",
      "Epoch 00326: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2276 - acc: 0.9214\n",
      "Epoch 327/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9208\n",
      "Epoch 00327: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2294 - acc: 0.9208\n",
      "Epoch 328/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9201\n",
      "Epoch 00328: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2314 - acc: 0.9203\n",
      "Epoch 329/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9204\n",
      "Epoch 00329: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2313 - acc: 0.9201\n",
      "Epoch 330/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9212\n",
      "Epoch 00330: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2289 - acc: 0.9212\n",
      "Epoch 331/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9207\n",
      "Epoch 00331: loss did not improve\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.2287 - acc: 0.9205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2342 - acc: 0.9188\n",
      "Epoch 00332: loss did not improve\n",
      "100/100 [==============================] - 89s 885ms/step - loss: 0.2335 - acc: 0.9191\n",
      "Epoch 333/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9224\n",
      "Epoch 00333: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2260 - acc: 0.9221\n",
      "Epoch 334/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9199\n",
      "Epoch 00334: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2296 - acc: 0.9197\n",
      "Epoch 335/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9214\n",
      "Epoch 00335: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2287 - acc: 0.9214\n",
      "Epoch 336/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9203\n",
      "Epoch 00336: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2294 - acc: 0.9202\n",
      "Epoch 337/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9206\n",
      "Epoch 00337: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2302 - acc: 0.9206\n",
      "Epoch 338/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9204\n",
      "Epoch 00338: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2302 - acc: 0.9204\n",
      "Epoch 339/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9200\n",
      "Epoch 00339: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2326 - acc: 0.9203\n",
      "Epoch 340/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9198\n",
      "Epoch 00340: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2311 - acc: 0.9200\n",
      "Epoch 341/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9226\n",
      "Epoch 00341: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2247 - acc: 0.9224\n",
      "Epoch 342/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9201\n",
      "Epoch 00342: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2293 - acc: 0.9203\n",
      "Epoch 343/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9216\n",
      "Epoch 00343: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2260 - acc: 0.9216\n",
      "Epoch 344/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9217\n",
      "Epoch 00344: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2252 - acc: 0.9216\n",
      "Epoch 345/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9193\n",
      "Epoch 00345: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2333 - acc: 0.9191\n",
      "Epoch 346/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9186\n",
      "Epoch 00346: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2347 - acc: 0.9185\n",
      "Epoch 347/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9213\n",
      "Epoch 00347: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2257 - acc: 0.9213\n",
      "Epoch 348/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9196\n",
      "Epoch 00348: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2309 - acc: 0.9195\n",
      "Epoch 349/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9227\n",
      "Epoch 00349: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2239 - acc: 0.9226\n",
      "Epoch 350/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2399 - acc: 0.9174\n",
      "Epoch 00350: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2399 - acc: 0.9174\n",
      "Epoch 351/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9185\n",
      "Epoch 00351: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2366 - acc: 0.9185\n",
      "Epoch 352/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9198\n",
      "Epoch 00352: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2307 - acc: 0.9199\n",
      "Epoch 353/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9211\n",
      "Epoch 00353: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2278 - acc: 0.9213\n",
      "Epoch 354/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9202\n",
      "Epoch 00354: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2318 - acc: 0.9201\n",
      "Epoch 355/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9205\n",
      "Epoch 00355: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2290 - acc: 0.9205\n",
      "Epoch 356/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9188\n",
      "Epoch 00356: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2363 - acc: 0.9187\n",
      "Epoch 357/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9204\n",
      "Epoch 00357: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2303 - acc: 0.9205\n",
      "Epoch 358/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9218\n",
      "Epoch 00358: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2291 - acc: 0.9216\n",
      "Epoch 359/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9200\n",
      "Epoch 00359: loss did not improve\n",
      "100/100 [==============================] - 85s 854ms/step - loss: 0.2312 - acc: 0.9199\n",
      "Epoch 360/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2330 - acc: 0.9189\n",
      "Epoch 00360: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2328 - acc: 0.9191\n",
      "Epoch 361/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9195\n",
      "Epoch 00361: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2347 - acc: 0.9193\n",
      "Epoch 362/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9211\n",
      "Epoch 00362: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2259 - acc: 0.9211\n",
      "Epoch 363/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9221\n",
      "Epoch 00363: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2252 - acc: 0.9221\n",
      "Epoch 364/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9205\n",
      "Epoch 00364: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2306 - acc: 0.9202\n",
      "Epoch 365/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2331 - acc: 0.9187\n",
      "Epoch 00365: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2323 - acc: 0.9191\n",
      "Epoch 366/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2371 - acc: 0.9178\n",
      "Epoch 00366: loss did not improve\n",
      "100/100 [==============================] - 85s 855ms/step - loss: 0.2366 - acc: 0.9180\n",
      "Epoch 367/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9208\n",
      "Epoch 00367: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2308 - acc: 0.9203\n",
      "Epoch 368/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9213\n",
      "Epoch 00368: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2290 - acc: 0.9213\n",
      "Epoch 369/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2378 - acc: 0.9175\n",
      "Epoch 00369: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2380 - acc: 0.9173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 370/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2324 - acc: 0.9194\n",
      "Epoch 00370: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2321 - acc: 0.9195\n",
      "Epoch 371/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9211\n",
      "Epoch 00371: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2297 - acc: 0.9212\n",
      "Epoch 372/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9213\n",
      "Epoch 00372: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2284 - acc: 0.9213\n",
      "Epoch 373/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9198\n",
      "Epoch 00373: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2310 - acc: 0.9196\n",
      "Epoch 374/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9236\n",
      "Epoch 00374: loss did not improve\n",
      "100/100 [==============================] - 86s 865ms/step - loss: 0.2203 - acc: 0.9236\n",
      "Epoch 375/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2385 - acc: 0.9178\n",
      "Epoch 00375: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2382 - acc: 0.9179\n",
      "Epoch 376/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9195\n",
      "Epoch 00376: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2323 - acc: 0.9198\n",
      "Epoch 377/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9205\n",
      "Epoch 00377: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2292 - acc: 0.9206\n",
      "Epoch 378/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2365 - acc: 0.9186\n",
      "Epoch 00378: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2369 - acc: 0.9184\n",
      "Epoch 379/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9201\n",
      "Epoch 00379: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2325 - acc: 0.9203\n",
      "Epoch 380/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9202\n",
      "Epoch 00380: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2318 - acc: 0.9204\n",
      "Epoch 381/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9206\n",
      "Epoch 00381: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2300 - acc: 0.9207\n",
      "Epoch 382/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9214\n",
      "Epoch 00382: loss did not improve\n",
      "100/100 [==============================] - 88s 883ms/step - loss: 0.2294 - acc: 0.9214\n",
      "Epoch 383/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9205\n",
      "Epoch 00383: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2294 - acc: 0.9205\n",
      "Epoch 384/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9221\n",
      "Epoch 00384: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2231 - acc: 0.9222\n",
      "Epoch 385/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9207\n",
      "Epoch 00385: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2278 - acc: 0.9208\n",
      "Epoch 386/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9225\n",
      "Epoch 00386: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2249 - acc: 0.9227\n",
      "Epoch 387/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9218\n",
      "Epoch 00387: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2269 - acc: 0.9217\n",
      "Epoch 388/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9221\n",
      "Epoch 00388: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2257 - acc: 0.9222\n",
      "Epoch 389/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9226\n",
      "Epoch 00389: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2240 - acc: 0.9227\n",
      "Epoch 390/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9208\n",
      "Epoch 00390: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2274 - acc: 0.9208\n",
      "Epoch 391/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9211\n",
      "Epoch 00391: loss did not improve\n",
      "100/100 [==============================] - 88s 883ms/step - loss: 0.2292 - acc: 0.9212\n",
      "Epoch 392/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9209\n",
      "Epoch 00392: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2267 - acc: 0.9209\n",
      "Epoch 393/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9205\n",
      "Epoch 00393: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2300 - acc: 0.9203\n",
      "Epoch 394/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9207\n",
      "Epoch 00394: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2296 - acc: 0.9208\n",
      "Epoch 395/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9207\n",
      "Epoch 00395: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2294 - acc: 0.9206\n",
      "Epoch 396/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9203\n",
      "Epoch 00396: loss did not improve\n",
      "100/100 [==============================] - 88s 883ms/step - loss: 0.2292 - acc: 0.9201\n",
      "Epoch 397/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9236\n",
      "Epoch 00397: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2219 - acc: 0.9237\n",
      "Epoch 398/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9221\n",
      "Epoch 00398: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2231 - acc: 0.9221\n",
      "Epoch 399/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2378 - acc: 0.9179\n",
      "Epoch 00399: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2376 - acc: 0.9180\n",
      "Epoch 400/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.9192\n",
      "Epoch 00400: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2345 - acc: 0.9193\n",
      "Epoch 401/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9211\n",
      "Epoch 00401: loss did not improve\n",
      "100/100 [==============================] - 89s 887ms/step - loss: 0.2277 - acc: 0.9211\n",
      "Epoch 402/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9222\n",
      "Epoch 00402: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2259 - acc: 0.9218\n",
      "Epoch 403/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9221\n",
      "Epoch 00403: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2261 - acc: 0.9221\n",
      "Epoch 404/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2220 - acc: 0.9232\n",
      "Epoch 00404: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2220 - acc: 0.9232\n",
      "Epoch 405/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9213\n",
      "Epoch 00405: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2261 - acc: 0.9213\n",
      "Epoch 406/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2358 - acc: 0.9181\n",
      "Epoch 00406: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2359 - acc: 0.9180\n",
      "Epoch 407/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2309 - acc: 0.9207\n",
      "Epoch 00407: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2310 - acc: 0.9205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 408/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.9180\n",
      "Epoch 00408: loss did not improve\n",
      "100/100 [==============================] - 88s 884ms/step - loss: 0.2393 - acc: 0.9180\n",
      "Epoch 409/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9215\n",
      "Epoch 00409: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2260 - acc: 0.9217\n",
      "Epoch 410/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9201\n",
      "Epoch 00410: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2303 - acc: 0.9199\n",
      "Epoch 411/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9213\n",
      "Epoch 00411: loss did not improve\n",
      "100/100 [==============================] - 89s 891ms/step - loss: 0.2278 - acc: 0.9214\n",
      "Epoch 412/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9200\n",
      "Epoch 00412: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2325 - acc: 0.9198\n",
      "Epoch 413/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2314 - acc: 0.9206\n",
      "Epoch 00413: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2317 - acc: 0.9205\n",
      "Epoch 414/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9223\n",
      "Epoch 00414: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2272 - acc: 0.9224\n",
      "Epoch 415/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9190\n",
      "Epoch 00415: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2362 - acc: 0.9189\n",
      "Epoch 416/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9201\n",
      "Epoch 00416: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2311 - acc: 0.9201\n",
      "Epoch 417/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9206\n",
      "Epoch 00417: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2295 - acc: 0.9204\n",
      "Epoch 418/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9197\n",
      "Epoch 00418: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2339 - acc: 0.9195\n",
      "Epoch 419/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9232\n",
      "Epoch 00419: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2251 - acc: 0.9232\n",
      "Epoch 420/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9199\n",
      "Epoch 00420: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2319 - acc: 0.9199\n",
      "Epoch 421/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9195\n",
      "Epoch 00421: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2318 - acc: 0.9195\n",
      "Epoch 422/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9217\n",
      "Epoch 00422: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2269 - acc: 0.9216\n",
      "Epoch 423/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9211\n",
      "Epoch 00423: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2288 - acc: 0.9212\n",
      "Epoch 424/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9213\n",
      "Epoch 00424: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2265 - acc: 0.9212\n",
      "Epoch 425/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9217\n",
      "Epoch 00425: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2249 - acc: 0.9218\n",
      "Epoch 426/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9209\n",
      "Epoch 00426: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2276 - acc: 0.9208\n",
      "Epoch 427/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9209\n",
      "Epoch 00427: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2281 - acc: 0.9210\n",
      "Epoch 428/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9198\n",
      "Epoch 00428: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2277 - acc: 0.9197\n",
      "Epoch 429/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9232\n",
      "Epoch 00429: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2222 - acc: 0.9233\n",
      "Epoch 430/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2305 - acc: 0.9206\n",
      "Epoch 00430: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2316 - acc: 0.9203\n",
      "Epoch 431/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9203\n",
      "Epoch 00431: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2297 - acc: 0.9204\n",
      "Epoch 432/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2341 - acc: 0.9182\n",
      "Epoch 00432: loss did not improve\n",
      "100/100 [==============================] - 86s 865ms/step - loss: 0.2346 - acc: 0.9180\n",
      "Epoch 433/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2336 - acc: 0.9195\n",
      "Epoch 00433: loss did not improve\n",
      "100/100 [==============================] - 85s 853ms/step - loss: 0.2335 - acc: 0.9196\n",
      "Epoch 434/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9205\n",
      "Epoch 00434: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2287 - acc: 0.9207\n",
      "Epoch 435/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9225\n",
      "Epoch 00435: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2231 - acc: 0.9226\n",
      "Epoch 436/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2325 - acc: 0.9197\n",
      "Epoch 00436: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2322 - acc: 0.9198\n",
      "Epoch 437/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9216\n",
      "Epoch 00437: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2263 - acc: 0.9217\n",
      "Epoch 438/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9202\n",
      "Epoch 00438: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2303 - acc: 0.9201\n",
      "Epoch 439/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9228\n",
      "Epoch 00439: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2237 - acc: 0.9226\n",
      "Epoch 440/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9205\n",
      "Epoch 00440: loss did not improve\n",
      "100/100 [==============================] - 88s 884ms/step - loss: 0.2281 - acc: 0.9207\n",
      "Epoch 441/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9220\n",
      "Epoch 00441: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2261 - acc: 0.9219\n",
      "Epoch 442/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9223\n",
      "Epoch 00442: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2250 - acc: 0.9225\n",
      "Epoch 443/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9223\n",
      "Epoch 00443: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2243 - acc: 0.9224\n",
      "Epoch 444/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2332 - acc: 0.9196\n",
      "Epoch 00444: loss did not improve\n",
      "100/100 [==============================] - 85s 854ms/step - loss: 0.2329 - acc: 0.9198\n",
      "Epoch 445/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9219\n",
      "Epoch 00445: loss did not improve\n",
      "100/100 [==============================] - 88s 885ms/step - loss: 0.2259 - acc: 0.9218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 446/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9192\n",
      "Epoch 00446: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2327 - acc: 0.9191\n",
      "Epoch 447/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9233\n",
      "Epoch 00447: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2207 - acc: 0.9232\n",
      "Epoch 448/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9221\n",
      "Epoch 00448: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2255 - acc: 0.9223\n",
      "Epoch 449/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9194\n",
      "Epoch 00449: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2321 - acc: 0.9194\n",
      "Epoch 450/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9233\n",
      "Epoch 00450: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2244 - acc: 0.9234\n",
      "Epoch 451/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9211\n",
      "Epoch 00451: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2280 - acc: 0.9207\n",
      "Epoch 452/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9216\n",
      "Epoch 00452: loss did not improve\n",
      "100/100 [==============================] - 88s 885ms/step - loss: 0.2258 - acc: 0.9217\n",
      "Epoch 453/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9209\n",
      "Epoch 00453: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2299 - acc: 0.9209\n",
      "Epoch 454/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9185\n",
      "Epoch 00454: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2345 - acc: 0.9185\n",
      "Epoch 455/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2319 - acc: 0.9192\n",
      "Epoch 00455: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2316 - acc: 0.9192\n",
      "Epoch 456/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9234\n",
      "Epoch 00456: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2214 - acc: 0.9234\n",
      "Epoch 457/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9212\n",
      "Epoch 00457: loss did not improve\n",
      "100/100 [==============================] - 88s 884ms/step - loss: 0.2288 - acc: 0.9212\n",
      "Epoch 458/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9223\n",
      "Epoch 00458: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2241 - acc: 0.9222\n",
      "Epoch 459/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9206\n",
      "Epoch 00459: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2301 - acc: 0.9203\n",
      "Epoch 460/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9197\n",
      "Epoch 00460: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2338 - acc: 0.9196\n",
      "Epoch 461/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9211\n",
      "Epoch 00461: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2277 - acc: 0.9213\n",
      "Epoch 462/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9207\n",
      "Epoch 00462: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2294 - acc: 0.9205\n",
      "Epoch 463/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9208\n",
      "Epoch 00463: loss did not improve\n",
      "100/100 [==============================] - 88s 883ms/step - loss: 0.2314 - acc: 0.9208\n",
      "Epoch 464/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2224 - acc: 0.9229\n",
      "Epoch 00464: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2221 - acc: 0.9230\n",
      "Epoch 465/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9205\n",
      "Epoch 00465: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2295 - acc: 0.9204\n",
      "Epoch 466/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9231\n",
      "Epoch 00466: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2229 - acc: 0.9232\n",
      "Epoch 467/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9207\n",
      "Epoch 00467: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2300 - acc: 0.9203\n",
      "Epoch 468/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9220\n",
      "Epoch 00468: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2260 - acc: 0.9219\n",
      "Epoch 469/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9211\n",
      "Epoch 00469: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2283 - acc: 0.9211\n",
      "Epoch 470/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9207\n",
      "Epoch 00470: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2271 - acc: 0.9208\n",
      "Epoch 471/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9198\n",
      "Epoch 00471: loss did not improve\n",
      "100/100 [==============================] - 86s 865ms/step - loss: 0.2313 - acc: 0.9199\n",
      "Epoch 472/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9219\n",
      "Epoch 00472: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2245 - acc: 0.9221\n",
      "Epoch 473/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9207\n",
      "Epoch 00473: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2286 - acc: 0.9207\n",
      "Epoch 474/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9234\n",
      "Epoch 00474: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2206 - acc: 0.9233\n",
      "Epoch 475/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9212\n",
      "Epoch 00475: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2273 - acc: 0.9213\n",
      "Epoch 476/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2299 - acc: 0.9203\n",
      "Epoch 00476: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2301 - acc: 0.9201\n",
      "Epoch 477/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9217\n",
      "Epoch 00477: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2243 - acc: 0.9218\n",
      "Epoch 478/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9229\n",
      "Epoch 00478: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2242 - acc: 0.9231\n",
      "Epoch 479/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9233\n",
      "Epoch 00479: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2233 - acc: 0.9232\n",
      "Epoch 480/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2311 - acc: 0.9194\n",
      "Epoch 00480: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2320 - acc: 0.9191\n",
      "Epoch 481/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9200\n",
      "Epoch 00481: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2300 - acc: 0.9201\n",
      "Epoch 482/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2195 - acc: 0.9238\n",
      "Epoch 00482: loss improved from 0.22026 to 0.21966, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2197 - acc: 0.9237\n",
      "Epoch 483/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9217\n",
      "Epoch 00483: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2248 - acc: 0.9217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 484/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9223\n",
      "Epoch 00484: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2238 - acc: 0.9223\n",
      "Epoch 485/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2369 - acc: 0.9186\n",
      "Epoch 00485: loss did not improve\n",
      "100/100 [==============================] - 86s 865ms/step - loss: 0.2364 - acc: 0.9188\n",
      "Epoch 486/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9179\n",
      "Epoch 00486: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2361 - acc: 0.9178\n",
      "Epoch 487/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9217\n",
      "Epoch 00487: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2259 - acc: 0.9219\n",
      "Epoch 488/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9201\n",
      "Epoch 00488: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2293 - acc: 0.9200\n",
      "Epoch 489/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9227\n",
      "Epoch 00489: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2234 - acc: 0.9228\n",
      "Epoch 490/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9240\n",
      "Epoch 00490: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2213 - acc: 0.9240\n",
      "Epoch 491/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9184\n",
      "Epoch 00491: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2326 - acc: 0.9185\n",
      "Epoch 492/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9204\n",
      "Epoch 00492: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2332 - acc: 0.9200\n",
      "Epoch 493/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.9185\n",
      "Epoch 00493: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2344 - acc: 0.9185\n",
      "Epoch 494/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9220\n",
      "Epoch 00494: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2234 - acc: 0.9219\n",
      "Epoch 495/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9210\n",
      "Epoch 00495: loss did not improve\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.2277 - acc: 0.9209\n",
      "Epoch 496/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9227\n",
      "Epoch 00496: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2221 - acc: 0.9228\n",
      "Epoch 497/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9207\n",
      "Epoch 00497: loss did not improve\n",
      "100/100 [==============================] - 85s 854ms/step - loss: 0.2291 - acc: 0.9207\n",
      "Epoch 498/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9216\n",
      "Epoch 00498: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2269 - acc: 0.9216\n",
      "Epoch 499/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9219\n",
      "Epoch 00499: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2258 - acc: 0.9220\n",
      "Epoch 500/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9226\n",
      "Epoch 00500: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2246 - acc: 0.9226\n",
      "Epoch 501/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9214\n",
      "Epoch 00501: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2265 - acc: 0.9213\n",
      "Epoch 502/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9214\n",
      "Epoch 00502: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2270 - acc: 0.9215\n",
      "Epoch 503/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9196\n",
      "Epoch 00503: loss did not improve\n",
      "100/100 [==============================] - 85s 854ms/step - loss: 0.2319 - acc: 0.9196\n",
      "Epoch 504/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9228\n",
      "Epoch 00504: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2243 - acc: 0.9229\n",
      "Epoch 505/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9188\n",
      "Epoch 00505: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2337 - acc: 0.9188\n",
      "Epoch 506/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9221\n",
      "Epoch 00506: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2265 - acc: 0.9216\n",
      "Epoch 507/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9228\n",
      "Epoch 00507: loss did not improve\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.2223 - acc: 0.9227\n",
      "Epoch 508/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2348 - acc: 0.9190\n",
      "Epoch 00508: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2361 - acc: 0.9185\n",
      "Epoch 509/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9217\n",
      "Epoch 00509: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2259 - acc: 0.9216\n",
      "Epoch 510/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9200\n",
      "Epoch 00510: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2305 - acc: 0.9200\n",
      "Epoch 511/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2279 - acc: 0.9200\n",
      "Epoch 00511: loss did not improve\n",
      "100/100 [==============================] - 89s 888ms/step - loss: 0.2278 - acc: 0.9200\n",
      "Epoch 512/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9196\n",
      "Epoch 00512: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2284 - acc: 0.9195\n",
      "Epoch 513/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2334 - acc: 0.9194\n",
      "Epoch 00513: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2332 - acc: 0.9195\n",
      "Epoch 514/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9197\n",
      "Epoch 00514: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2311 - acc: 0.9197\n",
      "Epoch 515/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9226\n",
      "Epoch 00515: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2229 - acc: 0.9227\n",
      "Epoch 516/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9213\n",
      "Epoch 00516: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2226 - acc: 0.9214\n",
      "Epoch 517/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9197\n",
      "Epoch 00517: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2289 - acc: 0.9197\n",
      "Epoch 518/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9214\n",
      "Epoch 00518: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2256 - acc: 0.9213\n",
      "Epoch 519/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2216 - acc: 0.9226\n",
      "Epoch 00519: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2214 - acc: 0.9226\n",
      "Epoch 520/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9204\n",
      "Epoch 00520: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2273 - acc: 0.9203\n",
      "Epoch 521/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9224\n",
      "Epoch 00521: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2229 - acc: 0.9224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 522/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9203\n",
      "Epoch 00522: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2294 - acc: 0.9205\n",
      "Epoch 523/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9191\n",
      "Epoch 00523: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2312 - acc: 0.9194\n",
      "Epoch 524/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9230\n",
      "Epoch 00524: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2223 - acc: 0.9226\n",
      "Epoch 525/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9216\n",
      "Epoch 00525: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2284 - acc: 0.9213\n",
      "Epoch 526/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9224\n",
      "Epoch 00526: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2228 - acc: 0.9224\n",
      "Epoch 527/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9220\n",
      "Epoch 00527: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2255 - acc: 0.9221\n",
      "Epoch 528/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9201\n",
      "Epoch 00528: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2288 - acc: 0.9203\n",
      "Epoch 529/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9203\n",
      "Epoch 00529: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2274 - acc: 0.9202\n",
      "Epoch 530/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9202\n",
      "Epoch 00530: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2283 - acc: 0.9203\n",
      "Epoch 531/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9204\n",
      "Epoch 00531: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2290 - acc: 0.9202\n",
      "Epoch 532/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9219\n",
      "Epoch 00532: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2248 - acc: 0.9218\n",
      "Epoch 533/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9225\n",
      "Epoch 00533: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2238 - acc: 0.9226\n",
      "Epoch 534/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2307 - acc: 0.9199\n",
      "Epoch 00534: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2318 - acc: 0.9196\n",
      "Epoch 535/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9212\n",
      "Epoch 00535: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2276 - acc: 0.9213\n",
      "Epoch 536/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9212\n",
      "Epoch 00536: loss did not improve\n",
      "100/100 [==============================] - 86s 865ms/step - loss: 0.2261 - acc: 0.9211\n",
      "Epoch 537/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9202\n",
      "Epoch 00537: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2271 - acc: 0.9202\n",
      "Epoch 538/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9211\n",
      "Epoch 00538: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2285 - acc: 0.9210\n",
      "Epoch 539/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2306 - acc: 0.9200\n",
      "Epoch 00539: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2301 - acc: 0.9202\n",
      "Epoch 540/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9222\n",
      "Epoch 00540: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2231 - acc: 0.9220\n",
      "Epoch 541/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2294 - acc: 0.9197\n",
      "Epoch 00541: loss did not improve\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.2289 - acc: 0.9199\n",
      "Epoch 542/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9229\n",
      "Epoch 00542: loss improved from 0.21966 to 0.21963, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2196 - acc: 0.9230\n",
      "Epoch 543/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9221\n",
      "Epoch 00543: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2261 - acc: 0.9221\n",
      "Epoch 544/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2172 - acc: 0.9245\n",
      "Epoch 00544: loss improved from 0.21963 to 0.21741, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2174 - acc: 0.9244\n",
      "Epoch 545/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9212\n",
      "Epoch 00545: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2279 - acc: 0.9210\n",
      "Epoch 546/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9197\n",
      "Epoch 00546: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2287 - acc: 0.9199\n",
      "Epoch 547/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9209\n",
      "Epoch 00547: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2291 - acc: 0.9208\n",
      "Epoch 548/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9213\n",
      "Epoch 00548: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2251 - acc: 0.9214\n",
      "Epoch 549/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9224\n",
      "Epoch 00549: loss did not improve\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.2238 - acc: 0.9221\n",
      "Epoch 550/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2304 - acc: 0.9192\n",
      "Epoch 00550: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2303 - acc: 0.9192\n",
      "Epoch 551/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2296 - acc: 0.9200\n",
      "Epoch 00551: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2292 - acc: 0.9201\n",
      "Epoch 552/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9236\n",
      "Epoch 00552: loss did not improve\n",
      "100/100 [==============================] - 88s 883ms/step - loss: 0.2198 - acc: 0.9237\n",
      "Epoch 553/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9199\n",
      "Epoch 00553: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2278 - acc: 0.9200\n",
      "Epoch 554/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9206\n",
      "Epoch 00554: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2291 - acc: 0.9206\n",
      "Epoch 555/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9217\n",
      "Epoch 00555: loss did not improve\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.2246 - acc: 0.9218\n",
      "Epoch 556/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9224\n",
      "Epoch 00556: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2224 - acc: 0.9225\n",
      "Epoch 557/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9209\n",
      "Epoch 00557: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2279 - acc: 0.9210\n",
      "Epoch 558/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9209\n",
      "Epoch 00558: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2277 - acc: 0.9209\n",
      "Epoch 559/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9207\n",
      "Epoch 00559: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2270 - acc: 0.9205\n",
      "Epoch 560/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2155 - acc: 0.9253\n",
      "Epoch 00560: loss improved from 0.21741 to 0.21522, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2152 - acc: 0.9254\n",
      "Epoch 561/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9199\n",
      "Epoch 00561: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2296 - acc: 0.9198\n",
      "Epoch 562/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9224\n",
      "Epoch 00562: loss did not improve\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.2236 - acc: 0.9223\n",
      "Epoch 563/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9199\n",
      "Epoch 00563: loss did not improve\n",
      "100/100 [==============================] - 89s 886ms/step - loss: 0.2309 - acc: 0.9199\n",
      "Epoch 564/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9208\n",
      "Epoch 00564: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2236 - acc: 0.9211\n",
      "Epoch 565/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9209\n",
      "Epoch 00565: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2271 - acc: 0.9208\n",
      "Epoch 566/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9203\n",
      "Epoch 00566: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2285 - acc: 0.9201\n",
      "Epoch 567/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9232\n",
      "Epoch 00567: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2221 - acc: 0.9233\n",
      "Epoch 568/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9198\n",
      "Epoch 00568: loss did not improve\n",
      "100/100 [==============================] - 88s 883ms/step - loss: 0.2280 - acc: 0.9196\n",
      "Epoch 569/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9214\n",
      "Epoch 00569: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2255 - acc: 0.9215\n",
      "Epoch 570/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9211\n",
      "Epoch 00570: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2261 - acc: 0.9208\n",
      "Epoch 571/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9206\n",
      "Epoch 00571: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2270 - acc: 0.9206\n",
      "Epoch 572/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9206\n",
      "Epoch 00572: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2283 - acc: 0.9206\n",
      "Epoch 573/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2232 - acc: 0.9225\n",
      "Epoch 00573: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2233 - acc: 0.9225\n",
      "Epoch 574/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9224\n",
      "Epoch 00574: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2245 - acc: 0.9222\n",
      "Epoch 575/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9207\n",
      "Epoch 00575: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2300 - acc: 0.9208\n",
      "Epoch 576/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2237 - acc: 0.9219\n",
      "Epoch 00576: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2240 - acc: 0.9218\n",
      "Epoch 577/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9227\n",
      "Epoch 00577: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2215 - acc: 0.9227\n",
      "Epoch 578/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9232\n",
      "Epoch 00578: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2216 - acc: 0.9230\n",
      "Epoch 579/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2249 - acc: 0.9217\n",
      "Epoch 00579: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2252 - acc: 0.9215\n",
      "Epoch 580/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9197\n",
      "Epoch 00580: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2289 - acc: 0.9198\n",
      "Epoch 581/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9222\n",
      "Epoch 00581: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2236 - acc: 0.9221\n",
      "Epoch 582/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9200\n",
      "Epoch 00582: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2281 - acc: 0.9202\n",
      "Epoch 583/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9226\n",
      "Epoch 00583: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2229 - acc: 0.9225\n",
      "Epoch 584/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9218\n",
      "Epoch 00584: loss did not improve\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.2249 - acc: 0.9218\n",
      "Epoch 585/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9231\n",
      "Epoch 00585: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2200 - acc: 0.9230\n",
      "Epoch 586/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9213\n",
      "Epoch 00586: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2265 - acc: 0.9212\n",
      "Epoch 587/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9223\n",
      "Epoch 00587: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2225 - acc: 0.9222\n",
      "Epoch 588/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.9192\n",
      "Epoch 00588: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2324 - acc: 0.9190\n",
      "Epoch 589/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2164 - acc: 0.9246\n",
      "Epoch 00589: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2166 - acc: 0.9246\n",
      "Epoch 590/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9211\n",
      "Epoch 00590: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2276 - acc: 0.9210\n",
      "Epoch 591/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2272 - acc: 0.9216\n",
      "Epoch 00591: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2274 - acc: 0.9215\n",
      "Epoch 592/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9202\n",
      "Epoch 00592: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2282 - acc: 0.9203\n",
      "Epoch 593/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9203\n",
      "Epoch 00593: loss did not improve\n",
      "100/100 [==============================] - 86s 858ms/step - loss: 0.2290 - acc: 0.9202\n",
      "Epoch 594/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2297 - acc: 0.9207\n",
      "Epoch 00594: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2297 - acc: 0.9207\n",
      "Epoch 595/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2327 - acc: 0.9189\n",
      "Epoch 00595: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2327 - acc: 0.9188\n",
      "Epoch 596/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9215\n",
      "Epoch 00596: loss did not improve\n",
      "100/100 [==============================] - 89s 885ms/step - loss: 0.2238 - acc: 0.9215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 597/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9214\n",
      "Epoch 00597: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2260 - acc: 0.9217\n",
      "Epoch 598/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9225\n",
      "Epoch 00598: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2234 - acc: 0.9224\n",
      "Epoch 599/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9224\n",
      "Epoch 00599: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2231 - acc: 0.9224\n",
      "Epoch 600/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9209\n",
      "Epoch 00600: loss did not improve\n",
      "100/100 [==============================] - 89s 887ms/step - loss: 0.2294 - acc: 0.9208\n",
      "Epoch 601/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9224\n",
      "Epoch 00601: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2213 - acc: 0.9222\n",
      "Epoch 602/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9219\n",
      "Epoch 00602: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2224 - acc: 0.9220\n",
      "Epoch 603/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9196\n",
      "Epoch 00603: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2312 - acc: 0.9195\n",
      "Epoch 604/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9218\n",
      "Epoch 00604: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2263 - acc: 0.9215\n",
      "Epoch 605/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9206\n",
      "Epoch 00605: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2284 - acc: 0.9209\n",
      "Epoch 606/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9197\n",
      "Epoch 00606: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2281 - acc: 0.9197\n",
      "Epoch 607/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9228\n",
      "Epoch 00607: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2229 - acc: 0.9228\n",
      "Epoch 608/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.9188\n",
      "Epoch 00608: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2339 - acc: 0.9188\n",
      "Epoch 609/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9200\n",
      "Epoch 00609: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2316 - acc: 0.9201\n",
      "Epoch 610/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9197\n",
      "Epoch 00610: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2292 - acc: 0.9198\n",
      "Epoch 611/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9218\n",
      "Epoch 00611: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2255 - acc: 0.9219\n",
      "Epoch 612/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9229\n",
      "Epoch 00612: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2214 - acc: 0.9227\n",
      "Epoch 613/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9205\n",
      "Epoch 00613: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2278 - acc: 0.9206\n",
      "Epoch 614/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9200\n",
      "Epoch 00614: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2282 - acc: 0.9199\n",
      "Epoch 615/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9219\n",
      "Epoch 00615: loss did not improve\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.2250 - acc: 0.9219\n",
      "Epoch 616/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9215\n",
      "Epoch 00616: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2226 - acc: 0.9214\n",
      "Epoch 617/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9219\n",
      "Epoch 00617: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2249 - acc: 0.9220\n",
      "Epoch 618/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9225\n",
      "Epoch 00618: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2211 - acc: 0.9227\n",
      "Epoch 619/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9213\n",
      "Epoch 00619: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2255 - acc: 0.9212\n",
      "Epoch 620/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9231\n",
      "Epoch 00620: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2208 - acc: 0.9234\n",
      "Epoch 621/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2285 - acc: 0.9202\n",
      "Epoch 00621: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2286 - acc: 0.9201\n",
      "Epoch 622/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9218\n",
      "Epoch 00622: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2247 - acc: 0.9215\n",
      "Epoch 623/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9196\n",
      "Epoch 00623: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2288 - acc: 0.9197\n",
      "Epoch 624/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9202\n",
      "Epoch 00624: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2295 - acc: 0.9201\n",
      "Epoch 625/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9231\n",
      "Epoch 00625: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2207 - acc: 0.9233\n",
      "Epoch 626/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9213\n",
      "Epoch 00626: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2229 - acc: 0.9216\n",
      "Epoch 627/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9225\n",
      "Epoch 00627: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2217 - acc: 0.9222\n",
      "Epoch 628/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9230\n",
      "Epoch 00628: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2205 - acc: 0.9229\n",
      "Epoch 629/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9222\n",
      "Epoch 00629: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2220 - acc: 0.9223\n",
      "Epoch 630/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9200\n",
      "Epoch 00630: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2306 - acc: 0.9200\n",
      "Epoch 631/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2275 - acc: 0.9196\n",
      "Epoch 00631: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2273 - acc: 0.9197\n",
      "Epoch 632/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9209\n",
      "Epoch 00632: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2256 - acc: 0.9209\n",
      "Epoch 633/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2207 - acc: 0.9217\n",
      "Epoch 00633: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2201 - acc: 0.9219\n",
      "Epoch 634/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2224 - acc: 0.9228\n",
      "Epoch 00634: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2223 - acc: 0.9227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 635/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9170\n",
      "Epoch 00635: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2393 - acc: 0.9170\n",
      "Epoch 636/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9203\n",
      "Epoch 00636: loss did not improve\n",
      "100/100 [==============================] - 85s 849ms/step - loss: 0.2276 - acc: 0.9204\n",
      "Epoch 637/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9200\n",
      "Epoch 00637: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2266 - acc: 0.9200\n",
      "Epoch 638/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 0.9233\n",
      "Epoch 00638: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2197 - acc: 0.9234\n",
      "Epoch 639/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2316 - acc: 0.9195\n",
      "Epoch 00639: loss did not improve\n",
      "100/100 [==============================] - 89s 886ms/step - loss: 0.2310 - acc: 0.9197\n",
      "Epoch 640/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9207\n",
      "Epoch 00640: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2256 - acc: 0.9205\n",
      "Epoch 641/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9192\n",
      "Epoch 00641: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2308 - acc: 0.9194\n",
      "Epoch 642/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9209\n",
      "Epoch 00642: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2268 - acc: 0.9210\n",
      "Epoch 643/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2224 - acc: 0.9227\n",
      "Epoch 00643: loss did not improve\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.2226 - acc: 0.9226\n",
      "Epoch 644/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9199\n",
      "Epoch 00644: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2279 - acc: 0.9198\n",
      "Epoch 645/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9196\n",
      "Epoch 00645: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2313 - acc: 0.9196\n",
      "Epoch 646/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9198\n",
      "Epoch 00646: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2287 - acc: 0.9199\n",
      "Epoch 647/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2171 - acc: 0.9232\n",
      "Epoch 00647: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2165 - acc: 0.9235\n",
      "Epoch 648/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2244 - acc: 0.9220\n",
      "Epoch 00648: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2242 - acc: 0.9220\n",
      "Epoch 649/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9211\n",
      "Epoch 00649: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2256 - acc: 0.9212\n",
      "Epoch 650/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9194\n",
      "Epoch 00650: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2290 - acc: 0.9195\n",
      "Epoch 651/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2320 - acc: 0.9193\n",
      "Epoch 00651: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2317 - acc: 0.9194\n",
      "Epoch 652/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9211\n",
      "Epoch 00652: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2278 - acc: 0.9210\n",
      "Epoch 653/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9206\n",
      "Epoch 00653: loss did not improve\n",
      "100/100 [==============================] - 89s 887ms/step - loss: 0.2248 - acc: 0.9207\n",
      "Epoch 654/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2359 - acc: 0.9185\n",
      "Epoch 00654: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2360 - acc: 0.9184\n",
      "Epoch 655/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9215\n",
      "Epoch 00655: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2237 - acc: 0.9212\n",
      "Epoch 656/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9230\n",
      "Epoch 00656: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2241 - acc: 0.9228\n",
      "Epoch 657/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2230 - acc: 0.9220\n",
      "Epoch 00657: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2232 - acc: 0.9220\n",
      "Epoch 658/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9220\n",
      "Epoch 00658: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2231 - acc: 0.9223\n",
      "Epoch 659/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2186 - acc: 0.9235\n",
      "Epoch 00659: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2197 - acc: 0.9231\n",
      "Epoch 660/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2385 - acc: 0.9177\n",
      "Epoch 00660: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2381 - acc: 0.9178\n",
      "Epoch 661/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9226\n",
      "Epoch 00661: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2219 - acc: 0.9226\n",
      "Epoch 662/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9195\n",
      "Epoch 00662: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2292 - acc: 0.9192\n",
      "Epoch 663/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9207\n",
      "Epoch 00663: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2277 - acc: 0.9208\n",
      "Epoch 664/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2295 - acc: 0.9206\n",
      "Epoch 00664: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2293 - acc: 0.9206\n",
      "Epoch 665/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9199\n",
      "Epoch 00665: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2280 - acc: 0.9200\n",
      "Epoch 666/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9218\n",
      "Epoch 00666: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2233 - acc: 0.9219\n",
      "Epoch 667/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9224\n",
      "Epoch 00667: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2242 - acc: 0.9224\n",
      "Epoch 668/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9212\n",
      "Epoch 00668: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2246 - acc: 0.9214\n",
      "Epoch 669/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9218\n",
      "Epoch 00669: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2225 - acc: 0.9219\n",
      "Epoch 670/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9231\n",
      "Epoch 00670: loss did not improve\n",
      "100/100 [==============================] - 89s 885ms/step - loss: 0.2196 - acc: 0.9232\n",
      "Epoch 671/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9236\n",
      "Epoch 00671: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2213 - acc: 0.9236\n",
      "Epoch 672/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9210\n",
      "Epoch 00672: loss did not improve\n",
      "100/100 [==============================] - 88s 883ms/step - loss: 0.2269 - acc: 0.9210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 673/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2162 - acc: 0.9243\n",
      "Epoch 00673: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2161 - acc: 0.9244\n",
      "Epoch 674/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2234 - acc: 0.9213\n",
      "Epoch 00674: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2237 - acc: 0.9211\n",
      "Epoch 675/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9216\n",
      "Epoch 00675: loss did not improve\n",
      "100/100 [==============================] - 88s 883ms/step - loss: 0.2235 - acc: 0.9218\n",
      "Epoch 676/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9218\n",
      "Epoch 00676: loss did not improve\n",
      "100/100 [==============================] - 89s 887ms/step - loss: 0.2242 - acc: 0.9220\n",
      "Epoch 677/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9219\n",
      "Epoch 00677: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2217 - acc: 0.9219\n",
      "Epoch 678/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9207\n",
      "Epoch 00678: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2272 - acc: 0.9210\n",
      "Epoch 679/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9210\n",
      "Epoch 00679: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2260 - acc: 0.9211\n",
      "Epoch 680/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2262 - acc: 0.9203\n",
      "Epoch 00680: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2263 - acc: 0.9204\n",
      "Epoch 681/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9239\n",
      "Epoch 00681: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2186 - acc: 0.9239\n",
      "Epoch 682/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9224\n",
      "Epoch 00682: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2222 - acc: 0.9224\n",
      "Epoch 683/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9227\n",
      "Epoch 00683: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2207 - acc: 0.9227\n",
      "Epoch 684/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2333 - acc: 0.9190\n",
      "Epoch 00684: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2331 - acc: 0.9190\n",
      "Epoch 685/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9202\n",
      "Epoch 00685: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2280 - acc: 0.9203\n",
      "Epoch 686/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9227\n",
      "Epoch 00686: loss did not improve\n",
      "100/100 [==============================] - 88s 885ms/step - loss: 0.2211 - acc: 0.9228\n",
      "Epoch 687/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9226\n",
      "Epoch 00687: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2235 - acc: 0.9227\n",
      "Epoch 688/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9240\n",
      "Epoch 00688: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2198 - acc: 0.9239\n",
      "Epoch 689/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2184 - acc: 0.9232\n",
      "Epoch 00689: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2184 - acc: 0.9232\n",
      "Epoch 690/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9204\n",
      "Epoch 00690: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2282 - acc: 0.9203\n",
      "Epoch 691/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2269 - acc: 0.9210\n",
      "Epoch 00691: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2272 - acc: 0.9209\n",
      "Epoch 692/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2302 - acc: 0.9196\n",
      "Epoch 00692: loss did not improve\n",
      "100/100 [==============================] - 88s 884ms/step - loss: 0.2303 - acc: 0.9196\n",
      "Epoch 693/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9204\n",
      "Epoch 00693: loss did not improve\n",
      "100/100 [==============================] - 89s 890ms/step - loss: 0.2282 - acc: 0.9203\n",
      "Epoch 694/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9208\n",
      "Epoch 00694: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2269 - acc: 0.9209\n",
      "Epoch 695/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9204\n",
      "Epoch 00695: loss did not improve\n",
      "100/100 [==============================] - 88s 884ms/step - loss: 0.2270 - acc: 0.9205\n",
      "Epoch 696/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9233\n",
      "Epoch 00696: loss did not improve\n",
      "100/100 [==============================] - 89s 886ms/step - loss: 0.2220 - acc: 0.9232\n",
      "Epoch 697/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9204\n",
      "Epoch 00697: loss did not improve\n",
      "100/100 [==============================] - 89s 886ms/step - loss: 0.2264 - acc: 0.9205\n",
      "Epoch 698/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9208\n",
      "Epoch 00698: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2291 - acc: 0.9207\n",
      "Epoch 699/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9220\n",
      "Epoch 00699: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2256 - acc: 0.9220\n",
      "Epoch 700/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9212\n",
      "Epoch 00700: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2274 - acc: 0.9211\n",
      "Epoch 701/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2292 - acc: 0.9200\n",
      "Epoch 00701: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2289 - acc: 0.9202\n",
      "Epoch 702/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9206\n",
      "Epoch 00702: loss did not improve\n",
      "100/100 [==============================] - 89s 887ms/step - loss: 0.2262 - acc: 0.9207\n",
      "Epoch 703/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2288 - acc: 0.9201\n",
      "Epoch 00703: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2285 - acc: 0.9202\n",
      "Epoch 704/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2200 - acc: 0.9237\n",
      "Epoch 00704: loss did not improve\n",
      "100/100 [==============================] - 89s 886ms/step - loss: 0.2203 - acc: 0.9235\n",
      "Epoch 705/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2315 - acc: 0.9198\n",
      "Epoch 00705: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2329 - acc: 0.9193\n",
      "Epoch 706/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2258 - acc: 0.9212\n",
      "Epoch 00706: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2257 - acc: 0.9213\n",
      "Epoch 707/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2208 - acc: 0.9231\n",
      "Epoch 00707: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2210 - acc: 0.9231\n",
      "Epoch 708/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9216\n",
      "Epoch 00708: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2270 - acc: 0.9215\n",
      "Epoch 709/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9222\n",
      "Epoch 00709: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2212 - acc: 0.9222\n",
      "Epoch 710/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9227\n",
      "Epoch 00710: loss did not improve\n",
      "100/100 [==============================] - 89s 891ms/step - loss: 0.2201 - acc: 0.9227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 711/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9212\n",
      "Epoch 00711: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2236 - acc: 0.9211\n",
      "Epoch 712/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9213\n",
      "Epoch 00712: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2243 - acc: 0.9213\n",
      "Epoch 713/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9211\n",
      "Epoch 00713: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2223 - acc: 0.9213\n",
      "Epoch 714/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9215\n",
      "Epoch 00714: loss did not improve\n",
      "100/100 [==============================] - 89s 888ms/step - loss: 0.2258 - acc: 0.9216\n",
      "Epoch 715/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9217\n",
      "Epoch 00715: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2245 - acc: 0.9215\n",
      "Epoch 716/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9215\n",
      "Epoch 00716: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2243 - acc: 0.9215\n",
      "Epoch 717/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9194\n",
      "Epoch 00717: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2286 - acc: 0.9194\n",
      "Epoch 718/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2287 - acc: 0.9196\n",
      "Epoch 00718: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2287 - acc: 0.9197\n",
      "Epoch 719/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2238 - acc: 0.9209\n",
      "Epoch 00719: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2237 - acc: 0.9209\n",
      "Epoch 720/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9225\n",
      "Epoch 00720: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2204 - acc: 0.9227\n",
      "Epoch 721/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2265 - acc: 0.9207\n",
      "Epoch 00721: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2280 - acc: 0.9201\n",
      "Epoch 722/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9215\n",
      "Epoch 00722: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2234 - acc: 0.9212\n",
      "Epoch 723/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9214\n",
      "Epoch 00723: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2240 - acc: 0.9214\n",
      "Epoch 724/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9232\n",
      "Epoch 00724: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2208 - acc: 0.9232\n",
      "Epoch 725/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9222\n",
      "Epoch 00725: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2232 - acc: 0.9222\n",
      "Epoch 726/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9235\n",
      "Epoch 00726: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2192 - acc: 0.9236\n",
      "Epoch 727/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2226 - acc: 0.9229\n",
      "Epoch 00727: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2223 - acc: 0.9230\n",
      "Epoch 728/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2191 - acc: 0.9238\n",
      "Epoch 00728: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2186 - acc: 0.9239\n",
      "Epoch 729/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9201\n",
      "Epoch 00729: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2268 - acc: 0.9202\n",
      "Epoch 730/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9210\n",
      "Epoch 00730: loss did not improve\n",
      "100/100 [==============================] - 89s 887ms/step - loss: 0.2259 - acc: 0.9209\n",
      "Epoch 731/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9204\n",
      "Epoch 00731: loss did not improve\n",
      "100/100 [==============================] - 88s 883ms/step - loss: 0.2259 - acc: 0.9201\n",
      "Epoch 732/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9257\n",
      "Epoch 00732: loss improved from 0.21522 to 0.21418, saving model to net_depth_seg_new_small_aug.hdf5\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2142 - acc: 0.9256\n",
      "Epoch 733/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9205\n",
      "Epoch 00733: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2255 - acc: 0.9205\n",
      "Epoch 734/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9208\n",
      "Epoch 00734: loss did not improve\n",
      "100/100 [==============================] - 89s 886ms/step - loss: 0.2276 - acc: 0.9208\n",
      "Epoch 735/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9234\n",
      "Epoch 00735: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2202 - acc: 0.9236\n",
      "Epoch 736/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2247 - acc: 0.9212\n",
      "Epoch 00736: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2243 - acc: 0.9214\n",
      "Epoch 737/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2255 - acc: 0.9216\n",
      "Epoch 00737: loss did not improve\n",
      "100/100 [==============================] - 89s 889ms/step - loss: 0.2256 - acc: 0.9215\n",
      "Epoch 738/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2271 - acc: 0.9210\n",
      "Epoch 00738: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2268 - acc: 0.9211\n",
      "Epoch 739/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2277 - acc: 0.9207\n",
      "Epoch 00739: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2279 - acc: 0.9207\n",
      "Epoch 740/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2270 - acc: 0.9202\n",
      "Epoch 00740: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2271 - acc: 0.9200\n",
      "Epoch 741/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2167 - acc: 0.9242\n",
      "Epoch 00741: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2168 - acc: 0.9241\n",
      "Epoch 742/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9216\n",
      "Epoch 00742: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2250 - acc: 0.9219\n",
      "Epoch 743/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9222\n",
      "Epoch 00743: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2190 - acc: 0.9224\n",
      "Epoch 744/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9198\n",
      "Epoch 00744: loss did not improve\n",
      "100/100 [==============================] - 89s 890ms/step - loss: 0.2286 - acc: 0.9199\n",
      "Epoch 745/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9213\n",
      "Epoch 00745: loss did not improve\n",
      "100/100 [==============================] - 89s 890ms/step - loss: 0.2258 - acc: 0.9214\n",
      "Epoch 746/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9210\n",
      "Epoch 00746: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2261 - acc: 0.9209\n",
      "Epoch 747/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9209\n",
      "Epoch 00747: loss did not improve\n",
      "100/100 [==============================] - 89s 885ms/step - loss: 0.2262 - acc: 0.9209\n",
      "Epoch 748/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9215\n",
      "Epoch 00748: loss did not improve\n",
      "100/100 [==============================] - 88s 883ms/step - loss: 0.2260 - acc: 0.9215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 749/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9224\n",
      "Epoch 00749: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2202 - acc: 0.9223\n",
      "Epoch 750/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9209\n",
      "Epoch 00750: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2239 - acc: 0.9209\n",
      "Epoch 751/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2231 - acc: 0.9226\n",
      "Epoch 00751: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2232 - acc: 0.9225\n",
      "Epoch 752/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9215\n",
      "Epoch 00752: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2238 - acc: 0.9215\n",
      "Epoch 753/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2226 - acc: 0.9226\n",
      "Epoch 00753: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2232 - acc: 0.9225\n",
      "Epoch 754/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9220\n",
      "Epoch 00754: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2222 - acc: 0.9220\n",
      "Epoch 755/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9196\n",
      "Epoch 00755: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2290 - acc: 0.9193\n",
      "Epoch 756/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2190 - acc: 0.9230\n",
      "Epoch 00756: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2190 - acc: 0.9230\n",
      "Epoch 757/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2199 - acc: 0.9234\n",
      "Epoch 00757: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2198 - acc: 0.9234\n",
      "Epoch 758/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9218\n",
      "Epoch 00758: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2242 - acc: 0.9217\n",
      "Epoch 759/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2204 - acc: 0.9232\n",
      "Epoch 00759: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2203 - acc: 0.9232\n",
      "Epoch 760/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9207\n",
      "Epoch 00760: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2266 - acc: 0.9206\n",
      "Epoch 761/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9218\n",
      "Epoch 00761: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2230 - acc: 0.9217\n",
      "Epoch 762/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9227\n",
      "Epoch 00762: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2219 - acc: 0.9226\n",
      "Epoch 763/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2178 - acc: 0.9244\n",
      "Epoch 00763: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2175 - acc: 0.9244\n",
      "Epoch 764/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9201\n",
      "Epoch 00764: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2285 - acc: 0.9200\n",
      "Epoch 765/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2273 - acc: 0.9212\n",
      "Epoch 00765: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2275 - acc: 0.9211\n",
      "Epoch 766/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2209 - acc: 0.9220\n",
      "Epoch 00766: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2208 - acc: 0.9220\n",
      "Epoch 767/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9228\n",
      "Epoch 00767: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2216 - acc: 0.9228\n",
      "Epoch 768/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2214 - acc: 0.9219\n",
      "Epoch 00768: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2216 - acc: 0.9219\n",
      "Epoch 769/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2206 - acc: 0.9232\n",
      "Epoch 00769: loss did not improve\n",
      "100/100 [==============================] - 85s 847ms/step - loss: 0.2223 - acc: 0.9227\n",
      "Epoch 770/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9195\n",
      "Epoch 00770: loss did not improve\n",
      "100/100 [==============================] - 86s 864ms/step - loss: 0.2302 - acc: 0.9195\n",
      "Epoch 771/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9201\n",
      "Epoch 00771: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2288 - acc: 0.9202\n",
      "Epoch 772/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9199\n",
      "Epoch 00772: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2283 - acc: 0.9199\n",
      "Epoch 773/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9225\n",
      "Epoch 00773: loss did not improve\n",
      "100/100 [==============================] - 86s 865ms/step - loss: 0.2192 - acc: 0.9227\n",
      "Epoch 774/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9226\n",
      "Epoch 00774: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2224 - acc: 0.9226\n",
      "Epoch 775/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2203 - acc: 0.9227\n",
      "Epoch 00775: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2206 - acc: 0.9226\n",
      "Epoch 776/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9200\n",
      "Epoch 00776: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2283 - acc: 0.9200\n",
      "Epoch 777/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9210\n",
      "Epoch 00777: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2280 - acc: 0.9210\n",
      "Epoch 778/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9222\n",
      "Epoch 00778: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2220 - acc: 0.9224\n",
      "Epoch 779/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9200\n",
      "Epoch 00779: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2280 - acc: 0.9201\n",
      "Epoch 780/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.9233\n",
      "Epoch 00780: loss did not improve\n",
      "100/100 [==============================] - 87s 868ms/step - loss: 0.2194 - acc: 0.9231\n",
      "Epoch 781/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2223 - acc: 0.9226\n",
      "Epoch 00781: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2220 - acc: 0.9226\n",
      "Epoch 782/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9212\n",
      "Epoch 00782: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2253 - acc: 0.9214\n",
      "Epoch 783/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9215\n",
      "Epoch 00783: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2239 - acc: 0.9215\n",
      "Epoch 784/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9222\n",
      "Epoch 00784: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2228 - acc: 0.9224\n",
      "Epoch 785/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2227 - acc: 0.9224\n",
      "Epoch 00785: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2224 - acc: 0.9224\n",
      "Epoch 786/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9238\n",
      "Epoch 00786: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2193 - acc: 0.9236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 787/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9218\n",
      "Epoch 00787: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2240 - acc: 0.9217\n",
      "Epoch 788/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9196\n",
      "Epoch 00788: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2289 - acc: 0.9200\n",
      "Epoch 789/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2221 - acc: 0.9229\n",
      "Epoch 00789: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2229 - acc: 0.9227\n",
      "Epoch 790/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2257 - acc: 0.9215\n",
      "Epoch 00790: loss did not improve\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.2263 - acc: 0.9213\n",
      "Epoch 791/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9227\n",
      "Epoch 00791: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2201 - acc: 0.9228\n",
      "Epoch 792/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9218\n",
      "Epoch 00792: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2245 - acc: 0.9217\n",
      "Epoch 793/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9203\n",
      "Epoch 00793: loss did not improve\n",
      "100/100 [==============================] - 88s 883ms/step - loss: 0.2276 - acc: 0.9204\n",
      "Epoch 794/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9212\n",
      "Epoch 00794: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2256 - acc: 0.9212\n",
      "Epoch 795/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9201\n",
      "Epoch 00795: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2265 - acc: 0.9201\n",
      "Epoch 796/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9212\n",
      "Epoch 00796: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2210 - acc: 0.9212\n",
      "Epoch 797/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9195\n",
      "Epoch 00797: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2300 - acc: 0.9196\n",
      "Epoch 798/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9202\n",
      "Epoch 00798: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2283 - acc: 0.9201\n",
      "Epoch 799/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9217\n",
      "Epoch 00799: loss did not improve\n",
      "100/100 [==============================] - 87s 873ms/step - loss: 0.2238 - acc: 0.9218\n",
      "Epoch 800/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9233\n",
      "Epoch 00800: loss did not improve\n",
      "100/100 [==============================] - 87s 875ms/step - loss: 0.2193 - acc: 0.9233\n",
      "Epoch 801/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9199\n",
      "Epoch 00801: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2261 - acc: 0.9197\n",
      "Epoch 802/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9206\n",
      "Epoch 00802: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2244 - acc: 0.9208\n",
      "Epoch 803/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2177 - acc: 0.9234\n",
      "Epoch 00803: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2177 - acc: 0.9234\n",
      "Epoch 804/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9200\n",
      "Epoch 00804: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2268 - acc: 0.9199\n",
      "Epoch 805/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9245\n",
      "Epoch 00805: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2145 - acc: 0.9247\n",
      "Epoch 806/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9211\n",
      "Epoch 00806: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2254 - acc: 0.9211\n",
      "Epoch 807/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2266 - acc: 0.9208\n",
      "Epoch 00807: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2262 - acc: 0.9210\n",
      "Epoch 808/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9218\n",
      "Epoch 00808: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2234 - acc: 0.9219\n",
      "Epoch 809/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9231\n",
      "Epoch 00809: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2206 - acc: 0.9231\n",
      "Epoch 810/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9220\n",
      "Epoch 00810: loss did not improve\n",
      "100/100 [==============================] - 88s 875ms/step - loss: 0.2245 - acc: 0.9219\n",
      "Epoch 811/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2156 - acc: 0.9242\n",
      "Epoch 00811: loss did not improve\n",
      "100/100 [==============================] - 86s 865ms/step - loss: 0.2159 - acc: 0.9241\n",
      "Epoch 812/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9213\n",
      "Epoch 00812: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2264 - acc: 0.9213\n",
      "Epoch 813/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9209\n",
      "Epoch 00813: loss did not improve\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.2257 - acc: 0.9210\n",
      "Epoch 814/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2252 - acc: 0.9206\n",
      "Epoch 00814: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2256 - acc: 0.9206\n",
      "Epoch 815/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9204\n",
      "Epoch 00815: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2261 - acc: 0.9202\n",
      "Epoch 816/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9211\n",
      "Epoch 00816: loss did not improve\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.2243 - acc: 0.9212\n",
      "Epoch 817/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9225\n",
      "Epoch 00817: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2208 - acc: 0.9226\n",
      "Epoch 818/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9220\n",
      "Epoch 00818: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2241 - acc: 0.9219\n",
      "Epoch 819/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2326 - acc: 0.9189\n",
      "Epoch 00819: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2327 - acc: 0.9189\n",
      "Epoch 820/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9228\n",
      "Epoch 00820: loss did not improve\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.2197 - acc: 0.9230\n",
      "Epoch 821/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9221\n",
      "Epoch 00821: loss did not improve\n",
      "100/100 [==============================] - 88s 876ms/step - loss: 0.2215 - acc: 0.9222\n",
      "Epoch 822/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2261 - acc: 0.9204\n",
      "Epoch 00822: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2256 - acc: 0.9206\n",
      "Epoch 823/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9200\n",
      "Epoch 00823: loss did not improve\n",
      "100/100 [==============================] - 88s 883ms/step - loss: 0.2305 - acc: 0.9201\n",
      "Epoch 824/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9220\n",
      "Epoch 00824: loss did not improve\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.2230 - acc: 0.9216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 825/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2197 - acc: 0.9223\n",
      "Epoch 00825: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2198 - acc: 0.9223\n",
      "Epoch 826/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2228 - acc: 0.9216\n",
      "Epoch 00826: loss did not improve\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.2226 - acc: 0.9216\n",
      "Epoch 827/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9235\n",
      "Epoch 00827: loss did not improve\n",
      "100/100 [==============================] - 88s 881ms/step - loss: 0.2192 - acc: 0.9235\n",
      "Epoch 828/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2242 - acc: 0.9218\n",
      "Epoch 00828: loss did not improve\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.2241 - acc: 0.9218\n",
      "Epoch 829/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2159 - acc: 0.9246\n",
      "Epoch 00829: loss did not improve\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.2156 - acc: 0.9246\n",
      "Epoch 830/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9208\n",
      "Epoch 00830: loss did not improve\n",
      "100/100 [==============================] - 88s 877ms/step - loss: 0.2254 - acc: 0.9208\n",
      "Epoch 831/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9197\n",
      "Epoch 00831: loss did not improve\n",
      "100/100 [==============================] - 87s 872ms/step - loss: 0.2272 - acc: 0.9199\n",
      "Epoch 832/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2254 - acc: 0.9217\n",
      "Epoch 00832: loss did not improve\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.2250 - acc: 0.9219\n",
      "Epoch 833/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2205 - acc: 0.9225\n",
      "Epoch 00833: loss did not improve\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.2207 - acc: 0.9225\n",
      "Epoch 834/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2256 - acc: 0.9204\n",
      "Epoch 00834: loss did not improve\n",
      "100/100 [==============================] - 87s 870ms/step - loss: 0.2254 - acc: 0.9205\n",
      "Epoch 835/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2276 - acc: 0.9205\n",
      "Epoch 00835: loss did not improve\n",
      "100/100 [==============================] - 88s 880ms/step - loss: 0.2277 - acc: 0.9205\n",
      "Epoch 836/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9220\n",
      "Epoch 00836: loss did not improve\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.2215 - acc: 0.9220\n",
      "Epoch 837/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2162 - acc: 0.9240\n",
      "Epoch 00837: loss did not improve\n",
      "100/100 [==============================] - 89s 891ms/step - loss: 0.2163 - acc: 0.9239\n",
      "Epoch 838/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9206\n",
      "Epoch 00838: loss did not improve\n",
      "100/100 [==============================] - 87s 871ms/step - loss: 0.2260 - acc: 0.9207\n",
      "Epoch 839/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2259 - acc: 0.9209\n",
      "Epoch 00839: loss did not improve\n",
      "100/100 [==============================] - 87s 865ms/step - loss: 0.2258 - acc: 0.9209\n",
      "Epoch 840/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2219 - acc: 0.9223\n",
      "Epoch 00840: loss did not improve\n",
      "100/100 [==============================] - 87s 866ms/step - loss: 0.2219 - acc: 0.9222\n",
      "Epoch 841/1000\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9242\n",
      "Epoch 00841: loss did not improve\n",
      "100/100 [==============================] - 89s 885ms/step - loss: 0.2183 - acc: 0.9241\n",
      "Epoch 842/1000\n",
      " 32/100 [========>.....................] - ETA: 59s - loss: 0.2326 - acc: 0.9196 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6a7ba4804f28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheck2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logs1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistogram_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_grads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fitting model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheck2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/drc/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/drc/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/drc/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/drc/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/drc/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_checkpoint = ModelCheckpoint('net_depth_seg_new_small_aug.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "check2 = TensorBoard(log_dir='logs1', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=True, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "print('Fitting model...')\n",
    "model.fit_generator(train, nb_epoch=1000,steps_per_epoch=100, verbose=1, shuffle=True, callbacks=[model_checkpoint,check2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa24385e550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(img.ravel(), bins=256)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-336-e5d82995ad77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_Test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mY_Test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimgs_mask_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_Test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "test = train.next()\n",
    "X_Test = test[0][0:1]\n",
    "\n",
    "Y_Test = test[1][0:1]\n",
    "imgs_mask_test = autoencoder.predict_on_batch(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcc60926050>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAD8CAYAAADzEfagAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJztnXuQJdV93z+/7rmzy3sBURixKx6CSGDeIARCdlySbEuEsqRIVpBlm1KRomzLKbvkKgc5VU4cV1KWXSXJShw5lGUH24okjIxEqeRgPXCiFwhWwLILQiywa5aHELAsC+xj5vYvf/Tpe0/37b739sydubMz309V1+0+fbr71zNzv/M75/c755i7I4QQYjySaRsghBCHEhJNIYRogURTCCFaINEUQogWSDSFEKIFEk0hhGjBkoimmb3dzB4ys+1mdv1SPEMIIaaBTTpP08xS4IfAzwK7gLuA97v7AxN9kBBCTIGl8DQvBba7+6PufhD4HPDOJXiOEEIsOzNLcM+Tgcej413AG4ddMGvrfD1HLIEpQggxHnvZ/ay7nzCq3lKI5liY2XXAdQDrOZw32lunZYoQQvA1v3nnOPWWonn+BLApOt4Yykq4+w3ufom7X9Jh3RKYIYQQk2cpRPMu4EwzO83MZoGrgVuX4DlCCLHsTLx57u7zZvabwG1ACvylu2+b9HOEEGIaLEmfprt/BfjKUtxbCCGmiUYECSFECySaQgjRAommEEK0QKIphBAtkGgKIUQLJJpCCNECiaYQQrRAoimEEC2QaAohRAskmkII0QKJphBCtECiKYQQLZBoCiFECySaQgjRAommEEK0QKIphBAtkGgKIUQLJJpCCNECiaYQQrRAoimEEC2QaAohRAskmkII0QKJphBCtECiKYQQLZBoCiFECySaQgjRAommEEK0QKIphBAtkGgKIUQLJJpCCNECiaYQQrRAoimEEC2QaAohRAskmkII0QKJphBCtECiKYQQLRgpmmb2l2b2jJltjcqOM7OvmtnD4fPYUG5m9kkz225mW8zsoqU0XgghlptxPM3/Bby9UnY98HV3PxP4ejgGeAdwZtiuAz41GTOFEGJlMFI03f3/Ac9Xit8J3Bj2bwTeFZX/tefcAWwws5MmZawQQkybhfZpnujuT4X9p4ETw/7JwONRvV2hTAghVgWLDgS5uwPe9jozu87M7jazu+c4sFgzhBBiWVioaP6oaHaHz2dC+RPApqjexlA2gLvf4O6XuPslHdYt0AwhhFheFiqatwLXhP1rgC9F5b8aouiXAXuiZrwQQhzyzIyqYGafBX4GeJWZ7QL+I/BHwE1mdi2wE3hfqP4V4EpgO/AK8MElsFkIIabGSNF09/c3nHprTV0HPrRYo4QQYqWiEUFCCNECiaYQQrRAoimEEC2QaAohRAskmkII0QKJphBCtECiKYQQLZBoCiFECySaQgjRAommEEK0QKIphBAtGDn2XAixfHxsx3d7+x8+9fIpWiKakGgKsUL4kx138LLPkHneAPyDRzcDkFgGQIrze6ddOjX7RI6a50KsEOa8/HXsYnQx5jxlzlP2+wx/+NhdU7JOFMjTFGuSi+/JvbdXslkAUvLjxJwtF7VevWVZSNuvKiOWAHmaYs1xwT1wIJvhpW5/mZUuCV0SMjfO2bz8X4u4L1OsbCSaYk1xwT0wnxUCmTCfpRzMZtjX7eSf2Sz7sllee9f6aZtKipe2okxMFzXPxZohFsz5LAVyjxPyYMtL87O9IAzAid+d5eiZAzz8huVdLTWxrGRHjAJB00eiKVY9l903x1yW9gIqmSc9sczcyDDmuh0yt9J1mae8MHcYJ3znMBLL+NHlLy6ZjXEQqE4wiwi6mD5qnotVTzeIUOZG5gkZRifp9gSzG4llhpW2eU+Y96TR85sERX9mF2uss5TPF+3Qb0KsCbrRn3rXjQPdmdzDzFLmszQXRiwI6+A27wnHf/vYJbLNeoI5ShwVMJo+Ek2xqnnTfQd7+4VwFsLULUSxIpbzWcJ8lnCwOxNtKQezdOLC+bEd3yXzXDR7dgURjcW0/w7N3qhYHiSaYk2RBdGZC5HzQjCBSDgT5j0tNc8PZjMczGZ4ZX52osI55wkHSUqCWSUuq/a7iuVHoinWHPu6nQHBzL3LNAhmUvI457opc92UA/Mz7O/O8PyBw0lvf/Wi7fjoY3f2BLOLcZC0ly8ab0DJE/1vO7+96GeLhSPRFKua1DLSFpHnWDAzN7zi2Xkoy9wWJZx/suOOkmDGfa5ZCDwVW6k/thK4EsuPRFOsar553nqSEQnhuUAmvaZ7IZZVwSwi6r3jRQjn/pD6VAhmLJJ1xML526e+aUHPFJNBeZpiTZCS9caXx/T6M6NgUK1YRv2eBTNJRmLd1rb818e+x5ynHCQN9yx7kgN4QmIZXYxO66eJSSNPU6x6vnP+LIk5idV7nLFoDSS4R2LazZKeB9oNfZ375jutvc1CMGPPsi5SXhD3Z4rpo9+EWBPcdUGaC+eQpnpVMOc9DwR1w9YLDGUJXTfmsoSD3ZQXD6xn322njW3Lfu9ETfOwedK4FXRJOCqZa//yYqJINMWa4d4L8+GI+VYWz2rTvOi79KhZ7oATpyZZr7zrxsv/5/Sx7Kh6ld2KF5mRlLZuEFiAf3fKFQt4czFJJJpiTXFkOv7kG1lFMOOyar24zrjEnmQskk3M0r7/VEweiaZYU2y+MOHEdXtzb5PBfk4Lx9WE97gM+qlH8ZaN4W1+5JEtA2VZNaWosmUkpLgm7VghSDTFmqNjXY6bfYWZpBsCRBkzNU32ahR9oeVNFPmjSRTVr5svMyGjY/PM1kT/xfIj0RRrjs0XJqRkHD1zgBnLmE3zZm9TdD0uHUhHio7NfGTieWJZozAWVCcdnrUuHetyeDI/9N5ieZBoijXNYelcEM75XlqSVdKTjOa+zLheW08TGDpaKcV7XmaK09Gs7SsCJbeLNUsRSV8XPLi8/zJPH3fPxZMghIn5gHCmSVnwvCKiw0jxXkQ8tYyuJz1vM+7jTC2jY106lrFeoydXBPI0xZqmGD6ZWMb6dJ4Ns/tIzOmk3UYBtEp57KFOgrip3rEuKc5669IxqeZKYKRomtkmM7vdzB4ws21m9luh/Dgz+6qZPRw+jw3lZmafNLPtZrbFzC5a6pcQYqF0ki4JThpFzY/s5GlJh83MDYwkWoww/v6j3wf6S1fEfZvVZnqRgDRL7mV2cD6wSTmaK4FxPM154Hfc/WzgMuBDZnY2cD3wdXc/E/h6OAZ4B3Bm2K4DPjVxq4WYIDNBOAsxy9w4fOYgh88cZMO6faRJRjpEPIu0pNScI9/+aONz8gBPNlDW268IZ2oZ622eDmqaryRGiqa7P+Xu3w/7e4EHgZOBdwI3hmo3Au8K++8E/tpz7gA2mNlJE7dciAlTeJuxOK6fmePEw/eSWO6NFuJZ9UBTc44YIphV4pzLqnAW4rneck931jI1zVcQrQJBZnYqcCFwJ3Ciuz8VTj0NnBj2TwYejy7bFcqeisows+vIPVHWc3hLs4VYPIk5ifcDMgnOunSefd3OQH/mup/bAcCBfzy1NAKorViWn9+8VG8vABTCQqmWuVgxjC2aZnYk8AXgt939RbN4dIS7tezscfcbgBsAjrbjlEshlo18/fPm89VJPWIBLcRzoeReZVaaH7Mor85yNEveLdAxSJbB0/zdR+4veb2FN/xfTr9gyZ89TT740E7+6nWnjF1/LNE0sw65YH7G3f8+FP/IzE5y96dC8/uZUP4EsCm6fGMoE2JFEacQxSlFxSihgqVc7zz2NqvC2QmJ8CnQCXNvTpoPPfzD/NmWsd87ZdH0/GfwO9u39boM/vi15y6JHZPiqm27gTjY1v89lrpTQnnx3u//wZN87XXjPWOkaFruUn4aeNDdPxaduhW4Bvij8PmlqPw3zexzwBuBPVEzXogVT6m/cUJpRH/42F0AkUD2vc2qcPbswEmAjhnv3njpROwA+NWH+r1ne7PDKsLS3NcK8OsPbw+2ZaVznzzj9ROzD+AN99ZPTjKYZeCl8r3d9aU6dZkKw8rHYRxP8wrgV4D7zezeUPZ75GJ5k5ldC+wE3hfOfQW4EtgOvAJ8sLVVQiwDuXdZf25SYtm7X5zMHvabhDO2oWPQscmkU7/nwbwx+HK2riSUJZHxhn8YXvXaonruXPfDR0v3+tSZZwy15azNfemp8wYPZDMDZZBP4BxTd20aJmOpfb+KyFbfZRxGiqa7fwsae6HfWlPfgQ+1skKIZeKy++bYn/UXjcjHgltJPGOxePKyvUtqT1U4y+fypvkkgkBXbdvNgfDeiWXMheZ+SlYSomSIULYR1w8+tLP3rE//i/IEzWdtnolmjkqYr+l6SKy+Wd17ZrBrvvezc/D8M36flPJELL1JUrw51WsUGkYp1iSJOd3wvZmxLkniHMhmaseYT/a5uUdZ9jwHA0OQ5wOmZiQTGLhXPDefJT4tebVVrzEun4S4fuAHu0J5fs1dL5VFtJjwuev9ZT0G/oEU6WDBU4yFNrGsN6tKtd9ynqRUVtevOfA+I5BoijVLSgaWhIk3Mg5LD5KYs2fusCV6XjnQM0w4UzJSI/RpTi4IVLUhFtFcJKO6XhaSSYtrTDw7VJ7SVX7neH6nsghmUPQHF95mRVjrvNZCUHtlLXpjJJpizVP1Pl41+xJbLppcn+af7LgjX5ityAdtiJjHgrLe5oNgJlx18sWLev67H/jxgCdb9Xh7nmhvJvlBj7uaijWsX7Ap8PL4/uPC8eAEKFlYc754fjdqwldnlKp7blpZA6qXBeFJ31P1UGb9gFzTvZuQaIo1RXU9ntjDK0SrzReoLVWRisti8nSjySa113UNjKJJRBMqfYeVYEvJY7UMKv2Mdd0RdcTdCPNxl0JPHOub8zHVpn1sR35tu9+3RFOsOYpmOeRNwYX2bU3Wpvy5hZh1yCbWn9nKjjBNXRPFubkopzU/TnvXFxQiVXeuiToPdPQ1g/et/h4LW6p1i3ryNIVoILWs1F9WTWqHfNXKJXl2L9Vo0NuM6wDMWkaCkU5oJNAw767OngQnw0aKaDUFqBq5LliXzPfWZGojinU/o74A9gchpDXBnibvMzFf1D9Kzacp1hTdUtQ4/vK19zja0HTfxi92+JyZ8EigOKm7TX5iXX5jTDEefz5aH/5ANkM3il6P8jTHsScWzIK65ncspqO8zLZINMWaIg0LqHWsvy5QvDVFdxf1zDjROlr7J42+zPEXurCtYwlXnry46Wjf8+AzpYjxWDb2FnxrFpXqPasCVIhlngWQTz7y/PwRrWyvDjCoCmZi2YBgNnuffTvj3M1xfjZVJJpizfAvt+zr7Vf7MlOygUToSdP/gg6KaH4+F8+O5elGk6BODMopOO29zaF1IhHq/TPCR3qZ447Aqmt6j+991k/E0vYfpURTrBniCHD8hYm9pbajQ8YlrfliV88VdIrJJCJ7f+GB53qTUSwldSJaNznyuBTzg7Ydqlgn0FXBrKYYxefieySWjRRIBYKEaCDBoVjIrPJFKZqR0Km/eIIMTXS3fKKOInIe51n+/NZ8xqUuxtfOOWoBz41TrIbZUCz25rU5m9APojUFmVKyMOmID/xjGEZdsKhOMOvO152L7SnqLtTLBImmWEMkOMXcOdWocDHJwzfPWz/RZ35ix3caRadOtOLPgjmfGRjFA/CW+1+m6wlznvKd82cH7v/+HzxJ15PSyKeCUqR8yGTIVcYR0fgThnumRfJ50z3rqOvHLO41eK7c5VI3jLJtC0OiKdYEb9u6l7kwc06cTlMwTn/dQiiG9mUh1QjoTX+Te2nl/Mw0TAf32RfP79n9StYXxFjoY6F5030HwyiaXBw3X7jwnrdeEnz0j2VU6lGVmaQfAEos49m5IxdsT2zXMMHs1auJlleDP4sZyCDRFKuan9/6Il1sqCc1Kp1mEtR5aHEzNMW57aVzeueaxLEqXE0e2gX35J+x2NU1p9uMDopJoy6O+J6lvNcQAGpqmtc3w+u93jpRrJ6rSy8alpPZNJXcKCSaYtUSC2ZVbGoDDTg/v/VFbjvn6CW1KxaqxJwvv3j+2MI4cG5Iovi2V04mc+Psw58cadOwIZZVwR/WRC8oUo2q6VQLYVg/5qj0orhurZ0LsE2iKVYldYI56oueYcx5ylXbdvPlnzy2dO7dD/yY/d7hH35yQys7Uhwsn8UnFptb9pTzL6u2lTxNj/szhzeR6wIoD7zyauY85Zwjnqica+7LXEwTvZRqhPPs3GDAqs348/zZ9c3yunPx+aaczJKgtuyakWiKVUEhknFTsBDM/nyNo7+kXU/Y781fi7dt3Tt21PrPd36rNjZ7856Le/2PfVuHC2OTRzlqSGJ8ry0v5Ut3nXPEriVpovfKg5fZ1otLzUtTxMXlMH7gJ7ajoC5aHjfLFQgSa445T/NIclTWC4zUNG9rvySe0AWyzPipLfvpWJdvnNsfxZJWZu9pQyEIhbdZBGwK4SqGIfbsHOKFjTN2e1gf7paXN/Um/D3vyF298qbp4qAcPGv65xP3ayZhmGYxuqk4n5HW9mU20TbwUzdSqa5soYKZP0+IVcC6ZC6fjAPrNbMLwex6wlyW5pvn2/6sw/6s0y/PUg5kM71zxWeRFwkMNNlH8WunvDmsWV58OZ0vvHhRo2B2SeiSpxDl47jT2i2fX7J+m/eUeU97P4fqNudJyZu7d+8m7t27qVUeJQwGz6pLShQBoGr/YuMY/GFDNhsEs2lceV1kvClaPs6Q0SryNMWqYM5TEsvY3x3MsywEqc67yUj7U5eFL9ZMkvXqznm78dJVfu2UN/M/dn6rVBbPtFQ8J7av8O7GyV2sa87GjJt/+f29r6HrxgVH7Ro41ybdKE5oTyzjmbnFBdWGRcoH6tbOdDQ6Wq4+TbFmiWf+jikEKfbuyteVZ+4+kM30J/TAufie2Itpn9f3G6e8ubF/s3h+/A5Qnrk8Lh9Fm2nX6kT5+y++hsyNS47ZuaAmetH0jVON2uZ49u5VNws7dU320f2YdYn26tMUa5qvnXMUb7n/ZY5MDwC551ksAxsL5rCAStZbUyaflbxYCiGzdssh1PFrp7wZKE8aUn1+b58aER3lUU7AKy1IzLl7zylkGBcfs7O5Xo2I9pvrId1ojPSkcewpaBLMqkdZO8Y/aopXxbMNEk2xKnjfg08z5yk/mjsGyEWk98Vxov7D5hSbgsLz7HmlnvHwvhO5atvzA5PuLpZy32Yc6bcBz7nqfRbXjPuckXWK3NHIw7vrhVO5+Jh/HnssekI+7V6Ks7vlVHBN98w/myPl1RE+TYGf6rvF4tkml1SiKVYVVVGb1JK8xcJoi03UBnozmHcbnNc6j7C60FhhU6O9i/Du2lwbi2g8o9FEfk5DIuXVOtX9uKwqmEWzPBbMVhOKjF1TiEOAV3X2cnhykPXJHB3r0kn6KS/zWbOXGEeg8+N+ClDcD9o2ub1K88zn1ZE+w1dmjM9Vt/mozrCtmGG9Wh6TmLN5z2tGvkchREVC+7giFPdVVsvzz/pp4Nr0Y0LzUNm2ggnyNMUqIokmx4jTjYpyqF9VcRgDTfUlsLkb2TbQ/K4RzKI8tjH+bKLufDVnsjpDUcw4TfSFNHebaB4+OV6zvLBnID0qeJkLn4xZiFXATWf9RO/L37FunpOYpdGEGA0eXvDOSmWRx5kf5+eLNbuXmp43WRMQgkHBbPYkm/M7q9cP2tCuiQ6UmubjzFPZbomJ4YGf6j2r0701CeZCsiHkaYpVQ+ENFlHbuH8z9hRjb65T2zQsl+Uik4Vlfxc/+UQvCT+K2DdRBIRyO/pCWvUus6geNPdLFuITz6VZXN8kREUTetRY9CIAlK8HNHwquGGjgkalFg0bV17s10XKB6Lm0X3VpynWJLeefXxPDIovRzECqGDctJs6JhVUqhJ7XHETvI1gluwkFtJyX2WdZ11lIelVRTL7uF5mE6PWCqprgo8zEUdT4GchfZoSTbHq6HtRXmqqtrm+LjADcN73JyucTQKV0U4wq6OI2ja5ayPPIbBz6TE7as/FFMnsKfU5kvEzxhHlYTOwt+3HbAogVcVzXCSaYlVx2zlH18w5mZQ8rFKUvCb6XLqu1P+Xe4GTEM5kiLhA4WVaSTCbGDdXs9GWSMySiliV6xXN5vq+wqJOnac5TgJ5X+hGj/hp049Z7FfXfE9H2NyE+jTFqqRp5vOqwFSb66kNTrhbqk8CnsECAghQLGfRX6uo95xoPPq+bmfA7tz24c3yUVQFsex1LXwsNvRFKSXj+e4RpfH1CxkVNE4+ZmMfbE2zvCnws5DuBHmaYtXxf887LB/eF74MXbfeVqVOREtjvmv6ABeTflQ3K3xizrMHjuT5g4ezZ249857U5lrm9k5OMHuTBceeJmWvs9/8bU40L0QpXw9occNN4/vGduY2NE8o3EusZ3C/6mEWXmYsmG3slmiKVckd53d6czvGVPuu6tf5rlyzQK+yiU4yH01qkfH4vmNzoaxJNB+WeD6Mpj7KpnrjLA3R+KxeQvtor616vno8LIG9zQzsdZHyweZ5/TpCo1DzXKxaejmaVp4RvPRlHFNc4usWEx0GejO/v+m+g2x/+YSR3mM7L6jfFG4OyNQvaxv3s/ZTkRomHK6kHhX37Nj8+LaOkXY0+E+vfeAnbpIvVjBBoinWCIU4Vpvo3UhYCzKseR7HRQpmwVHffBX37RnPcyyEpSp0jYJT04c4qh+zVLcSYR41o3shTCkZz86Xux/GX1docL2iJo8yPj9sqre6SHn1/YvfZ5uZjka+jZmtN7Pvmdl9ZrbNzP4glJ9mZnea2XYz+7yZzYbydeF4ezh/6tjWCDFBtlw0elmF1Hzs5njxxd168eKE89hvH8ds0i2N0KnbqozyykYxbDRNNZo/Kl+yXy/qH6x2fQybjX3I/evSi+JrRi2MNixSHl8fC2bSanTSaA4Ab3H384ELgLeb2WXAR4GPu/sZwG7g2lD/WmB3KP94qCfEVHjokjkeecP+Xs5hx7KeUNaJZZ2HWQQ5Hrx4ftGC2XuOOUfP7uPo2X2NkeVRqURl8at4VuFd4lnU42uq6UV1U7DV/YxK4hMNm7z17OP57OtfPdb7N71H9R2a8jF7+0MCP/2c0b6oF83yQjD712Vj/5OAMZrn7u7AS+GwEzYH3gL8Uii/EfhPwKeAd4Z9gJuB/25mFu4jxFR4+A0HSsdn3rWuthleUG3GTUIsmzh+3cs8dyCfe7KtdzY44cawZXlrgj7V/r9ISDOMJy/by5PkMzvF6yUVpPiiZ36qs3NcwYzfo2kSjrhrJb4+vy78E510IMjMUmAzcAbwZ8AjwAvuXvT67gJODvsnA48DuPu8me0BjgeerdzzOuA6gPUcPrbBQkyCqogCvO7uPD/yoUsWuuZkOzJPSCxrPbSz+OLHfZ3DhLOuH3MwvWgwlagq4HG61Nu27iXzhBu3v5Gf4MFSnbdt3dvqfererbofHw8L/MBgpDz/rPFgg2D+9es2hTN3jWXjWKLp7l3gAjPbANwCvH6suw+/5w3ADQBH23HyQsXUWS6xXAhNqUHVIFH/uJpa5Y33KBi3ifrkLWfzN9u916nw9BfPKp3/2+3wy2d8b+RSv1VG5WPmNg4P/IwTKb/prJ8Y6z2baBU9d/cXzOx24HJgg5nNBG9zI/BEqPYEsAnYZWYzwDHAc4uyUohVRjGrUOERbph9hRcPHtY737T07Wgvc3jwq26YZK2XOeQ+7oaZD3QkxLX/dvulpfPvPm1LaZRQk13DAj9Ny+2OipTfcvYJtc9cKCNF08xOAOaCYB4G/Cx5cOd24L3A54BrgC+FS24Nx98N57+h/kwh+jx3xW6O/fZxuXAGAXhpLhfMWCyLbfcVz/PqO46iG088UqNndalJBXXjyuNmeTEz+jhJ7llm1MWQrVI/Pr7lsfN6+x6J8c+95ge9/VH9mIXtdXNjFrmvy4GN0jMzO4880JOS/6Rucvf/bGankwvmccA9wC+7+wEzWw/8DXAh8Dxwtbs/OuwZR9tx/kZ766JfRohDjWO+dTyJOQe7KYk5e3/q2aH1T/zu0QOTFA9MDdcwQ3s1Wl63lERcvuPS+pUzAR6/+RygLIzW4OBWxXQYbz/lwZ4dTYGf1DK+ed7g+vaL5Wt+82Z3v2RUvZGiuRxINIUYn1ffcVRvVvq6ZS+q+8O8zFgwZ5IuHct6S2w0ieaTt5zNgQMzpGksmPX7+XHzu1TrekXw33V63qy/4/xO800mxLiiqRFBQhxiPHlZHp0+8btH58tYMNi/OWzseVUwd1320kDdYRTC1p1PSNKsV1aIY3G+EMTCL6vzOOPr6vjio6FZ//f5fTa+Z1srW5cCiaYQhyg/ujzPmzz+28fWCmdBtZ+yEMxCfBeCWS5iWTfB3UjSbEAc++JJ5bjqXQ5vwseiuusLP9nbn5aASjSFOMR57ordPa8TqA0SzVjGj9/0wkSe1+vRM4dispFuP6admOfnimqRPWY+0ATPywfL+iJcvr7g8ZvP6d3rNb94f/sXWSASTSFWAYXXecJ3NvSWMYa8Sf7cFbsn9pwnbzkb78Yqludrulv+SehPrfSpFnhJAPv77oMiOdi8LyL3g9fuvOlcTnnf8ginRFOIVcSkvMkm3K3BU/SScMYMiGhS9IMWF4cdNzAPzfXBoFDcjM9F1kpdAY997jxOu3rLot9xFBJNIcREKaSuJJ6RMGZZP8fTHZIkqlkkznvNtQwKaakvYolWC60i0RRCjMVTXzwLry5uNIQ6r7PkVVJucveqJP3r47qE+8W1ncHA01Kj5S6EEGNRNM3rAjT58WAE3KiNS+XiGeqb9TcAzwzPjCwzsm5/c8/L3Q2H3paFZUIceOQzF07kXYchT1MIMRaNnlwURW/y+mq9zsq1+fXFs6ILyS/2rH/OPcEGmvWwHGN1JJpCiJE8/cWzIBu/CVyXWjSucObX558l8fT4vONZ1dV1zviVe8aybzFINIUQI3FGeHE1wldbjfGFEyh7j5XOzFJ3gMEZv7z0ggnq0xRCLBFNo3wapbWSFN+/T8NNom3YUMxJI9EUQowky5KG/Mz4oE7wvDFA1Mi4whlbN6thAAAIO0lEQVSde+0HlsfLBImmEGIE1ZnZYchY8SbvcomEczk9zAKJphBisiyxcPaHWeafy+llgkRTCDGCUUGg+j7H8XN/2gpn4zOXCYmmEGIo46QZjSuck+rjHOvcEiHRFEIMpUk0x5qhvTEi3mL0UHGfmrLX/tK9TVcsGRJNIUQjP771dQNloyLZ9SfGE04YIyWpQYiXC4mmEKKRUU3zWo+xKW+yhXCuZCSaQohGhq2hXtBK+Fo01UfeavynThSJphCikXHlcBIe50AfKc3CaMDpU+jPBImmEGJCNPZRtkhJGsfrnGK2ESDRFEIMYWCmomgOzDqWUjhjz3OafaESTSHERJmEcI7itPfft6DrJoFEUwjRyKvf/UDjueGpR4sTzuHroE834i7RFEIM5eR/va10HItWY7CHIaN/FiGcZj5VLxMkmkKIMdj4nm1seu/WxvNtvc62UfUmAZ4GEk0hxNhseu/WRvFcKuFsdX4Z0HIXQojWvOYX7y8d//PfnQtUlqcYg9r6TUtnmHPa1VtaWjp5JJpCiEUTi+g//925tcJZt9jaoYhEUwgxUapeaMHOm86tXeK30dusFq0QvZVoCiGWhVPe1xfTnTedOyCcMLxpf+q/mX7THCSaQogpEAtowY7Pn9dYf6V4mSDRFEKsEFaKJzkKpRwJIUQLxhZNM0vN7B4z+3I4Ps3M7jSz7Wb2eTObDeXrwvH2cP7UpTFdCCGWnzae5m8BD0bHHwU+7u5nALuBa0P5tcDuUP7xUE8IIVYFY4mmmW0E/hXwF+HYgLcAN4cqNwLvCvvvDMeE828N9YUQ4pBnXE/zE8DvAlk4Ph54wd3nw/Eu4OSwfzLwOEA4vyfUF0KIQ56RomlmVwHPuPvmST7YzK4zs7vN7O45Dkzy1kIIsWSMk3J0BfALZnYlsB44GvhTYIOZzQRvciPwRKj/BLAJ2GVmM8AxwHPVm7r7DcANAEfbcdMfhS+EEGMw0tN094+4+0Z3PxW4GviGu38AuB14b6h2DfClsH9rOCac/4Z7myH8QgixcllMnua/Bz5sZtvJ+yw/Hco/DRwfyj8MXL84E4UQYuXQakSQu/8T8E9h/1Hg0po6+4FfnIBtQgix4tCIICGEaIFEUwghWiDRFEKIFkg0hRCiBRJNIYRogURTCCFaINEUQogWSDSFEKIFEk0hhGiBRFMIIVog0RRCiBZINIUQogUSTSGEaIFEUwghWiDRFEKIFkg0hRCiBRJNIYRogURTCCFaINEUQogWSDSFEKIFEk0hhGiBRFMIIVog0RRCiBZINIUQogUSTSGEaIFEUwghWiDRFEKIFkg0hRCiBRJNIYRogURTCCFaINEUQogWmLtP2wbMbC/w0LTtWACvAp6dthEtkc3Lx6Fo91q2+RR3P2FUpZkJPGgSPOTul0zbiLaY2d2Hmt2yefk4FO2WzaNR81wIIVog0RRCiBasFNG8YdoGLJBD0W7ZvHwcinbL5hGsiECQEEIcKqwUT1MIIQ4Jpi6aZvZ2M3vIzLab2fXTtqfAzP7SzJ4xs61R2XFm9lUzezh8HhvKzcw+Gd5hi5ldNCWbN5nZ7Wb2gJltM7PfOkTsXm9m3zOz+4LdfxDKTzOzO4N9nzez2VC+LhxvD+dPnYbdwZbUzO4xsy8fCjab2Q4zu9/M7jWzu0PZSv/72GBmN5vZD8zsQTO7fKo2u/vUNiAFHgFOB2aB+4Czp2lTZNtPAxcBW6OyPwauD/vXAx8N+1cC/wAYcBlw55RsPgm4KOwfBfwQOPsQsNuAI8N+B7gz2HMTcHUo/3Pg18P+bwB/HvavBj4/xb+TDwP/G/hyOF7RNgM7gFdVylb638eNwL8N+7PAhmnaPJU/tOiHcTlwW3T8EeAj07SpYt+pFdF8CDgp7J9Enl8K8D+B99fVm7L9XwJ+9lCyGzgc+D7wRvKE5Znq3wpwG3B52J8J9WwKtm4Evg68Bfhy+KKudJvrRHPF/n0AxwCPVX9W07R52s3zk4HHo+NdoWylcqK7PxX2nwZODPsr7j1C8+9Ccq9txdsdmrn3As8AXyVvgbzg7vM1tvXsDuf3AMcvr8UAfAL4XSALx8ez8m124B/NbLOZXRfKVvLfx2nAj4G/Ct0gf2FmRzBFm6ctmocsnv8bW5GpB2Z2JPAF4Lfd/cX43Eq129277n4Bufd2KfD6KZs0FDO7CnjG3TdP25aWvNndLwLeAXzIzH46PrkC/z5myLvJPuXuFwIvkzfHeyy3zdMWzSeATdHxxlC2UvmRmZ0EED6fCeUr5j3MrEMumJ9x978PxSve7gJ3fwG4nbxpu8HMiqG+sW09u8P5Y4DnltnUK4BfMLMdwOfIm+h/ysq2GXd/Inw+A9xC/g9qJf997AJ2ufud4fhmchGdms3TFs27gDNDxHGWvIP81inbNIxbgWvC/jXkfYZF+a+GyN1lwJ6o6bBsmJkBnwYedPePRadWut0nmNmGsH8YeT/sg+Ti+d5QrWp38T7vBb4RvI1lw90/4u4b3f1U8r/bb7j7B1jBNpvZEWZ2VLEP/BywlRX89+HuTwOPm9nrQtFbgQemavNyduo2dPReSR7lfQT4D9O2J7Lrs8BTwBz5f7tryfugvg48DHwNOC7UNeDPwjvcD1wyJZvfTN5M2QLcG7YrDwG7zwPuCXZvBX4/lJ8OfA/YDvwdsC6Urw/H28P506f8t/Iz9KPnK9bmYNt9YdtWfN8Ogb+PC4C7w9/HF4Fjp2mzRgQJIUQLpt08F0KIQwqJphBCtECiKYQQLZBoCiFECySaQgjRAommEEK0QKIphBAtkGgKIUQL/j+bbN/YVKMLXgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcc293a8750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcc293a8850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = \"/media/drc/DATA/chris_labelfusion/CORL2017/object_database/\"\n",
    "d_dir = os.listdir(path+\"depth/\")\n",
    "gt_dir = os.listdir(path+\"gtdepth/\")\n",
    "choice = np.random.randint(len(d_dir))\n",
    "g= misc.imread(path+\"gtdepth/\"+gt_dir[choice])\n",
    "d= misc.imread(path+\"depth/\"+d_dir[choice])\n",
    "plt.imshow(g)\n",
    "plt.figure()\n",
    "plt.imshow(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1081"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-c5dacb432bd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m plt.step(recall, precision, color='b', alpha=0.2,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_set(test,model,size=1):\n",
    "    data = np.zeros((1,480,640,1))\n",
    "    pred = []\n",
    "    targ = []\n",
    "    prob = []\n",
    "    for i in test[:size]:\n",
    "        r = normalize(misc.imread(i[0]))\n",
    "        r = np.reshape(r,(480,640,1))\n",
    "        data[0] = r\n",
    "        d = crop(hot_vectorize(misc.imread(i[1])))\n",
    "        p = model.predict_on_batch(data)\n",
    "        p = crop(p[0,:,:,0]).flatten()\n",
    "        prob.extend(p)\n",
    "        pred.extend(list((np.round(p).astype(int))))\n",
    "        targ.extend(list(d.flatten().astype(int)))\n",
    "    roc = roc_curve(targ,prob)\n",
    "    plt.figure()\n",
    "    plt.title(\"ROC curve for NDP prediction\")\n",
    "    plt.plot(roc[0],roc[1])\n",
    "    plt.xlabel(\"false positive rate\")\n",
    "    plt.ylabel(\"true positive rate\")\n",
    "    return (f1_score(targ,pred),roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  import sys\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.627944416200644,\n",
       " (array([0.        , 0.        , 0.        , ..., 0.99401588, 0.99402977,\n",
       "         1.        ]),\n",
       "  array([6.26880642e-05, 1.24749248e-02, 1.26003009e-02, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00]),\n",
       "  array([9.9514741e-01, 9.9110246e-01, 9.9107790e-01, ..., 6.2720223e-06,\n",
       "         6.2700069e-06, 1.7953365e-06], dtype=float32)))"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa23afa45d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_on_set(test,autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcc27efb290>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcc29437210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "gt = g/3000.\n",
    "pred = autoencoder.predict_on_batch(np.reshape(gt,(1,480,640,1)))\n",
    "plt.imshow(np.reshape(pred,(480,640)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa240551f10>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAD8CAYAAADzEfagAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJztvXusZMl5H/ar07f7zp2ZOzuc3eVyXwopg4ajGIpMKJKNCAYTwY5EGNgYEAgqQCw5QjZIRCQGHEC0DcRKDANMECtQ4EDOGhIsGrYoxbYgwqAjy7IEIUD0sqy3LHktkxHJJXdndud5Z6b79qn8cU6dU1Xn+6q+qlOn+9zZ/gEzt7seX1WfPufX36uqlNYaBxxwwAEHyFDtewIHHHDAARcJB9I84IADDkjAgTQPOOCAAxJwIM0DDjjggAQcSPOAAw44IAEH0jzggAMOSMBkpKmU+hal1O8qpV5XSn1iqnEOOOCAA3YJNUWeplJqAeD3APwpAF8A8EsAvl1r/dvFBzvggAMO2CGm0jS/AcDrWuvf11qvAXwawCsTjXXAAQccsDMcTST3RQB/YL3/AoBv5BqvqhN9sjgFYGm9QQWYqZxqcVNEGy83bIKkw0KuA0JQkzXOl6LKjCMfOG28u9ubN7XWz8baTUWaUSilXgXwKgBcqq7iT9z4NkDXPUHVHivomhbkt7O7hMiuZuQ1HfP6Scbt5AhZj/vcZNNMJk0YQybvXcLopUlA5Rl+qkqYh3QMgUwV+/xVZKxQ/0Df4LjcvLnPbbX/yZuvfZ4X3GMq0vwigJet9y+1ZR201q8BeA0Anlq+d/iUVcolFvtD2w+5f5GsPvbFHRCZ/6XYZOh/KXZf6sv0iNT/UkkSDczbFUaMx5Ac9/BEyVTyIKUQ61gy2RXpTqX5sOON84YlkWPqeDMmyeDYoXkLiDIHU5HmLwH4oFLqA2jI8mMA/jNRT6X6h8b+cBICDfSJEpn9pfnapN2XJMAAARNj0+MzXyRFeNzNkEimTRchQcUewJLa6q7JrARGEmIvJuOzFyZHQECQwDiSjPTP0iaBNKLMvM8mIU2t9blS6uMAfhLAAsAPaa1/K9yJ0PQcDY/RzPyLxGmhHjkESTRXC5X0l4zvyCK+2AJaad8ljVBVpWiiHUMapd0DOShEerKhRvwoTECQQCGSbARl9Y9rsflmNzGYXA6ByXyaWuvPAvisuAM1aQX+gVooxoxb0KSyCMgCoEIuztjNEvNzCm42JfCVAi25pjx0dSYZ6Jp9uEc99CDIeIeE1Q/JkL9VP+Hg+X0T5yUiQ2A8IQpk7ER7bAYqJ4vA3gJBA+RokpzZPFYrtfsg0bTv+ge0U3++nAxfDjGX4LyA8M0QMs2lD3aGlighJJ/QpiCxSYixxA/AVMQIyMixETpazs5IshmsnKwI5kGaqrnAmiM+iT8z5HeUyPLljSFRIG6iS8x8iRxHpMB36sjOJFRn0AySEBDtpJqeFFNpwCM+WxJBAhebJKP9yhBl6jWdB2m2YKPdEs2RI9BGWFxWSJ7fL+IfbYYsoI02guJyKHmBufWiMwnVGdMK2pUgWo5Q92DCZ6EA2U9GjI3wIvJkPtAdaZOcvICs5GtsYT6k6WmK4UBNWyfVGqUkGpJnyxQEZ4poo40gYh5CrTQkF/EbR5Rval+LVKIlJ1WIHFNInOs/EbIf2BRybAYqJjc7UAPEv9Md+CbJ+c8pep4Ob/JE9JwkoVytkSPRWO6kVBMl+mZro0DctO8F0uWJZNoPI/A9EmMOXC2D+Qhu1jpgHUixBxN/jAbTYY/k2IgbqUUCeSZ3bNwEogx+Bqou4ZrPhDRBa10BDZE05aUBICAvoJQiM0MbbYYXaKSAXCtthNLlY7MCwN+cuYTbITFdpuTGM0WIL4RUUjSYIOCzN4LsJ1BMJvtZRpKkj/mQpq9dUsnmDImyprBUEwV6wouZwzkyub5+f4wkUiCNTBvhfJ30xhKmS/koSU6TE50EIx7EDhP5M+WpR4J2pQM3mTKTSBKgr1XGfTMf0jQgE9sZ3x+jIY4m0ZA5L1nJI9VGY/274ekvNolMgXRCbQYJ18fGlCCVdKsqm6ijcqdGDrknzmsWBNlMJE92TpQ7VZu88IEghbSHN0ELbapGROW7jkJNNCQ3Jpvr78swolLIFEgn1GYQvs9wYHlbGzlktQuCk2IffkwkatdjCVIio7Am2YgsoE1y7S92IAgE0Xj1lAYKAItF0q5EipMDAIuQb4xZaQT0cazk1UuWbCAc7V1ExnCmQ2QXxLBoBkj2DdrXd7Ggy59EFCLtLLdC6V2NRIQ6QXQcBUmR68PKueiaZpPd3r/lIuOVcsnUPOAh7Y/biEPoH22qMqLrQDjtyZ8nJZ8aJ2WzjpRNQIx46Zr4bgzOxJpYE5SQ8p610dF+1rmRo3SsUuZ1Jy/BF5lCkhde01SKJkGfsOwczVD+JSkjEIUuYd6beXUdBaa4ZGWQxFxPSRhPSPmRPvg68KNj1xfDjgmxeKBpJrsZZfsdBeNkkWSOH5L0Zya0vcgrgkiik+RU1hGNM5jcLtRE237R6PZYIpWmDZXYSs4fOyQ/YurHSKUk6aQQ8M6j6mNzQ1OT+0uRo2TsyFjJpnYnt5AfMkWbfOICQSGiC2mhNigijZnzqUsdJSZ9bH6AzOQeS6bUuKGx/XlMlSiesWpn+jzKieTnrHZKnUsJchSMW9TUjvUZq0ly5RfdPNftRVC+2R3SOIO+zARNlJMP8OlOEXmjiRTIM++JuTjI2aRDuk+nqtJ3PcohqBSinXpV0B52NUp62OdIkKF+U2uSvs8+4/6YDWka2B9iQKBdI6EmaspTNVFODpCtjTZVGURqzxVIM7VLEqo/H24eJTfWkLoOUrGrzT/GzLM0MQKi+US1+NIkydVNqEmSRJlwvedDmgTZ+B9OpZjcIXMeoH12OZookBZg8vqLAidS/2UpQrXmR2Jq7dBgEUjz2jVKaqw5ZuGuz/vJ9UPG+uekAKWY1pRiUjByDsyGNFXYp9k+wCSJpmicUnMe4E36kF/UkZexFVyKVgpMGwxKualSd0NKwRz21ZRirK91oiDQpASZbboX0CSZctbk9tteaJ+mAoIpRwzBiEiUk5ESnTeIaaMheUAekVoyxHtjThEMAnhzuURgJka89v0xBaYMLuW6AxI3LRE0EoyZqUGG6nLWo09lbj9RKUeVAuwt4qQ+R6tML5i2rVxVM+QWy+80WAzHtOU785bIA9xVNBJSkOyNGZOzcG8SeQrPgi4eY0Lby1Nj5HIRlM4RmnFyVkAJEpTIydEYQ/0SyychxYu/IghDcpH4HBNTk6iLn2Tic2PafaRBJ78fV++3kawfT/RX5mzj5mzHlqrZOoJmtIYcmMwdkJUmJe1T6kiLWJuSBMnUBaPZUkKUkuSFNs8BV3OktKZcEuVAEKmqG5OfPJkyJeAUm7sBZ+5LZPttAPmGHJJrJCBW8c7vF8kvKcDoPNGc/rsiRmDkxhzp5CnWJFNkT+jXnAdpKgWtFBRFmBwZSUm0lR8sM1WV+SvwlUoCQtFgUUArjclOaQPEHzrp7vCSsToRaRrsLPbEHIMx859it3aR+Z5BgBLZYwI0sbqxmqRXri+sTxPu5AcE2jWaQBONaJHigFNIlrROlKs5gkyptoD8oc3Zk5Mar+taiCjHBIqmJOtd7NI+lhxjMjLrkklyh5rkgCgTvqb5kOZggyDCHCytiXIyIuTG+kYDfbLq7DaSlTwpRJlLdLkksIut4uZIfhSmCPgA44hR0qYkQXJ1XPsRJjypSY74OudDmgPm9x7YevjhycvLkaA0hShmVpciUslYnaAMM9+GlFR9uakPdzAhfmYBn5IYQ9YpfYtuzpFvehfTILm6kTsUiUhyxHc2T9KkNEr/Q1MkmhoIKrUqKIFIgRFaqV0fauO3k5BqpfLJVTKfsUg1vX1zfZf+0tyxkvbPLESgOdpjTG4qeY4MBIk1yQJBIGAmpKlV/8HFJrhAEyXlBWUSk0sx62N1gdQnYISv1G8Ta2fP0f7xSE6KH0lEKUSYM1ZV5fk5SxNs1t6ZhQk0lxxDfXOIc2TOJRu0kWqSI4NAwExIE0DzoXNNcEoTBSNPGmDq5ArN+pCcULmpwzD9yZ24kARj8NtJSRjITx1id0/agfZXcoxSqVOlCVHYLpscc+sKbASc5I8UaJIl/JszIU3VaAXKe7i0hvZps4ZzMVhNEnAXsBhiouSBkOmVuXKZcvNZHPkJpji3uolCaFWSKSHXoCt6PpRGlhss8hH7LLngxt6lKV5q7IR+ou3MRpjl0b6FfJqjCJEpE0XFR5rp8yBNM+eYWe5rlJxJLh1X6CsNys1ZIQS42mqKH7ObTFxD1Iu4eRoNVsXmU4qgdmlG+/7OMbJi4yQiaX/HKQM+BU32oAksIbUUuZnydMJlnwdpYjhpJaE+368JjA8QUTJLyLUxMPvb15RmmLHKyfHlJbgEQmDX7adiLtphQbNXguTNbkua8KU1x0jdFBqkWK6YcH058us9G9L0J62pc74ziVSU8wlE8j69spBGmhskkkbzOdgaVEwTTSTinB2uu2mVItw9Y8w16LAnf2ZJzdGgiAbJlJeMiJNapM83Cd/LbEhzoMUNSErnaaOAnEhl0liZIdlAYhAKCOyqREBiZqfkX+bmatp9zdsnbO15hzE/AKXJc8LEdnHEOiZrrG9zjJkdI8kLZ54rDCbtB2zoY2oytVFgGo3U1Akj+az8FKQkuE9JlvvKiZwLpg4AlUhsF4yVTJCJ45UkyEYe0Ze4VlGSvIia5nDSnrZCRYcF2ihAEKk09xMjNVInwZxpwmil3Vg5pj41B8lDl+oOsOVPTZS568oNdkXkOePsMPgTNUMLaY/sWCPM9mwtEoiS5M4CQUqpzwG4B2AL4Fxr/fVKqRsAfhTA+wF8DsBHtdbvxGTFTW+C0Pw2C9WnC9ntPI1UrI0CYiIFPIKTapABMz/kNwUI8s5NGzJ9U83oHJKNzYHDVKQ3hdw9J7MXJ8ZIXZJ2mmS2+/KEmislLkaSO9Y0/yOt9U3r/ScA/LTW+pNKqU+0778nKsX/UJAQnYBIMdRIKbOeH4MAQ3Q2mQYlTUSoWY9/jqaYor1KkJP2NAVK+15zPo+wz66JMTjm6NxKpn+OBikdw2+z512OXgHw4fb1DwP4WcRIUykiOk38MugyRMptFj7QSDXS/JkWycW0UXL/UH+c0DLABA2YQ5Y/dawP1pZjMMdAUSkCz5AjiuTGHvKYjD1FxRv5vkyhBpujQXLz8BWpHWqaGsA/VUppAP+n1vo1AM9prd9o678M4DmJkHpRuaZ1hQH/DRYMkTusD4sG/aBEpqwGCHNfscsClf2NmS/FjyRb33zQTWcIMCV4xIzZldl9qNVWIRArp1IwuI771ioTkbNG2UHqJk8jfZdABvlJ5I7QFNk55WqLVDsqYBwh311vQvxNWusvKqXeC+CnlFL/ypmM1rol1AGUUq8CeBUAji9dB1Q7eTsI6weDvHosOFJzS3xTv2lVztzv2uZE8xVD4BkpTf2YzDhjEHIVCJCqBc8OY3e2S73+YyLdBpOY6NQ4Cb7OPRPkoE3GbTmKNLXWX2z/vqmU+nEA3wDgK0qp57XWbyilngfwJtP3NQCvAcDptZe0uUj2ZxiQJAgiJUnN60Q8sKWJtBmX6R3YsCLZBQBECUxCUEn3SgmTfCTp7h0lNOMEGZOb6FLT2RkvgRyBsgQJyEzqmBY5qCfGjiCbNJVSVwBUWut77es/DeB/AvAZAN8B4JPt35+IC0P3hfgk43+mAZGqobZZUiPtiM2fLxWlJ0ge4DVTQOhHhTdeVMOIE1SK5jeaLkr5QfeNkZ8hyQyUaLaj/JbcuIm+Tq55ihk/hQbJtrHHyfs+x2iazwH48faMlyMAf19r/X8rpX4JwI8ppb4LwOcBfDQmqD5SePhMM5VLt87b1KGmjtTUlOunLKaREkTa9E0xuxkypavKR/alWCDiVO0x1rR+AugSwA79mtJxoj5Nbh6JxAgkkSM7tkB7ZOdQQoO0PneOhmmQTZpa698H8O8T5bcAfHOyvDbwY8jz5OZ5X7kYanuTaKSAXCtltE1SM0Xblvrly8w1DYKbQy4SCJaCZn6MLhR26dMU+TMjDXKIEZhGcwSyyZFsl6FBxlKOLt4uR6q9MBaxPXz2qCO0k1tbOuXI/uUAsjRSkkypb0VIps04w7myREa0dW4SU2WRqbPLPQMRyaaQKxewkmIk8e4VYzXMJF+msGFpUgTGa42AnByJucg0TGrMCAHGCDLx650HaQLdxB1Ttn358Jl+N+GTm9vu9YCcBBqpT6TdmBGtlGwHysRvQQWfGO00aHpbffybivOhAug2+wgRK7BjDXYs8e4Do/2YiR0ifraoiyCDFHvZCX24DBJJbqWwnWxFkFcgytuMzy+E2ZCm9vIy7c9ik8PZs82Ulda4dGs7eJBjGilAEGk7YFQrBeErNe2UN382GMOY+hjOu+8hI1UKQWIFxOTayQP4bAAJOi05X8QuMMbn1R1SlxBoEPlLY01yAj+hfqEAZo7fkW1HjV2AICPvc33UsyBNrYB64ZGWz2m+dqgVzt57NKhrfKGueWuqHRnmS7GPmCDG6dAShTHd2XZdAKufw4CQKrdtD5rAunesX3D4WchWgWrHJREj0Mq9vjkIdZMS+FiMDuxQMCJLHEnRIkrgmfmaqUGflOi5fNWPLy9ClhHNUUTmFV8nwSxIE0Dr12xfW6Z5F0X3NDlbM7Trek0UOLl17miAjvxOjkuCrNbnmf6sxiUx/5228tQlc+YRTypDLdaVa15ESCnJ12kPUIbsomSWM7cpkUi+Yi12ZK5mjm8zhUjla8YpeYlmNDuHsJmv/esTI1gBZkOa2j5XjSO6EJESdWdtMOmyHYn35HfjD0iZ8I/6pnBn2nrtANL8J9sGCLVpn0CqgIBYW+GRaLaYYB2xOzK9d0WGESSb8AXzNHN9m6lEmrZEkpIpMbOlc1Hh+oAGWTLJfR6kqRDVNGHVRYnU1LX9jPZpcHJzS5OSR5ycZkqSKTAgVIo8Ugi1kZ1OqsF+3fzC/QcDAeK0IfIGvGgBIB9jTPmEdKWxvs3kPEukESPZXhp1F5Bjsnnt1SdH4C+yea5tP59veptywCWYkOlttfPrzp7to/GXb24HZn/TL5FMgQGhhkz97jNY6H2XiaRKzcXq1/QNkCvX34d1JHK63zGdfPeOEbmZyaZfzCMxImo+mhS7MaTapaxsjGlN9vfahEh73/tpFkF3AXyiCZjftqbZ1dmfndFKbTkdgWrg5O0tVG3NJ5FMu3nBK2fMVjZBPpVUnfkNq5qxIuQoINgkeSEs6OJdBYBsFA8GJYgrETHPWyseGm8EMRLlWZqjRE6i9hgiyVQ3yzxIU8H9VdcWaXkPc6pp3tV7dVTbh083T/PJrT4XFEBHpE0/mkzN/JzyEYTqtpdpqo5odpMQM1e6uhnX/knm29nyurdjVg5NEc2eGKU1yl5upEEwuZ0bO7HPGGKk2pUmR1G94uszb7d5kCbg5mkqjugs7cbS/JwLkRgwojJtzp5ddO0v39pCG83It5oH+Zae9kVpp/aYEpMfIP2oTh978gaLvjJIrkA475LQzkNgH6AL7tIcE3wqsconKicncR0Qm9KNjDChde0iZrVEVozEY0GnJHK9iJqmRvshfPIzfzkTXIeJKmSaD4i27euTqU2gJ7dqhwA5MgUI7dQfy5urX+a3JefrDMinCQXJFYgSbGhYcaI7p4zMjExHJbUDyTvnjM7BDMlILE/Kw8zRGhl5SaZ1Yh5nkl9TiFmQJm2eI0x2vkap/P40UcU0U45MAeDhM80kT27VZL2RD4QJFWBIdRBYcmX65X6fKLECQXfAQAZAk6yNhdshlQSTOTN1NdLER2lMlXYUJ9O08uRUI/bcH4HcGPGRcqix0kzrHPLMcQnNgzQB1GYm7TPhPHx+TEjT5QPzWQ/LNQiNzJZDaY2GCNv/Hry3sur6die3amgtIxHqpmQj3MwT5LsH+g5MuTVOlHq6BoKbSpMvkyEi3AlIcLR2aZAiR3JZc3MxhYQXHIdtm9EuI1cz2Tx36hJM/4zvfh6kqdAnt5tfAII0AULbtMqDkXO4dc5rTmPt5Mi01rNnq67+8s2G0Vjtzx7TVKeY8bC0Wb+9B9b3GupLPUec1sndeKla57RKYTkkzlOszUSasWQY6TuKFJm2pVb5jPU9cgnvT7xPE8Bgww5H47QvBkOmHBF2Mqw6lsgocvXlWERMkk9bdPZsf4eftARKzSlGqm7bANkRLoKumZ/iIyAzSoslH76QLMHNuI80oxBGR/CF3YMEKJSVvnZcPo40Al+cGNn50OQ46C82y+NjcpgNaQLMh5eQYYgI7fchEgRByF5bLrBEy+rfnL3Xisb7GigTBCKT5O3P4rW3+0Q1yUG/IXHlEK0P1n1gi72IaUY5Se9TmuOhbskEKiBFQm4Z8sw3q7OCQZm33ixIU6vhjegcd9EV+h0xaDSJmR4hZTKAxJDzmfGH6lYD9W4UWgONE+uwjy10OF+3n0CD5JSa0LZ0TBJ7cJx9YySHlzLFG1l5/SWmc7itUK5EQ40Ro9cmbRVPYGxp24zvexakCWB40hxHiOjLyQ/OEGMSEQbINSqHaE/Wq55AL7/Vq2R09F5ArABLrnYfsp83Z6o/JaeRJXn6A3UXSdHMmKvYTxs1xUN9EwgxMFYWKRLtkjcYTtBIo3PMIeDE73UepKlarYTzaZqiECG6RY5ssoLQFmPjiOVwJj/T5+y9hP+TcwEgQNLEw8P28SdHKQDE3NkHsQAxUprz1Hmc+4icj8nNLGaKj2w7lhhj9UFyLGC2x3ZMCmEepAlAV65K1j3sMa1P2sYfMKAdhjTLIKFScijiC2iqZ+/tI/CGQONkSYxhmoS0SubhDBKtIydMupxcVpxUs9kxZpeLGeufch0lhEi0K+HDHLQZYX6LyNGuk3weBjMiTbgPoUM+QzKF3VyqNXKmvi9CueVUzigrJ6SVUmVEGyPzYauBnrw1OKB9ME+7X9T8jmiVYm0yQg5Bn+sFRjqJFpAZqNeqrRe4WEYdQTERcZbwaQY1R4FZfvFIU6ELBJHaYy6Zen0GZvxIQh1WhuWE5hYqc8z3tzRDejpMliWIMnBjJZnyHPYVFCpA5qV8l1FZrFmtooQUlS/VvmJkk6IxxuRJtEbvfZYv86KRplaAPkJjqnaFzR/nswzqtWt+R8hSm8EssCuPQhpliHwH4yEICbHabc6e99TgvlVDqAljss2i/YcNony3L0IshQxiHXVyJPK0z9Emew7xZtXLNENHlpR8xaZ7eI4cZkGaABqfJtB9EpJ4lPta+fXea5q8NHzS6/M9eUJN1kyFJr0WmFUDF8FgsAZnzzVLRC+9xTMUGc33ZUU0yqTcSoHMC4XEzyHSQGOmd2LfARnowBgZJjfZbiAnQIpemTiVqAS5Sgk4gHmQpmWeG+1xSDxDU1zbD6RHpk53wvdJE2KAUL15BAmVeJ/aftCHIleAJNiHz/WFNoFGCVNgfotuLqFJL0WpCHpxn2rKb0cG+Yn6SsiQM1uJ/sWi5wkmvDRSXsLspog19b6YB2kC0NYRvg6pWMxH1xNk6vSz4BNqxJwn29mbd4An1agc4n1UY6XeU2NZBPvwvf18TjgNNEKYoXGzZEVQMoAUSlsqQqJCGaM0zhxClWpSIQ0xNL7UVI7UpbSVRNqT5sHJimAepKkAVNrmx/avS0C05qj5eksGa+5b7wc7tAnM8SGx9RMJaaoyWcQ8KJnCvmetBuqTZ1STC2mogfFyENSGM6EVT56jx0rpH2mb81CXTiXK8X2mkGKsvTQFKTkKLh1DgHmQJjT0QkPZn8YzlzlzndUuPQ01RKhGzqCNZ/b7RRK/5bCtHvwhSStGrsSYbJnXvzPfNXDppg77TCNyB2OMIaEJCNMRLzQ9i8hNHCfLBEeeZmh+QIqZ66E+CYSZFcBh5paslSbcBzMhTQAVoD1CcbU7mlDFZMgQKt+GJq2BpkqQ6qAdNUd/foP6Ibna83DlDr9xNm3KK7cJlDXfiX5ckyL+xxIyJNgVYQrGyjl6l6uTEF0t3XMzhfT8+pLkSrSTRM8lvs6L6dNszXMA6HxiWrkfzGcP6zVJqO2fEBmy2qBn9pNtJLI8898nVn+6bmGPuJY5vDapJAsAZ+/rN2i+9BbVj+w2aFJsu7epyLOQNltsc44U8gv0STLvLW2LDfIlEmYRUvTajgnw8HI0XS7EPEgTABaMlumTZEdEPamGCBWwSJUi1EG/iMnuvNaRdgFt1X4RIddUc5xdi08JYjTSs/ehTV+Ka7HDKfk/YhOBkz2hib+riHmwf4KbQWqCR03VMRpnQNZYH2YyMca0UQHmQZoKUAtNL0Eky1q7WECoTRXR1gwd01IHcxBoqr4seMQakEm298mVKRuQ7OBNoMwa12728H19waU3lTt0CiHuyuQuAdvtYpcJMZYskzVPqdYZKcvO0ZTKyyFF6/1QHkGMUhNcefWQYyakqaGO6uZzmAuk4SpEA3JDdyX0gIAYrdXqY8qHftTAHcGQqvL7EARNm+EuzU2SouSME+4flAHg7AWf+Ruc+NroCIKcelcjH6MCT8K+uUGeMdqjaGym7+AUhVSyFRNXRBv03ou0zZR52Ddbwn0wD9K0oHVDnEqh93PCIxTtvXCI0G5IkWrkNVfvtCE02Zh8UOTq9svWRLn5dmVuBbuMMkRYgafv7H3a8oVyT29Adnqz3SOFXHPN7kj/JIKXmMuhMt9kp9pHTXbNlNPvJeTLkiInhyFGbWmai+fPkILZkSYHZX344VHnmiFIGak6xSLSlLRRw5uqBLm27aOEF9AuSWLi2Ir0Pbjy7FrXnJedC7Fr7ZJDttYp6JclO2T0ZJjfon6i9u3z5n1xNhFFZUk0RaZdjBS5cRbPn3XcYfgk57SVWZKm0TbtL0bDu36qDTcQoT9jylO+N373JMWX+c1TiTX23h+Hi85w7yEg2rafWLuMPOXRTT8APHxfv6VdiEBnwpnlMEbT5OrGmupSMuTqB+21Uy6OcnMyOTM+pCkyco6ePyOVrOamsL14AAAgAElEQVS19v665RJESVMp9UMA/gyAN7XWf7QtuwHgRwG8H8DnAHxUa/2OUkoB+H4AHwFwBuA7tda/IpqJz4qpULq/ANZX0xPdkFTNsF2fgbaqaXIbaH4RjTX2PqWtN17fJEK0TNmAbLu24ZsomlXk1T98fgsAuPQV/uCgfWmck/s1UzXGUD+ijA8QEZqgZF4J5nInm5Hhv2b9mhKt1JOzev5BUywgRbLOGqIqSZoA/g6AvwngU1bZJwD8tNb6k0qpT7TvvwfAtwL4YPvvGwH8QPs3H4y2qbUK/jqYusraCKSz1imSM2P59bbMmBvAkkGqdey4frvE9+I2ArIN9ZfUteDI+NEL2+71pS+7BDpHjXOMHzGljZT4gu1zNUyqvecCI+XFZKcQovfabnP8giFHW3scapJSUlRKO/2rxB/NKGlqrX9OKfV+r/gVAB9uX/8wgJ9FQ5qvAPiU1loD+Hml1HWl1PNa6zdEs8nRNm1S1Rj4KPrAkqWJqtojTkOEelBmZDjjUW3avzaxDvtaX5zTP4Fgu/aRn3rOcUk9NUKtlJsfPVKcAh++eN69vvTlBE+RlF3HaJDUsFJ5Ea2llHmeTZyxPlIy9NpmRdEtWZdacgRkmqItthpomENSdMt84iyraVJ4ziLCLwN4rn39IoA/sNp9oS0bkKZS6lUArwLA4umnwqMFfJtsF08T9d/bJGrfRo7SOPBpKtgaq98mhVyBCMFaROoS7LCeGnMoz+5XgjgZOUkyejx6YQMAOE4hzwIoHaCRtithlgMCAiTKY/7MOGkK5TIyT1643xczWqI/TAop+lrloMx6n0KWBqPvUK21Vile1L7fawBeA4Djr37RYi0AitYaeWEKOQ4xewxfZXfbUVopXd+/58nV7z8kO0aDJds675ynyHZlBDqx84g1FxNn21SCxy9uuterLy9lnQpgMvIMEWdqP7G2SUiOkiY/TpAM7ZxOr93lF+7zJrXVzidE+7VtOvsESGmKvubpE+hQ+UhHLml+xZjdSqnnAbzZln8RwMtWu5fasgjK2VESsuX8oXa5c/FbQjUXvGY0R4pczZyo8v69HhATq8F2hTzR+vOSEa7XS0K+gT5BCG/czYtr5/3Rl1fCAdKR/pOL8aQZ1DbDxMfKkJjnQo0w9l63/U5fuEc2D5FhU0+V8VoiJZtqT0FrFSTMotFzBp8B8B0APtn+/Qmr/ONKqU+jCQDdkfozHSKjtE3CRHf7DLVNU2/k2O1DdTZqrVC1hGm00Qq8SWAI1f+CatakV1YbnvhosrXfI1jPlw2K2JsrGDEXEuGYX3r9VQ+HZW9cypYH7EnDLECWMnLUwXpuju954U5THQiWhExhu5wiOqofV+fDf0b88sqXhSFxU0i5LyUpRz+CJujzjFLqCwD+Khqy/DGl1HcB+DyAj7bNP4sm3eh1NClHf148EwmEvs0k037QtyXTgPwQFlU9+NJrhiTtOinJhmR09d6cxhAo13ZYx7k22K7JY5F4qV/Ncf7GZV5uCWNGoo3441Cmq9j0ZsYTkp/bZkigz7TkCNAkaJf7db4ZPKgn5FFkxqEmPpRRYHwZsYAOR7T31sdsXQiS6Pm3M1XfTLTVAL47eRbB8cMEGEs9igWEJGOkotYKC+umWVi/fvaXxJn5g7qQNhogW6qeK6NunqBiKSLSOErtIgcARy/3ZuLDN66OF5hwT7AEF5OVct9JtFDC9H7uhdvNywC5SDXCWBufyCQkNriHoUhCXFaa1UI58rvzOGyJ7CUQVBIxAowLsDRRjwjHEKNtogPNF1nrcH4Xp6lSmqgZw+nPEGVIqySJT0CQdJtBUbC9pI4bfwqcvnQXAHD3jdNww6mmErqPUwg0gShfeOHtQZWEAEPtbI2QIk2qX+h+pb77RVWzMmN9TfldhhxDX++V5TpQy2NWpMmB822OlxsOCElMdNPWNx0MqHJTVimNCnTU3iFNyG/CmOlO9hm0CBMbR4xSMiwRwZTimZdvd69vfimS2kZBOtVSJMnIs3/wv+qFW1Y5T372e98c5vqFfIucSc3dnwNzeqR5zhEjNVeDywQx2vff3fVxcB4UZkeaHZElOBVjAaFge0F5CmIBJarM9LFvXLttjCgl5k6oPVcWIreYQz6Ggla5GC98VU82X/rSjXQBgnsjas1IAz0A/tCL7vb5UuKj3ocIq1L1oF3VKgIG7ut+HwH73qpg38O0ReXLsuXdWV8K3nNSYvTl3wsQ48U1zyd6glJM9NJ+TYDm/VRirqBxVG1FJj13M9NtZaQXJM7Ar1qKCb4Lc93H+19uCOnzX3q6KUicQvQ7DMjz77M//MJXBm1CBAi4xGeTXqwvRZ6UphgysY+qbVBuzDy/sz4ZlBlw19U3pX25RguNPVl2L1tm6F72MQ/S5GCxDmWiJyij/BCxQFI7hq0p9iZ549eMyeBMd8B1etvtqD6OJtr+qg9uykSy5MlzuCsRd2NJSG+OJFopjT/yVV/u3v/el55Lcv0ErWtCzte80I/lE509p6T3A62RJ1PnRzWi0VFmPUuMBOneeXzC3i/Us3C6fMxGzA0MMYbm7tdcXT0GEP7xv7s+LptytA/kmMmhPnTEPOLPzJhDiBzHtA2hucHrJkJv3eDUDbhlTCx7ToOyDKIsTaK7wte+/IXudaU0fuNLLwTbc/fH177wJfa7pXx5VFub8BZUfQKZbYlrbfc5Uv0mKvZY3P1Sa4Xb65Pod+h/1ivLx857/x4M+SztOfuf1xAjNx+tVdR3uYvk9vIooTZ2ssoFi7KGZ8YPEaVN0tKgEqeN+iTqEx/1EPk3MKuBMl8SJTMkX4KpiFX6Y/UfvPz/da//5Rsvdq+NxfOh5/ttFihiA/JIkpNJ/ShutRJbBcfVueNzpDRPv9/bj68M5IQ+x1WLGCkz/U6AbEPEdWW5Dt4PualFl5cbsjyE+ZAmhwQTPZSD6a8ActskmOiAo4lyJnoX5EHcr8mZ6H0dV+5qBYNAU9vmSDU+0QUa5/xS9T6phaqxFWqdW9DEFyM3Kflx8qfGArSpbONb3n83SrZcPSXfT88xn932Kdplvmxfpn3tehlN2d3zYzzaLvHIq/dhyp9aPep+BG8cPyDJ9e3Hw4UEWivceXwyKAd6LfOp1cPhDzRU1PQOaaGXl2ucrlwt1je3bU3TTh28v05fmjsv0tyxtrlLEz0UPY/JqnXlaCJDjbMaaCpbrVjtZ4vKeeiWaouFqlGhIdGmP0OQrKYZJrwUR3tMViksGL8ihViKDCfL1/a3ukINxX5G870uLZPZ9l3T5Nj8vfn46qDOl2tenx4NTVpbe729vhwkV1LLXD0ayrReS7TMhRpuqnF19ViU8SFd4UMl3qdonPMiTQFITRFpXMtFyqeIoPso5csMjzEk0ZTxK1XjuNpgAT0w4wy2rK8zTHhcPylSzXzuOkixiJAlFWzZojeZDUma14BJzWlI0SR22zDtKHJ9e32FNaXd1KFmLteXD3H3/Bi1rrCstlhV593cblnaon9dK+jBQ3X1yDK9E4I/NpFR954TxQ6QY8znycm/2spP+eEOYbakSW3gkdwvd7xCkBAUFZUP9Y35NUP9oCqRKepjqc47857SqGKaodTs3ldwSPojtkDtmMz966r7MTB/DYFVqh4Qr2ljEyvQP9RvrU+d91uHdGoAPfkCwJWjoR/RyL5pkey6XuABVt17V/vcNkEVxyc51GqpAJCtGVIa+WmngQ7NcoO7jy/FA0usX7InXXbhBRTuPU5PZKcwP9IsaaL7okcQI+U79f2akr4l5sLB93N25QHNMwcVmkDTArVjRvqIaZU5waFdwGjXhhS3WmGj+0fFPNzmB2ihapidPy+pTWdO26QKwNFAfdja6LOre7i1uQK0ba8vz9r+rt8TAG6ur+Lt9WU2kNelprXyTwmCtce/9fgKS14U2QK0We6/f4fwgQIe4QpIcUwwqNTzNj/SLIkd+zXtJZWUPzEnXzPUnyNJTm4IW10Bqu4e1BoaiwAhGjS+0EYDXUB3r+M+znkQ5lo35xTVqLC1NEYD4xteKNrnZbTAvn/FBNF8QnG1TB/vWZ5hqyu8tb7aaZ5Nv6HsZn7bbi7XPDP67uYSoJuk9HcITdEmXJ9kAdcsNzL9uVCkGDPLAQBKs5Fxv2zM5huUzzLXspk1aXImuuSIX1cOFTHfn18zBSmBJR+hYJCRATRkSRHrFqohUQVUoAl0y2ixl6pNY863flF+DtOT5xaqJTPLjNaVQ9wValSWxujfTEbTNNqiE5AJ/ADkfr5b66uooQaEWCng2pGr2flEffPxVdLneL6tmHSnLU5bs7z7ERDmZRpNkfxhVsDp8hF5Dez53X5ER9z9+YceS2rzDc6Hee9x3pZwBvMkzZEm+hSmb8k55ASDYiZ2rStHM4wFg1L9m6E+NYblzXysc8/VFgtoLAVugljOJztHqJbUmr8b7Z52Wame3oxbwehHhlB7/6L7HkgLYlF+XIlpbt4/vbrfEDuRknRzfbVtbxOj9QOgtLOe0NxrR9UWy3ZNuB9MutXmY3Jkwt2vEtP8ztrVEH2/Y+xZCCWvc6lFsRU+Y4Kx8yTNkvBM9LGE2uVfCuXsIhi0S3Ca5dYjbb6/wqolr6UCFgAWA9We/2xbrbEBsNXoaLohN9cUrAiCNmRq+tS6whoLIkARfm9/lv41r1FyZBvq89b6lNT6gF6rM2b0U8uHgwBQM+826ry5hEfbI6zrBWMGV45M0/fa8hHp92z68KZ5KN0JAGCR9imRTsSZ6qGNNwZjecFRALi6WrPyUzAf0hT4H53mhImePXSEAENbxY0NBknmHvNLSiPoQKOxcKa2FDUqQNdYeB9o2ybi22MtAmPVaEgTAJZQOFZHHYFutUaNGlto1Fo3f9u29nEjDXR3HWv0hGrI1Gig9jx9DHyZmSlVbo4itxggbrLbKT8LtcWpZ5L7prmTo0nMndq96KnlI6cttbrodru5RkrE3JA5ZZpTeyNI1pQDMvM8llZkxhgTSZ8PaTJIST2iIty9nHS/Zuk18MD0waBQBB3gt+vKwRYVlh4p+qa6IVI7OLa15rhFQ5wNKdZYtDS6UArHWGGhTMCkRt222ejtgEy3Wg/omSLMbp5k0Kdi37ukS/szu2h7wDS3fapNf/p97WnPb6+vWLJaLW35qCt75vh+N4aRcZMwuR0NsSNEd74m7ajWCtdXDx2ZtoyYhhnazcjgynKNa8e0ie+XGU2TapeyL+bl5Qanx/46eLn2OXvSnAJT7qeZKq+kqc0FfnIi6FX7N2Zyb6Gw8MsI/ypU7ZClrWXG59UT5tYQZYQwfUgfil2Z5jFU0Gyqkfn71vpqT5S+iQtFapeny8fEKqWhdmyWSXLXTevhPWWb9r6WGUp6l+RoAr1ClLqOnJNta5sXd5ejAqb2HEERY4lg0C4j6KElh5Sf0zfVpdhqTfg4ZbBNc1I2oXWWXO2UK3fQTle4vjzDVx5fa2UNCREwG3qYJPpmzOvLhwMyvLe51GZAaGy1wjvrywNZ/bzbaDhBuAa2Jkr1NTABIAkhhdOFZMnrBjYZSn8sL+YuRwFETfQd+DVt+MGgEhoqFwwaJ7NcBH3L+DE7eUQEnW6nhitktEZlEaUhzhparI0O58s/LD6hxciMX4c/NM1z4JvmAPD25oq1qqgpu3bkBmZsDdReAWTKunlarx9tj9zllnCvxzVq/XiGad7Mu7cqKHR+SI4IfRIWLKMEXALkFIbLy012QGi+pJmZdpSarxmVl0CMuYethWWmLKccJ1MCf7OP8DgxDTV8vcYiFBRI8We6/dKXjA5ImiBJHzeWD+CuKgqnGtkrgAA4N5r9Xa+qcyxVzZrmbzvr0fn5ccEfYJibGdsYO7Sm3B5Fcs/aJnpo/nbd/cSg0HxJswAkRDdFkrs0Kp4CqV+yaz9hWtK2TXj3I+OhtCMn+EO4DJpoty6+TiikRcYIcOyqJS4AJYHtr2z69dfPJigTWfeDQDYZ3mtTjgDg0XmXvu+0N/B3QzJjA/G9Mv3PGDvzx3cFcDDmOXlqaoEE9lSrbn6k+YT4NYNR8pGENqZ/E8XOHtrBVissPVlj0o5c2RqVGgaBatSDIFAMuUEgV4ZlijNBoLH5f7a8hdK4vnwwkEsmuzMrgOz3/f3inScE20c5DCpJdjMKfW7JmT+SPE2AjpDnHMRmcJKxATEwR9KcAgQR7yqCngpJ2lGoD/W+Ky+8cUcq7LSjktiiDwLxbYRBmIjZLkFKEIgz1a8fneGt9WkfMffyKe2xTDK/r7kZM/neeU82R6rGsj0YzUlpiqQn+aCO27iyfDz47JIzq2zzPHR3kM+C56K6ugrv8E7NIRUXhjSpYNBU56GXRIm0o32uAMoBlau59NpsOWJHjWqEWWyS2ilQuwzlLo2UBoFiSyc53OoCQf02cF1Susmd7FKQeNPc3qAjtAacXpPelF1d8qt2OAIKmebRFUNem5R15b5sP4k9JbWIw7xJc4Jg0FgY2X4EPQYu7Wis5uXLHZuradrKx5ctnwzBJLj34zfEWbf0myUz4eGIBYHGbpycg6eXD9j8zC2qbgUQpYna5c2ZUbq9T7ZdnX2kRVMWNs0BepNhjjQbvyuNayvaN8rJs4lOurbcnocD5nlLMdXnSZoFtcYxK4N8GZITL2MR9BAkaUfhNCJuXXj46AvoVqPpEtrb3drNVnFWOS1D9Qnx3Zg0meYmuOfAbN5Blfva4dikdn8catmmeMOOVtbtzclgRyODZgVQL9cms5venpg1QY4mZYgjQSro89Sxr+GmBYDsZHYKHAHbmmZ3H9vPNNnLlbmtqyDRpkTQ50maM8EU28SV1IIlZvvYoy9sxHI1h+3lCe5NkKdPbLeXVNpBoC3SgkBmHhSm2JYuRWbMX3rd2ojj7c0Vdtu27rVlattJ6b2mqXGkznFUeUtfCXk2mZrD0sZqmYbw7P0zH54v++3liE02TBu77LwOuUPSg1KpOJDmhJCQUlJSfWLaUVDWCD8pl6sZSnDfWkntdoJ7LFezjnxeEwQqAWkQaGp/JtD4NG2T2d4c2GjwW61woz3dMbQkstbKCQbdXtNLJLn33aFn6K0goCG5K1bk/eH5cuAjpWQ+2LgnQJ7X4bvalxF6XqTurjG+zdmQpkSrGxUMukAR9FLgiDElQZ0Dt0XccA7hBPdSCC2fBMb7JSUrfkRthJqo8WlS/kz7fa2r6Cog+/VaH5HbwJn3V48eo4bC2fmKlWFv7fZg45q154wZHItYh2pDRMjVGHlU3ukD69jeOuMxnw1pspgyqrNHBPM4C2qUu4Z0X022P4xWk/elpyyfJPsXCAKNSWo35GLSjUKpRgZU0rsxyZ9q98Q0mmatCQ3QknW/bbfeNq4RQ4CxUya7uURMcx/aew64yLp/7K5LfGnfkbvMMqkrgItAmpnI4dqpNMwxcsXLKIW5miXRmKlygoytCiqFrUc2/hzctuX9minwg0AGN1YP8Pb6ipOza1KNAODOpt92zd4H005NgkJHlut2RVAsKd33g3KmuVIaCzU0c+3dlagVP/Y9aWRTmwtHNVNvfXnI3OYi47km+hNLmhcNu8rVbAJD41KEYqBWBfn7bjpzQnhV0K4xNnLu9IkktQPAnXN3o19DgMtqa41Z4cF5szTwcX3ktu+i8zwhdpt/WPLJH2PVyyL9km2Qye9vE58OzGMwnlbkFbJ/UM2r1MPRQnVaKzxYr55Q83wCjI2K5+ZqPonYthmVcwT30MTSjYZy5Nrone2Js8rHTyPy/ZSUjxIAHtdHZCR7q5XlS27n1d56R1XtaJk2mbrR9G37171nK6Vx+3GvwTrkFzHPXaunh01+0lMnJdu/GTTEFybHEHJ+lN+VpJkD2eYf+bu2l8SUpu9Y2An93Kogsh/6oy/KzENGhLe3Vzpys3d7918DaaZ+TAOztUzXf9kTqe079q/jnc2lARGbfqF5sORnmeenK+/soMDSSYm/0dZSJe2TDmbz6oypPmn0XCn1QwD+DIA3tdZ/tC37XgD/JYC32mZ/WWv92bbuLwH4LjTOrv9Wa/2T2bPbIeZ2dO8U2GWASbq/po1mlyPfR1YP8jGlu7U3Ml15t+vL3fG95pxzcySvf4Z50twja65jMA/+0jtf/e3NlVa+S4Dcyh3Sj6i2fXkr41qbquT34wiPI7NaV85pkwMfJ9uvL7dbmB9U6i71zXOJbApjl1JKNM2/A+BvAviUV/6/aa3/V7tAKfU1AD4G4N8D8AKAf6aU+sNaa5ETLTntaAfIIdNa50Xlmr7pCeuSPlLtszare7p9GePHXnBLKSWrgvoyOOvTN7rGBltsa+1syNEfmNbkJtpH9vYE6GqGHUlCicnQpEktUJMRxa2uHNPavDbHBttnAvmaqGO2Mya8eX+j3enIL7fHuLm+YpUN/Zt2W0OKqbmaAL9j+1Or/qyi2G5LPmqtxJqm9DA0aTSdOx4jhihpaq1/Tin1fqG8VwB8Wmv9GMC/VUq9DuAbAPy/WbMTYo5aYnDJJWSR/ZwfiDE7GW0jSyVDdEOdFeTPS5qKZLZ+4+cp/7IX0O247dhWV45QbS3UzN1oomP32LQR0kb97d5iq4H84ymGEez+2q+qLZvmk6Jp+ru352iZgPssSEzt0K7rU2uZwDif5seVUn8OwC8D+Ita63cAvAjg5602X2jLdgetiiS4zwm78IVKNdzSieop688Xqmnrn2fuH9nbyFXYdFpnr4EC6LTQXIzdQ1Oi7T5zfL87gfLa0cOBltm8trVVotwqu7Nx13tTR/ParyUb/obOMwcsU1srXBWcb54SAAohtI48J1ruI5c0fwDAX0OjNP01AH8DwH+RIkAp9SqAVwFg8fT1cOM9JLjvkkynyON024S1zxIrhAD5KiEJlqrCEgssKjU4A30D3fk0qdFSHzK3bzzdSIqxq5CuL89IUvbPOJesBLJ/JNb1EXlwmnOIWlsf3hYube9MDqGjeSWaYdoyyzTZFLJIU2v9FfNaKfW3Afzj9u0XAbxsNX2pLaNkvAbgNQA4/sBL81b1RmBXEfNdoUkxSk1ol6NUdJzfUzMt3SgFEu01hdDfXl8hTfSjatuPpYcrgU6Phr5HW9M8qmoceT5xf34xwrtj7dMJMJqpsMyUSDYZtpGat1nCNAcySVMp9bzW+o327Z8F8Jvt688A+PtKqe9DEwj6IIBfHD3LAyZDDqk3aUMTTcgex9rpaF+YYickKW6sHpDBIX9etnnObTS8rvtH/dFW5rekVvsYcPtUcub5ZWojYYF5Ll2+GcJ950jfpK4kJClHPwLgwwCeUUp9AcBfBfBhpdTXoTGcPwfgvwIArfVvKaV+DMBvAzgH8N3SyPkUeBKXrYfShuag1ebsdPSkgTzuIoN8/dMnQ+lGMRPdP57CX6NuXlMRconJb88l1o/CvTV9EJp093cpSqwwk0TPv50o/sFA+78O4K+PmdQBu8EYki3lBwUk28PlJbbn7qMpIbgx55xL4R/Hu1DbdkVQ877WFa4vm/Xooeh6DYV7m95vuFpso+Y5hztEAAkYZ56bb5Uzz6k+fvpc7BzzmGmeQsKHFUFPOOagfY6Bn9i+bYNARccQ2iNskKgggTo5mgLznCI56lwgu9160z/2oYh2aDOOyiczxjwfEzkvYZqHcO/xKt6IwHxJM5Q69ITgohOaFM3RGWW9NLv0+exCqySj5Ix57rf3TXRqNRC1UYchwasmcMQEgSTmeW7k3MddZuOPFLny84NsuaIuAOZMmgdMilIbES8TVgKVhr1KiEPoZMpdYIwrwKwGck7BtAjU33yYWg1EaZo+8ZicTb8uFAii5FBl3CYdHJxD1xI0zV2Z5sCBNA94FyBn8+GpIUk9ur25HDzGwvd5UquB7BMoTf3Rokk58k1syRZzpXM0Q6CWTXJypdrlPpPbDzggC6GNiM3qIGrjDoncktjHsb0+ri/PAAz9mc1r+WogOxAkwR1rezjJskqJGc2SHTOHWtMLPtgNhwP5nG7/5q+d43nQNA94IlDr/mTKdyuoEyhDmmHM92jan28XAQ3SJavK4i0uVYkiLOrkSWo8QL58MkRuuRpojkZ8IM0DnhikLnXc91EXMfgnUPrpRuZoC9PmPauzoKZpB4rMph1zNs99hDYc5vM5i0/jQJoHvDtQytweu1lHCnLM8xzctlKUQru1J60eGmGeh64rm23C5nN64xX4zuZLmu+CVJwDnlyUIGnbPAd4Ez1l9U6zX2qzaQeVomSTzwKu/5A1z9t+9pgh83yKHM2Def4uwbshR3PXGLPD0dzgpxyNDQSZfM1K6W7TjpwlmfY4fl+q7S5BbQsXM9FTtc8nmjSfnMfngAPycOtxm8vpE+KWJkTOPE/VFiXmeYppLl2D7kfcteUHZqH0Ibn9gAOeBJjzgYD4ph2cRtjtb2qtBuI266i1GpjngCEeOzWMON6XMM+vtDsbHczzA5JwMMFlKLkffK4/seTemqUgTTmiSJNa5hgzvUsGgu4+vsS2B4aaZgrxjVkB1PSPNmFxMUgz5RlIOM4zKGamZJd6miRH2qV2KDpgOnApR/aKoEY7hLcKyKQiWauB0KYpeevO7T7mtflrb91na5tSTdP08/fSPGiaFxxzJcccXGStdn463v5RfkVQ/0O5qmqsqu2g3XAcWtusdYWH58veBB+xl+ZFw4W7VymSu+gLR54k4j5gvij9o3pylHcE7kX+cQdmpmnOkfxy5xQiwrl8zDnfvBtoPKrPyTPPN3rhnHcO9OeL22eer80Z5HDPPwfMueWmj3s8rznT3GhI9hnqkvPM+002XBmA63+M7ZG51WqgOXIbeMQ0Rh/r7RHW2yOnvd9H4sNM1TBLr1PfpVlucOE0zXcjQuS2T+IrfZzvAbuDfWpoqp9cPsZ8f5TH4ECaLUpquWPOIbFvtItqth/I9IAnGbMyz+cMCYHNleRCZ/9Q094AAB7wSURBVJHvWhuQHqa2hMLlaoEKlXNG0BYaG70lzfbGxO7N9Y1jgg9NbWOum9dAa8pb7UPme1OvrNd5JryRY+Zn3vtldrspt4ZLQQVNRrr3DcVtIVcA70rSHKtV5nbfBUGFCLIkSqQs+YdjvZuwa7I5XT52iLNZRnnutJnquIvY1nDA+LSjMXmbqf7N+ZNmbiAmp0/iQ2xIcNcapk+MEjIOEdQCdRKhp5L/roj8ImKhanaLOmo/zea1G4SxcysHbdp+7zk+c2Tb+Z3dqqCWyAebcQgQ+hHIkZeKXYxhMH/STMFMNZe5mu0+JL7ImIZZjdBAbeqo5phKkYFK1fA3t5D1082RFIYYVb/bvUlw9/fTpIjDHKz2zuPLXRm10xFFvJLVQSGNMRpJJ+RxskLtQu3dvs3f0+N1sF0MsyFNyTPikI9ZiCB9tsj8TprM9kFyMe0tRbsbY/aGIqkXQWPkNJ4F9GB55SJAaFT7KWEIb4G682FeX55F05MA168Z0kj9nY5yNiLO3enIfn1/fTxoZ9+z3avAjvDckRehFCQToH2w7o/uzVlOORvSfLchmEZkEVdJP6g0tcTXOCtViwM4B5QDd4Qvt1lHyDyncjEfbZeotcL11UPxnEqYwVfbEycvql/ziSfNMVrjWAtxysCPL7t0rt1C1clkGTPv50y8Id/ivvDM6r5Y07y9PkGtevLs14b3a8Ltc8/tnY7ubi55suMmOqdFSs1zaaI6R3ZXVkMTe1d+zXmS5o7N4yfEfeaAI9Fdm9gcUdrz848kyB5L6dFHP5ByUWOLRaQ+TLilSdn3lZ4uH3WvY2lHtVY4StiIeCArMYJ+f33sbi3HyO5Mb7uSIEKtlWNic3JNWwpP/i5HBZFDkEZblWitscR2vzpFE55q5UYJzFmLzA3GDOSg7nI1Q0RZary31+4GwpyZTpnoPrGRWuJ2SDTXVo/az1Augs6Z4xLzHJjWRD/bLJ+wndt3mG40VvZFiZAf0GDXgR6DChpbYdsbqwdFEtzvbC45ffwt3myiubt2TXWANtHHmudA2ES/1hLt1LDPP5di3qRpIRo533muZFqOpsS/mbqEsmTEfQw4k3+XyynHkKDfdy7+TTsQdGN1FmxLuSYMIT61fNSZ5yYFyT4nCBhG2u0y/7XTPhBBv9dGyQc7vwc0TtPu/mZF1kvTkKY80vfCkOYoJKQb7RL72tBgHxsQp/hSFwU1wF1qlJWqsdW975MbW0rKz6zu4+31FVRK487mBABtonNJ7kETvVYwKq9PjteP5dH0EK5ENh+mykpF0B9ulmRb4z8fs8RyfqRZkEhk68XpPqWCQyU0RrctvxqIIiaOrCqli5A2R8ChJHfb/xmijkqwn8xCAWjXn0uR4nOk2tqkNyZia3yARp6fq+mnHAFD0/vG6ozdLs5u568/r7XCarENaprOXJnP6V8f85kebFYis91/HzoCY6yWaUBF3lMwP9I0mMCfucsoee52blSO5ly32Bqz+qfM+GGylGiZdnCHet/JGhGZzw0MPbO6D8ANCPnHXtzenAS1TfPaz9estcK6PiLbAsD144ejfhCkh6pRZVJt8/565SbFm37+URwe6Mg72ZTEfEnTAuXPdBtMRyo5kfN9mv7UaqDYuvO8cWSaZCoqpbrcwuD4gDig4vaLpweNwRTugNixF7maZtfGy++stcLd9SVeU8wMBtnvrx0/QgqoyPzV1Toriu7PMRUXgjR3gaTUH49IpX1LPUo5ie3h5ZE6KWgzxifKH/Qmx0IpbAXEOhYpBDhFrqatYUpOpbyxOmODQcBwp6NLR5vOPLdl5uzkbmvSdj6ov+zRzMdeSmlkhUxzW0aozenx9FH3eZHmjjS0Xa85T42ck/UJ2ltuPmeFMHlKdkOi+nMkm6ORLqBQzzgn1KBEruaN1QMALjEYYr6zOemPw2gvx9vry077mIm+3hw59X4fPyAkNddPl49Z4pW8B4YEeX+zIs1un5SN6S31dU6y9lwp9TKATwF4Ds3X85rW+vuVUjcA/CiA9wP4HICPaq3fUUopAN8P4CMAzgB8p9b6V9KnNg5ziI6HECIfe+5F154Ltcnm6NhmCaX9d2rYUXPzeqEUat2+VxCZ7k3/oTkX0xwlmqA0wT06PxP8EeyrSZHV6VFj3lLnChnETHSzacf9zbFjopv2nImeYp7bdcYkz/GVXl2u2euUm+h+tllmrUaTaJrnAP6i1vpXlFKnAP6FUuqnAHwngJ/WWn9SKfUJAJ8A8D0AvhXAB9t/3wjgB9q/WUghv9QgEBcY2mXkPBcDEz3yPlZeZE6ERmmTba6v0+zeLm8/n93EpSY+FUGHqnDzcRNBlxy2ZpPEeyJ5nTauLnuTNtdEr3WFB+dtxNzTCBeq2UXdmOQs2XrzipnnY0xxpXRWYjsgIE2t9RsA3mhf31NK/Q6AFwG8AuDDbbMfBvCzaEjzFQCf0lprAD+vlLqulHq+lTMO1jXriG0HQaCSkJIb3Xe+W7OlJrLbwaldpJGTqUPW6py5Jrj7JjoVDHp7faUP4rSf4R3GVPc107PzYRK5HUGXolI1rhx5eZkjTPT76+PhdnEeGd9vTfFQ4Mmv95Gzx2aST1Mp9X4AfwzALwB4ziLCL6Mx34GGUP/A6vaFtixMmlORnyhXM9ymC/gE2pSInKduCRdfEcTnaJYEp0HuYz36FJt2VASBpuZqllqP3svrx7y+fOgQFLe3Zg3Va4PozXMAuLu51Mm0I+imLzCMoIcCQ6YtFyWPXTOzXn1g6meY6FyiO9Dk+XIbgHAQk6ZS6iqAfwjgL2it7yrLhtVaa5XIFkqpVwG8CgCLp697lSmSKNkyf2EOSkbO5+53lUCq/e6CQNkVOER5DolJCTlkkjcbH6fkCviy8/2ooQDhtaXxkQ7Nc7uce32v3V6uK2tJ0V5KGZM1Zi06n3yvcRIww3NSj0SkqZRaoiHMv6e1/kdt8VeM2a2Ueh7Am235FwG8bHV/qS3zJqtfA/AaABx/4CXy2+TyMynTfK7+TEqrC5noFJGGNFB3izU6R5N6WCqlk88GomTzdYETMAXbwj3SW9TYYltr5uTJJj9x22o82+51f5LkRst0glCCe2rakSOHIebBSiDvvdPW0/74zzD040p/GKh2qWf+nC7dwBQQJl6uLGZex+ZFmeyc7NxcTUn0XAH4QQC/o7X+PqvqMwC+A8An278/YZV/XCn1aTQBoDtF/Jk5GGGap2qSUyCH0HLSjdL9kXVwNVDOSqGF8NeqQjyp3Sa6wfG86I/uBYC1XnQPulSDC0XQed+p/AEdu5luTCO25TeEnD+nlLmW2iQ4RJyh4E4smi6F5Kf4PwTwnwP4DaXUr7ZlfxkNWf6YUuq7AHwewEfbus+iSTd6HU3K0Z8XzwaYrWme4s+cOnKdE0yKmdGxHE2DWGI7ZYbbZZJ155VSWKLCUjVm7FZr2Gefb6Bxi2HO2Frxfuye+Ow+nT+wPf8cMGelq+7M9KZM0Vpk8Iz56fyaUyAnC8H+jLnpVD6mPMPcHkMKSfT8/wHPGd9MtNcAvls8AwbRpZMQV6ePJ2xbciVQLAiUEjkfQ9qGPP0czbGmvASVUlhAYRvRlisATy80trpmTHfVmu7NMrtb9eWgvLEwWmStK2zrpZUeVDmvmzZ9EMZH7EeLOnzNqS9gou/yKFx6HrLx8wi9zGeb14ogAUr4HKf2ZxpIE9hzZAy1zeFhaBSm3BYuthooZQklt8NRJVxCWVkm8fXqoUts6LVG31y/tb0qjr5LCIl6uLeoGpL3UnxC5wD54ybPowBhSE30kLaZOw+pthn1/Ra4DvMizZKkVcCfOQXG5WnON9oeS2z3IY2mV6gAVQO62dMopoUC49KOri8euD7Qllgbgu1N+BoVbm/lGmxu3ie50sfbnYhq04xpu0Ly7rt9LBQYq21OrS3PizRHorQ/M6fPGGJLXT4ZC/pw0e2S5LsQ+0J3Q/ipaUdm02BuSzjTl/qE1xdnUb8n0GiMd7YnuR+pmUPCD0G/YohPIWrqFdZ6gXXdXoOIT97GqH1EC5LaPsz0+ZMmlWoEdJpk7KMPNxmm2kSS29u/ldIDf6Y0qV2SejQWZGoRm3g+7eqiEEnGVgPl7tzO7a9JEWZOvqPdJ0SylLn81KJ3EWyMpuj7O81qnk2vwY4NxgzraMIwa85Dx1e4JjitxRrrIiUgRM1pLLFNqW3OhzT353ueFKlBILJe8X7BMZHz1C3hfLC7FxVa7inZub0fswkG5cA3nXNN6UppZw6x4y44Mrl21Cea0weq9T5QO6k8Z752YKpJP5IFjs5rj1ztJHWGrDhLSqo8cH7NXZvp8yFNC6NNaa8/JW9qf2ZuEEhyM+XsCh/bLV5KdCZHM5TYHgJ7HnuWtL4vN3vqwUlKWO820ujJxDaVRTsjRYgolOQe04hPl48GgSNuSeW98+OhgARIScgnt24+Hul39YxM6lkodYz1GEKdB2lyc9+jae6vN6dW7XD5mZzMnO3gzE0SWglUKZfEQpFzCXlW7T8/3UiSo+kHhOxt5XyT3V8N5GwNF0hlaNrp5gvV/JG4kuWTg406rPchE3w4VkNu5m+JJHcOXOpRlzZkaV42wZ8ePca982OHMFbVFseLcwDuDkoGNUV+Xvn9zfGAhGzijEXQOQLbWtqs+Y5Tt4Hj7na7daqbbB6kaWG0ppfZf8rzg3bhz/SRqwnmjyfffBhwNUubIGPbwEk3IV5kaJgSkpT6NePzk7kA3lqfAmhI4Znj+6PNTUOcHHIS8C/7uxs5a8tdWQ/aXZUkxDllUru25pmaaTE70uwQ0TKdpokEsatUI+qr8MdI2pE90Z+5iyBQbEllN5dsc14BVtqRZF9NOvBDB4PYiHmhreHiRM2vQwcqPLu6h9vnl7HVqj/+QvcHqVH7a95YnRHanXdqZIZ5WsJ3eOVouJkwZ64/PF/CR+pyztYmIZFros+XNEeipGmeNm4+8eY4x7n+fdk0QaD4XGQ5mqHE9irjG4idUMnOKUCSizY1SerXpPyQY5ZQXj86Yw5V6wNCtVbtyZT9CZVAv4Ub1e+o8lwp7WdyIt+JJOXX5Xxu0//kyF1HHtrM4+H5cmfEOSvSnKtpbvsqY/7MmIxYWQyxlUDSsbu6hCCQjcbPWfbsnzGoAICJoMeCQSG/ZgoazbX/GeD8mkggklubK4Qm+YBse335sG3HB4TumCN/23+P6iM82h45fexrdX3lnRNUQNucImF+QLAB+WcbV4NNJc5ZkSYFzjRXXX0kx1IQSZ8SErIcEwQCIA4CBedpBYE4SCLntG/T0iwjOZo5CB3nO3YvTeO3dEm2rF+TM9EB4NlV78e8tWnM89uby44mKTXRATcY1Mxf46jadt+LIde76xPUULi76c8Jsv/6vsanAoewxa73mDzNHL/n5eXmCU9uz4AkqDOlX7NSeq9pp2NXAi2gkyPnAKJLKaP5qHYkWBq1DqxFp0jDJ9GBr28ECTbyxqU4hdBrkq6pbZvuKSb6asFvsndt9dC5LlQU3d7Z/e76kujgtadW7eFqBVfyUMQ55fLP2ZBmyq5GdJ9QOk/GfNq/KSZ0qtZrg/Nn9isswkGgwdpiLh9yIk071SS3041SKapSqnVS0csbcyFJct+1X/PtzRVGk0wz0e0I8Tvry6iU7hLUH22Puj4+4Rnz3NEcW0K6tno0IDGKKO+1R2k0r93D1Tqi9eRcO340flXQRMQ5G9KkEDPN4/0lSe7psubiz4y3n/6MoMZcj/s2paQa0hwXSgG6wja6DbE8gj412N3b2/JYAOrGcniw2u3NiWOibzpC7Yn1RnsaJUUcR6pGjWY3eGOeL9syoCHYu+tmrfzdjXtOkP9Xa+WY5hTRmV3d7Tl279sxzbEYRub99TFJwIAb1Dltj8HY5WqhmZBmuYc4N9/SDvKUQgl/Zlh+fhCo8U9O59/NydH0yxaqbamb9BvpMb5UBJ03m+3XdDCoiqzKkeV35qUeUbi+fBg10W9vTrr3AHDuaarAMHpu49rK1Vj9vp2vU+nOTA+Z50+t6JMtzec05Metize4b5FrrftjgTXRtv8sj4sS50xI00KnxdllES1TkLtpv+833QhOYbDCRykdXQXEmdGhs3/4pY9D0zwWBCLPCQqsBDLkaYJA7MmSVhDIRM65Xdp9Muc0TjvdqFJKfOQFOT/zwougm+vlE6IdwQ75Nd0ljcNgEGWicyt2zFhot5tLCkh5Mu+eX2rM6TZn01/NY2ubgGumm/Xqgw2LWwK735nQQ3Pd/2uO+ZWY6ebzDxLeN/0JmbED2OzTLdlVQEKN82yzfAJIc6TlNEbLLI2c9eGxOq5NaX9mv4wyLQiUGzmn0o0kOZr2ckrKpJdsp+YTG2cuu0f28mSX48sMrj1n5mNrm7bpDjRpRdtW4zT1diqRvYSSggnWOHt4eoRszPa760sDwrPJ6anVQ2L1j3s9riyHZ477ZGbMdaNZcuY6AFwl5AFD4gydJxTCvEiTAqNldkQXDb747yPtU+aWKENKltTRF7H8TGn+5hRbwnHaaUrkPG08BSigFuzgPuybcrqkvO3gRMuCUfTbm8uB1KJhQOgph1CHAaE7LZkak319vhi0f8/xGULojvwNrO658/gE99aXgtF0oCHp4TEd7vW7unrMaoWm/KzVWB9sVs7d5vfTWuH0OHwkcAjzJ80MlAgAUaZ5bDyOFGPH8kqR2idkkpeCJAg07NO/9nWoUKqR2cW9FlyGCsPczZgPS7JZB5evycuk2xg5EhP96dX9wRlDfkDIkGrTpml7Y/VgIH+hdBMI0qq7P44WTSAIaEjwzvoS7qwvDQjX1lT9pHcKp6tHznv/2t9r/aL3Nn3QhzPPQ9F0U37Z0zApM/3+ZgWo/qhfrl0I8yFNa94xQqO0TMnZ5ilbxEnGz4maS9tIgjySTTk4zTJ3JVAvl+9fSpvd6HiU3Ae3Mii6c3vI5CYi3VzqETuvDBMdlk+RQsxENzmbwDAQdERYKrVWqKA78xzgTfTbazfQRJGeT5o+mm3tLLPf+6wmou5H0ynivLoaao5URN2Y7mNSkeZDmhQKpBmJh9pRQjq1xRxVJ+kPuNpnTlK7ZCWQjdDyyWYsKsGd8WVav2q2X7NGjY13bK/obCDQK4NiSe5Sv2YMJsoe0kC5ozNCuLW+CoBe/cPtfBQy0W+3q32MjPX5Amft65B5bhP/taWnRVrX1/g5763dgJM/Tz+i7n9PXETdN8f915xpfs0i1jE5nPMmTSFSA0BTmua78GdK+0vHN7CP7bWDQNKNh1POPHfn1ly1RrPktcvmqF49OLZ3IM8en9ACc/yaTkRdaKLz5mSruQpM9AoaT6/uAxgeA2xM9PN6MUhir3XV+Tv9fTZNfqYx0Y+PzjvN851Wg7zTrfYZRtCBJumdc3X4Se8Ds7z1cxrz3AnqeCY5dR05c5way05Rur9ZDdrkbD03O9LsCC0WAJLKCfRLPb+8Uumm+RT+zJhWmLsJcS5SI+dNXU+AMTM8ZqTbZ52b99JUklKbEHf9CS01tDEx1dfWgt5an3akSYEz0e9sTnDv/FKnmdbW9am1wqrakqT+FBPgsX947q5PWPPc4FrANL+ydE1p/5oYH6dtnlNjUCa5D7sN6+NMxOxIcwwkPsspNxs24P2UQm3TjjqzwZy0aLpTVzSSLffJNueNAxsN2I9U7nG7gzE9k2vdJXz3Z50H+wuCMlK/Zs7BbRSeXd1zdzoicjKvL11TeoEaT3nr1O3+t9cnThrS+fnK0fjes3oYNF+vWqTnaJRtjmet6bXoBk8FtFQAOF3SyehdZJwwySnT/NQj1VI+zlmRpjgABHSaaOnczF2Z5iWOB44e4StIah8DbvNhm0iNFrixHvJdL2X00ZGorlCjCs7HbF6SHpJqMMZENwT99PKBZX67JvqtzRUngt60af7eWJ0NTdt22aQx16GAZbXtzPM7m0u44+xsNNxm7vrqjPxxqaB5QtUNoerW12lrvX76kZFFEVksp9Mxx73VQn5b38cpxWxIM7Y6J9bHXfHjloVWAEk3HK6U/VqmMVJLJH2fZcyfOSznNcqKWI1j2oROjnTPA9JtcvvQn2mvBDLmeKPBLbDRC1HwZMwOQj5Ca7Zt/PM7/y6A5jp8+Nq/EsttzPw2l7GNsteonMRzbvytdgNCtokeWhEUW0bprzp6evmg24TYwI+kN/dIP5Zvrtt5mu/xtMCQmW5km/b2OvVa09vHhVYOmbl1sr1rYGuZdl8/NSkk32DMMRqzIU0HBJGV3NFoblrmrkzzSpXZrX0LhU09PIqgr5cRYymzPAZ7CeLdbfzI22jAqyWutTa7A8VN/6A8T9ukVgTlnB0+bNPIv758iLvnxx1BLReNplmpGu+sLztk5KcEXV9RUfVmbDuibmuSIZPdjBVbmw7w0XSg1yr9jT78bzK2wYcE8yTNRIxJM5LAXm8uDaSUPgo4ZSNiIJwrmUqcNSpsapoUYmSRsra3hA+Qg7l+v/XwJXzV8a2uPDWnlP0hazVJY/rXlrZKtx9/f8SSvWN+VUrT7QJBvgbYJr03BFiRyzKphPdKuSY7JfvO+qSLpHNtbP8k9bmvCgjVXoapmbYSzI80C2mZKQGg0voOq+kFHhTONHflyoM9pj40lxB5Pg5qkvwVk2xAsW+f5lkti5hKiI3ffs/1WXYkaqXwhMx8N/+WX5vujinXNuOfi4isM2vSu3XonnlucN0zy33Zp8thpN3M0ey/GYqk+ya5DwmhpvyEzY80E5F73EX0ULVOw+Pb+KZ5StSc0iBzTHNXZvoKncr2TeoFNgBqQqvk/IfSqGNOsnipDWR9jetsyx9hy8oQaufsCaCB/pvWX+oHeah7gDsqmU+mj2ubfN1w4+FmDm6mgGOWe/7Qu+tLg009JNF0Q+6ndmCJWTFka5KD1CeLUEMaaoqprnTGpgeloZR6C8ADADf3PZdEPIPDnHeFizjvw5x3g1Jz/ne01s/GGs2CNAFAKfXLWuuv3/c8UnCY8+5wEed9mPNusOs5T+d5P+CAAw54AnEgzQMOOOCABMyJNF/b9wQycJjz7nAR532Y826w0znPxqd5wAEHHHARMCdN84ADDjhg9tg7aSqlvkUp9btKqdeVUp/Y93w4KKU+p5T6DaXUryqlfrktu6GU+iml1L9u/75nBvP8IaXUm0qp37TKyHmqBv97e+1/XSn1oRnN+XuVUl9sr/evKqU+YtX9pXbOv6uU+k/2NOeXlVI/o5T6baXUbyml/ru2fLbXOjDnuV/rS0qpX1RK/Vo77/+xLf+AUuoX2vn9qFJq1ZYft+9fb+vfX3RCWuu9/UOzreK/AfDVAFYAfg3A1+xzToG5fg7AM17Z/wLgE+3rTwD4n2cwzz8J4EMAfjM2TwAfAfBP0OTn/3EAvzCjOX8vgP+eaPs17X1yDOAD7f2z2MOcnwfwofb1KYDfa+c222sdmPPcr7UCcLV9vQTwC+01/DEAH2vL/xaA/7p9/d8A+Fvt648B+NGS89m3pvkNAF7XWv++1noN4NMAXtnznFLwCoAfbl//MID/dI9zAQBorX8OwNteMTfPVwB8Sjf4eQDXlVLP72amPZg5c3gFwKe11o+11v8WwOto7qOdQmv9htb6V9rX9wD8DoAXMeNrHZgzh7lca621NjsxL9t/GsB/DOAftOX+tTbfwT8A8M1KldtJd9+k+SKAP7DefwHhL3Gf0AD+qVLqXyilXm3LntNav9G+/jKA5/YztSi4ec79+n+8NWV/yHJ9zG7Orfn3x9BoQBfiWntzBmZ+rZVSC6XUrwJ4E8BPodF6b2utz4m5dfNu6+8AeLrUXPZNmhcJ36S1/hCAbwXw3UqpP2lX6sYWmH0qwkWZJ4AfAPCHAHwdgDcA/I39ToeGUuoqgH8I4C9ore/adXO91sScZ3+ttdZbrfXXAXgJjbb7R/Y1l32T5hcBvGy9f6ktmx201l9s/74J4MfRfHFfMSZW+/fN/c0wCG6es73+WuuvtA9KDeBvozcLZzNnpdQSDfn8Pa31P2qLZ32tqTlfhGttoLW+DeBnAPwJNC4Os+mQPbdu3m39UwBuoRD2TZq/BOCDbRRshcZp+5k9z2kApdQVpdSpeQ3gTwP4TTRz/Y622XcA+In9zDAKbp6fAfDn2sjuHwdwxzIt9wrP3/dn0VxvoJnzx9oI6QcAfBDAL+5hfgrADwL4Ha3191lVs73W3JwvwLV+Vil1vX19AuBPofHH/gyAb2ub+dfafAffBuCft1p/Gew6EkZExj6CJor3bwD8lX3Ph5njV6OJIv4agN8y80TjJ/lpAP8awD8DcGMGc/0RNCbWBo2f57u4eaKJSv4f7bX/DQBfP6M5/912Tr/ePgTPW+3/Sjvn3wXwrXua8zehMb1/HcCvtv8+MudrHZjz3K/11wL4l+38fhPA/9CWfzUaEn8dwP8F4Lgtv9S+f72t/+qS8zmsCDrggAMOSMC+zfMDDjjggAuFA2kecMABByTgQJoHHHDAAQk4kOYBBxxwQAIOpHnAAQcckIADaR5wwAEHJOBAmgcccMABCTiQ5gEHHHBAAv5/LvwCsozVFl4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa240768910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa2405895d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = np.random.randint(999)\n",
    "path  = \"/media/drc/DATA/train/0/\"+str(num)+\"/depth/\"\n",
    "img_path = path+np.random.choice(os.listdir(path))\n",
    "im = misc.imread(img_path)\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.imshow(im)\n",
    "plt.figure()\n",
    "im = misc.imresize(im,(480,640))/255.\n",
    "pred = autoencoder.predict_on_batch(np.reshape(im,(1,480,640,1)))\n",
    "plt.imshow(np.reshape(pred,(480,640)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa22df91cd0>"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa23bdf5090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "img = np.reshape(X_Test[0],(480,640))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa22df25c10>"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAD8CAYAAADzEfagAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzsvXeMZWl63vd7v++kmyp3TpN6ZnY2cWdnE0lpubsitaJJ0IYgWZQVIYiGTdp0gkT7H8MGBMgWYEECBQEkTJgSJBGyTDGn5ZKrpShxc5jZ2d2ZntAz07nyrZtO+F7/8Z1761Z1dXdVdYXbVfcZ9HTV7RvOPeE57/eG5xFVZYwxxhhjjO3BHPYGjDHGGGM8ShiT5hhjjDHGDjAmzTHGGGOMHWBMmmOMMcYYO8CYNMcYY4wxdoAxaY4xxhhj7AD7Qpoi8mkR+a6IXBGRn9mPzxhjjDHGOAzIXvdpiogFXgF+EHgH+BLw46r68p5+0BhjjDHGIWA/Is0PA1dU9XVVTYFfAn5sHz5njDHGGOPAEezDe54D3h76/R3gI/d7wdyM1ccuhPuwKWOMMcYY28NXvtmbV9UTD3refpDmtiAiPwH8BMDFcwFf/N0Lh7UpY4wxxhjYM1eubud5+7E8vwYMM+D58rENUNWfU9UXVPWFE7OWQt0+bMoYY4wxxt5iP0jzS8BlEXlcRCLgLwG/9qAXWRl3P40xxhijjz1fnqtqLiI/BfwuYIFfUNVv7fR91lyXWEJCsXu9iWOMMcYYu8a+5DRV9beA33qY92hrAYBBxlHoGGOMMTI4tELQMBRlqWhzxymJKD/2f/wdTv6T/3BwGyCCWIuZmkQqFTACTtE0hU4XzXNwDlUFpzCUf9VNv/sH90mjVGToZ4OYod+tRUTAGL+thfPbVd5wJAyQIEBmp3nrz5/j0o+8wfdMvUPVpFRtj0Qynq+8iUUJZf37OBW6GtDVECOORDIakmFFCfHfMxzeDCAUIRSDRQYrBYPBsP7Ee90IN+e29/qGuVXu3IqhUDe+OY+xLex5c/tu8MH3x/r7v+0r/YkE/IUrP0rv4zcPdiOMxSQxUq2AtZ4csxTt9jwBwYAc1Q3ts60KWCOwT48dRJAgxF44y/ILp0GVyc+/gVtYRIti/ZiIrN9IypvO4Hiqu+t5ppJgpqfQ1Sau3fbvBQd+jE2tRucHniP8H25yu1nn3Sdu8kz9Fj858yWmTeVQCb9Qx5r2yNQxaZJDTan1b4q72R/2zJWvqOoLD3reSESagpCI3xSD4b8+94f8Q951sBuhDs1y6HQRa8EIWpTRZfnv/q+hyHJMjqMDLY9LUWAyJVrJcYvLfpVw1/M88d23YUMVcGiaoWtr64R5SMfctVrEv/kl+E04DSwA/4GI/8D3Hcr23A92epobP/4u/tJ/9RkW8xqf+bmPcfqXvkOxtHRg2/Ab176yb+Q9EqQJnixd+V+qh3CnGrroFKBg/QIZE+ajATGQZsTLGeFiB9ePCneL8pzQNCuP/fiYPxAiEAR0T8BKXuHf3XiKme/0cGutA92M/Yx2RyaJYxACLAGWT1YWD207VMcXxyMJESQM0EYNkzrM/Aq4hyRN/I3SR5jjPuLtQKKI/Olz5M+0eWXtJK0/OEn0zTfRPDvAjZAHP+chMDKkOZyDOKycyIZcJYBzPre51b/t84EZY4cQgwQBWokwvRxtru3de4+jzO1BBNOos3y5wpnZFV68fpZTX+7iVtcOdv/t82eNDGmCJ85Dr2AOigLrS/O7CHOMkYMYgUqChhbTTnG93t69rxnfILcDCULcY2e489GCd03fxF2tEV+5dbBR5gFgpEizj4BDbGgfukBU9d6V8nHkMVqwFokiJHfI0io8bD5zjB3DVBKWn61z9rF5nBpmXgS3tHzkrpWRJM1DjzY3Q92YMEccvkdVkLYvOuhekeaonYsjDJmeZOUpw4lKi7dbU8x8bQndo4h/lDA+I3aCcZFoZCFJ7Bv7V5pot7c3x2mct94+jCW9OEf3Uo9cDa+8eAHeePvulq8jgDFpPgjjFqPRhwjEsT9Gne7eRZngj/84p/1AiLV0TkVUJ7rMt2tMftccySgTxqS5NTZHGGPCHG2IQcIQ8sKPvu5Ve9D4uG8bEga0TltqScr8UoPpK+ne3rxGCGPSvBfGS7NHBmItBBbtpeXM/R6T3bhH8/4QQaoV2meUyBZwMyZ5Y/HI3nTGpHkvmPGueVQg1vjjled7TnDjdrMHQ6xFJhqkJ3MUqF4zsLRy2Ju1bxgzw1YwBhHxFdkxRhsiSBQBoOk+LAnHfboPhFQqtJ8+wdyZFa5fm+HkV7voAY9NHiTGpHkvGBlHm48KohCy3PdmHtEl4cjCWMxEg+aFgCjISd6KiK5tIZRyhDAygh0jgeFKqbVgx7msUYdY64tAWbYu4bfXGOc07wkJA9zcJK2zQtaNmbziYGnlyBaBYBxp3g11fuZ8vDx/NCAGonBddHmvMY5c7w0RTBzTOV8nvdxh9WaDxls9tN050vttTJrDGD7QYny0OZ4IGWmILY/TOPd48BCDTE6w8ljIydlVGq8GhDeWfdvXEcaYETZBnfrcmPG6gGOMMEQgDFEjsF9L8zG2hgimktB+7jTLL/gm9hNf66E3bh/pfCaMSfNu6JBa+1jdZrRRysHRnzsfH6+DgxjM1CQrj4dcOLvI7fkJ4rcWj+wU0DDGpHkfiDHjC3FUUZrhEYW+y8GUqZS9zkP332+c394AsZbi5DRrj0E96hG9WoH5xSNdAOpjTJpbwZWuk+OWo5GGhAESRWgSIrUKEoV7m4MeEOb4PNgMiUK6Z6vYJ9fo5CETbyqu0z3SBaA+xkm7zdhQDJLxBTOiEGuRSoI2quRTCYEIptuDosClbHSW3PGbywaL5HGB6W6YRp21MwEnJtZ4+/YMl95JvTHhMcCYNLeCOu8vE9hxXnMUYSymWoWTs7SenGL1UoDtJUy8USF5s4pdWPbukWm6PeIciijF2kFFXqz1+e00PfIV4R3BWLInTrP4XiV0hsYfV0i+/Tr5HngyPQoYk+a9MF6ejywkDJCJBp1zE8y/J6DzbBcxSvtkhZPRHNUrBrMguJXiwba7/ajSWiQKkWrVa3Pa0hu928OBj6L0eJDCfSGCiUKWHq9QOd/k1p1JHv9OD7eyethbdmAYk+YW6Fv16jjKHEmYSoKbnWDpmZjoo4v8+ONfJzYZv3f2Oa6ZC5wqpqlmOdJql0vr+yzVy2W4WIPUa7izJ2g9VkecEq3mRLfWkDxHOp3xYBD4/dVosPKkoZGkuG9MEr91A3eMIvGjQ5rGliN1Abiybago1r3K+xfNNrQyhyvmIsI4ozVCEEHqdVoXGzS/t8N/f/mP+VPVVwlxXIrm+Xsf/GFW7kwRL04g84tInt87QBw+F8IQGjVWn25w41MFkhoaVxLmXhSSZhuWj65qz04gYQAnpuk91YV2wsx3HNxeOBZV8z6OBmmWntemWkWmJwHQVtuPc6XpOnlSEqIYxJqtiXXoQhKnnjBFjkVV8FGABCFudoKFdwd84qlv8dHKa5y1BQ54NrrFx89f4XcvvEB6JSKxBhUD3OeCVgf4PKZWY9bOGz75vpe406vzYnCRyp2Q+K3ogL7diKPMJXfPTxBXeuSv1Zl4eenYVM37OBqkSVlNrVVpP30CVKm+cseTZv/fh5fa6pCkgmk0wIgn2E53vTHXlm6Yzo3zmqMEYzG1CkvPTVL/07f5C7Nf5EKQkYg/XidMzvdPvMLvPPkuel+vkDyo86F/kyxHZvOJhPZp5Qenv0XLxaSF5dprl5iujEmzPwGUP3uRt34ooFgULn0+L32AjpZF74NwNBhBdSCykVcMNnVou+P1FbMcLYq7/rhOF/IcN91Az53EzEwjlQoSBL5pWsxdkecYh4hyNSG1GquPGd47e4PTQZMQwZSncSjCrF2jXu3hAll3Ed1OFBQEuNhS1ApOBKucC5e4VF8kr4DaQ7SUHhFIEGKmJll+ukoxUVC/ElJ9Zd4HGqMWZe7zNTuSkWaxi4x7f/RRnBI00wFp3uuAapriVlaRWoXs3BQmDgjy3DsZgp80GWM0UE7/SBCgEzXal3Kert2kJjlG1gmtf9aoCuIAp9vrsTSCGEMRGaSeU5OUQjJOx6vkVQU7vnFKGFCcnmbtgiA9w9w305HNZco+3+RGkjR3BeejzaDjMM2ur+bd7w6oiut0sLcXkBMTpNMxpj2FabYgL/yyfBxljgbK5bNUEorJCtUTLU4ETWIBy/oxSlVpugqtTsRE25XtRg+4AYtX6feRphAlGYn4Ju3poEVR9V0Ux/pMML4dK2vE2C40XrdUv3Odot0eySizr+S/XxhJ0szvl7i/F0odTNspkHbXKxU98DWKa7cJby6TTp8km6sSBgbppOAcMlbOOXyIlC1BPmfdnUt45sQbnA5WiIZual0taKvweu8kxY0q0WrvwedA//XGQGDJKoZGtUcsBQ5hyrbRxKHWHGvSlFLxy7YzJq6GxEs5bnF5ZKNMMzuzr58xkqTZ1Z2PY6lTtNvDdvMdecVonqNLy5h0jrXzEdFkQLSSY3oFwUrnwW8wxr7CN51HmIkG+Zlplp4J+PG5l3kiXCREKFAyzblVGF7sneUXXvkYM98UwttruG04U4oRsBYNA/KKMFXpYEVBIZQCQsexZkwYFEaD+SZT8010tUnRGsEoEzCNBm/+lYv7+hkjSZrZbrqI1fnIIh+yrNjuS9MMkyudOUMRCmqEsGWwneNVFRw5iCc0iWN0ok7ndELz6Yxn4xtURQEhU0dXldeyk/za/PeQvTzBiesZstramZK7NaiF0BQUKjiErgvBjUU7BjnCdgfNMrTTHVkLkPR7Huf//Fu/sK+f8cAzQUR+QURui8hLQ4/NiMhnROTV8u/p8nERkX8sIldE5Jsi8vxuNmpxN8dDFc1zZDd3P+ejid40pJNCb8KQ1SwuGldNDxVikChCJhtkpyZYejrgmcvXORs0iUTIUJpOeSev8Psr7+Y/fvtJJl6D5Jbv0d2u0MawrUnhDA6hQOhqCLlw7Kcbym4SLQul257pPwRc/76ED8UL+/oZ27l9/j/Apzc99jPAZ1X1MvDZ8neAPwdcLv/8BPBPd7NRTbe7yrUWhSfAnb5OFTVC1nDkVchqQp4IGtqRPTmOPMoJL1OrUsxNsPRMQvHhVf7Cma9QFaVQZdnBd7NZfrf5Xn7z2+9h4sWIyTd7mKXmjgQ2/JCDQwpoZRGZGjI1tF2E5GZ3N+KjApGyDc8M2vBGWfXpb/6l32XaJPv6GQ9cnqvq50XksU0P/xjwA+XPvwh8Dvi75eP/TH3/z5+IyJSInFHVGzvZqIWitpOnr8MpUuiOlYlEBBcaXOIoYkNRAbcmqLArEh7jIdEfiY1CdGaS5hN1Fj6U89cuf40PJlcBWHSWF3tn+Te3P8hXvv04018NmLrSI7q24j23t1M578M5JM0IOsriWpWuBqRYlrIapiP+nILjORkmBplsoIFF1tqo2ycDuz3CX5n4Bobqvn7GbnOap4aI8CZwqvz5HPD20PPeKR/bEWlez6d3tVFaFJDliOnfFbdf3SsSQWo5WdcghSFaLccoj9tFcpjo92NGEVKrwYlpbn58ls4n1/jp5/6IP1V9hUmTcSWb4F/Mf4zPfe59nPia8vSbbez8PNLqoJ2OX0Jux6emHzXlOdrqUL2dcetGjdtFAyuO5byKyQS1x9dKw9SqrD5/Bttx1L7+tt+vI3xNnLRV7D7nnx+6EKSqKiI73osi8hP4JTwXz/nN6De1v5PusmVAHVL2WIqR7d8QjSGPDUGck1UCXEdwFmQXRaUxdomyv66vH5CenWLlqQr66SX+xhNf4aOV1wjFccfF/Oz1T/Lt336ai1/okbyxgDZb0OvhisJf1AM9gW0cu34BMUsJV3rE8zGLRZ2q6fmCUKS42BJY6+fYj5M8nLGY2WmWLltmvqu+CDTi18N+EybsnjRv9ZfdInIGuF0+fg24MPS88+Vjd0FVfw74OYAX3p9sOBLN4iFyEmm2c8V1EYpICMOCLHS4wIL4SLPc2N1vzxgPhggShNjpKYozc6w822Dh/ULj3Qv89OXP8UR0GyOO17M5fnPx/bz6b57m3BdbhG/ewq02B0QJQyrrO1hCqlM0yzHLLWrXG1zpnuLx+A4Vm1HMZPSmQ8IoRNIU1eOzRDeVhM7TJ+nNKGGz8FXzYyI0fD/sljR/DfjrwN8v//7Vocd/SkR+CfgIsLLTfKZDudaZApZ3vlXq74YShjvOa6qBwDokdKjBV0wLHen8zZFAP8KcmKDznvPc+nBM8rF5/tqlb/KB6puctqs0XcLXe5f4J9/9OPLZac79wQLcvENR5i7v6sndCamp19vUNENW1ph4I+WP7zxBeLIglIK5E02aZ+do1OtImpVjtg9hpfGowFjM1CS3XohAlfidFVw6bsGDbZCmiPwrfNFnTkTeAf5XPFn+axH5W8BV4C+WT/8t4IeBK0Ab+Ju72ajV7CEizV7PL893qIPpSbNArIJRRAVRPfbdJvuGcjVgIi8EkT1xmjd/NORTH/0GPzrzNS6H80TieDuf4LdX38e//uKHOP05y/Q3F9A330HTrOyW2JvIR4sCbbdJ3lrmypfP8rkX4OnJ25xprPLSMzOcPDuDzTIMbByeOKLkKWFAdvEEnWe6TP1JDAtLx07N6F7YTvX8x+/xT5/a4rkK/ORuN8ahOBw3mhPMcHNX76HdHoTRjiXdxEFoHWLWLwIdz57vPfrWu9ZiJibQU7MsvneK+R/t8vMf+XmeC1eoGkumyjt5yP/8yn9G79dO8syXmpg3ruPWWnvfJ9iPNns9uHaTJ/5NwuJ3z/IHz51Fz3cITrW5/qcbnPhmTHJ1GVleXReEcdtbjWyZNhhVwjUWe+YUV3+wRn1yiVP/sYtbaY7u9h4wRm4iqFBlZS1ht9OjWjhkp9GHKuLAGocw1Mt8fIdAthYredBFc7/XDDk8ShAgtSru4kmWn21w+wcz/tv3f473RqvUxYsttLXgO+lp1j57inNfbmKuvI1rdXy0sx8Xr6qPHns9zNWbzHUyqrcnWXm8SuuC0jnjWEwjJqozVN9JsItrvgCVpWjh1ufcRQY37H7TvBYFUhS+E2O4kDJEoHqPxw+DqEwU0ntsjuK5NbJ2jLl1k3wcZQ4wcqQJkC0/xPJc3fbEOobhHCZXBBDjcMYv14+tAPG9Iux7Pn6f/TT0kr7To1QqyOQEzScb3Pkg/PC7X+KTte9QlxArQqYFd1zAbyy8n5Nf6/kIcz8Js4+SOF1zDZNm1FbWqFybpH2hwfKTAUUMqxcCskqdykJCfD3BtLtImvm2pcL5JvAg8MZsxjeDS5b7nGmfYLfaTf1+4P73G7JsgXXfqn0nURGkXmPh3Qmnp29w+wuncavjKHMYI0ma4eLuxxfV6T1PzPvB5IoRJQwLcqulTcIxwzApbv7+Wy1Bh7zBEXN38W1Te4pYgyQxUqtSTNdYftJw4T3X+ZHpr3PaFlixFKpllHmWP37pMu96awFda+0/Yfah6rVW+1Fnc436fJ3k9iy9uYTelKWIhfbJABc0CJsV7FqKSXPIC1wY4KohLrJoYJDcES51vPJWGvruDvAanuVsvY/Ct7ghOVcWupwn5TyHLEOLIdm7Pd4nEoRwao7l9+RIL2L2RbfuaDAGMKKkGbQeIpdYSsQB257g0MIRth1hkFNLUtqJ8ye9PSY6ikNLZ/97SZglUarTu0h04LUUhd5sKwi8uEY/Oi+1ACiGJkjKZXkx56NM93yTv33pj3guWiAuCbOnOa9mFX72jU9y+g8t3Jr3VduDjHQG215QpCmstZD5BSpBQLVahck6brJKNpWQVy1FpQKAC4TOXEB3RsgrEHQhWVCmXnHYvChXMsaLgwReWUmjABcHFLWQIrbkFUMRC0UouADf+lZA2FHCVkG4khGspZi1DtLp+dxqp+Pnwh9kV7wN2Nlprn9ilnOP32Txj09z5svXyLczKHCMMJKkufNW+a3eZAd0pw7bdVhx1KKU+aTAheHxEJ8d9v3uLymhXB6KH001bkPObdicTpIYSRKIQjQKcXGAWouoImkOaeYHDlTRMKCYrNK+4KPMTzz2Kk+Gt+krDWQULDrH7zXfy60vn+bSm21cr3d4bV99AtICTR2a5UivhzSbmPmYZGoSrVVw1Yi8FlLEAb1JoX1WyesF0ZIlWsGPYepQs30/sjQGDfyfPLHkNUNaN2R1IW1AOq0UFQcGJBVsNyBcDYmaVeLlCeJlR7zYI7yxjK610Xb74boKRCgunGT5vRmuGzP3zRw3v7h3+/OIYKRI0yBYEYLWQ77RDr191Cm2k2NEqYUpQZTjQo6273m/it2PFqPI5+H6EWWe+z9ZhhY+17v+2nIpbsr8XRiglZiiHpPXI/KaRQ2YVAm6BaZXIM5P1nRnIlaesLSfTPlw43UaxgtrZOrIVHk1m+U33no30y8r4c1lij2InvYEqp48VXxhJ8+RNEPiGFutYKoJZq5BcMr6SmLkyCsGteJn2/OijLoVyqhTRQa1xqDrVzY2UjIVXAj5TEYy2aOa9FAV0jyg04podwJMyxKtBERLAZNXYyo3u9j5JmbNKzy5TnfH5ClByOK768ycXaD50ixnX1tCO2NN2c0YKdLswzxMoW6XF5jp+CVINUgJwwK1HF27i/6Mdxz7SLFSQSdqaBSgIohzSLuHaXe9o2evh2b5esRX2t4Cg7ylGoOGliIx9CYNeSw+QioCTO6jrSIWunNC62LBu5+6xtlgCYC2glHljov5nZX30vrqHBeuttHm2uiN7ZXnlxYF2u0haYp0OkgrJuymTDRCiihiTUNMDrYH0itKrdfcC14AkgmS+YKR7VikV2BbIUEnwvYCTGbIGiFdq4goYVBgjCOpphRxTpqEdCsBWcNQJAHJbI3KYoV4KSNcbGPvLKMtH6lrlj+YPEUwE3XufKRgTmD2RUWu3fK53TE2YORI02CwvYe8UJxbt+HdDtRhuilOhcgUBNaRWnyUcNRQjiyaSoI06rjpCXona6ydi8ir4ELBZEr9ekH17TXMHcoCRL5xiU6BApLn0EuRwGJCSxh4R1A1QlaDoi4UkX/fIlbSuZy5C8s81biDw7DsYixKiuXL7Sf4/avP0LiqhEsdH+mOKoYjT6dI4e1RkqsJk3Yakwe4UIiaBdLtQS9Fe+l6NRx8tF5G7abdwYQhdikiWkhIFhKCTkznREx3LqJXVVzkIPTDF6h/EzWQ15SeE4qKpTNrCLoRlTt1kltt7PwKbmUV12rflzjFWjh9glOXFrnznTkuf7eJW2uNRpQ/Yhgp0rRiQB3Bw5ImlMZo2xRYUEU6PXpFgFPBqZeFUyOjF+k8DPqEOTUJ0xO0H5/m1oci0nd1eP7SK5xOVjkdrfJOb5rP/s4HONerUplfWZdZG+4rLADnPZYkTZFuD9NsYe+EhDcStBKRTyV05iI6c4b2KcHFgMBKs8KfmMeY79Ux4lhNK6ykCVevzZG8GlO9nZcaAqXwyijPew/I00HHIbcXqDpHtDpBkViCZurVl/qW0sXG/Qisp0TMeuN/FIXMvVFDqwmuGuGSkLxiSacCsqohr/jCE4DtKWHbYXJQge60MP89AegkjatTzL7UJfr2O2hzDdft3U2exmKmp7n+qVk0XeL8HzjMa+9QZNvXJD1OGCnS7MM8ZIChqjsv4GQ5aZHQLQKyzHoL2KOGMn/JVIPWM7Nc/1OWJ1+4yqdPfYvnK2+SSIZF+VLwOH+YPY9t52i3O/COv5u4fHGEUl1IOuKLQGsBEgRECwnh7SqV2RphK6E7bcjqAXnVshBXuV2bBQemJ5hcqKwIyYJiU7eeLxXjK4Ojri5UysxpmmLaXexKhOkFmE7mybLs49zaIbPfGG98scmIX/p3ukgUYoIAG3rRkKRWwdXisvDkc6YmV0zqvCqXCEEvZPUpQ/zEKu2Llt50nYvNE9jrBtwyLmWdOEUwUYg7f5LV53vYV6Y4+/ayjzLH2BIjR5pWDEH34BlL85x2L6IVxuRZQFRwtAy1RJAwQOo1slMT3PyI5QPf+wp/+dQXeCa8zazV0gLX0HQJtetKuNBC+wWFrSK9/vhhGXX6Z2RIr6yur7WQ1SbRUoXptVmy6YR0IiBPDC4AFwy1MQmoKEFPcVbQSoSpJki7DT032tFmH+VghaaZX5KDj5izfOjGc5/eSi1AxO9P8c3smuUbejplLcYmCTaO0LBsogdvOw2eBLMq4VqdiWqXD514i1dOnGT+xkVOFIpkGbLS9JEx5cDB5ARLzzW4ePYmnc+eQW4ujHOZ98HIkSZA0N4b0tyRpmae0+5GrMURRc8iR+ycEWsxcQxTE9z6cJWzH7nGf37yS7w/uknDCIlYDAUrCC82z1G/liPLTVyebxEZDWGoLcd/UHnRU6CFKavMKZLnxEtVojhCk3C93cZ4WxEX+SjUhV70N69HhL0EqST+M/oiGSNOnKqKuALJck9yWb6+D7fTjD7c5qQCFD4CBcT4BndJMyTwvbHehmLIijgMMN0IZ5VT1TVeqL/B+2tv8/c+cJ7KQoOJ5TWk3RkcU4lj3MVTzH8AJroxc1fa6MrqyO/nw8RIkqbtPCRjOd2xyhFFQdYNaCcRpAZzlEhT1ue905MN2h/s8DfOvMiz0S2mjCEUg8EABQuuwhffvsRjt9toq13mM3cqtdb/eb1QYtZaaC9d95uxFjPcspREmF6FvBqggcFFlqKREHSGrE8eEeKk8O1FkpUFtJ2IIg9j+IbUvxm5cn7dZr7Y2f9jxNuEGL9kzyaU89VlLoaLGHGcefIOrZdO0ahVkCDwaQBrkHqNlSdqVJ9aYfmNac5cv0Y+loC7L0aSNE128IylWQ5rIa04wrQNJoUjowsnvgndnZzm5kcq/O33fYY/U3+ZU9YRl/PehSpdVb7cfoLgqw3srasUvd62/ePviXIJ79IMyXO/S8uKsfTFLcrUQbhUIUxitBL79qfA4CZrPq8Xx4PpFz8htGlWexRRFGi3uzO/onth8D0dmjsfxZvc55BL4pRAgZiiHjH52DIfm7jC2aBJocJ00mG1JrhqhIkoe/EIAAAgAElEQVRCJM8xEw2yJ89w4wcccRpw9o+U4ubtsdDwAzCSpCm5ezi+6p+gO5kfV0V6QpFbTCqYbEi5/RGHWIvUaqQzFdaeyHl/5S1mTE4iAVYEgyEjo+kMX129SOW2+qbmvYrqhnOf4iNaCtZn1Ut7WMlypNOBdoyJIjQOIQohsGg1GYwgmrXWoAqtm6XZDplEN8yTF269VWuvtqs/uKEOdWa99Qt8ftMaetMhz5/y01aJKC0VFjpVbL8YLsZHmxN1Vp6oEM+ukb9do3qt44OHMe6LkSRN0+yyJ/e6nUz0OIdJhTy1BBnYVH018lGHiF8OhwG96ZD6mSZngxVqYgjFlstyKFCaGvLy/Clqy17JfE/brYYu9sFD/YMs6km0X4VPM79sj0I/ohkGflY7DtdHOLPUX+B57otQg4j4ECNQKcdQjbd+1iwrjcj2+DzaTJyi9Ns9NAxoz1o+NPEGp2yHEGhrwO2FCaZb6nuYjSf2fK7O8jNgjTL5ihDeWiEfR5kPxEiSphzC3U5V/SRSarA9wfZ03br1UYfxAhG9CcO5yRUakhPKxkPvVLlTNFi63WB6Jd+bJeVm3Gu8tS8M0s/ZFYXPuaW+IdyLgoRlC5IgcQSBhdj55vqyYu2nlsreWuFgl/D97yXiSUkV+nPg+ypn5/x31lInIQrpzgnPJdeoGZ92uVlMwJ2YuKlIVvh8fxTSPRGTnsyRWzVOv5aiSyv7t51HCCNJmjxkIlqdDnJm2z5dnWJTQTLBphB0FZMVRyatiTGkDeFCbZlE/OSV/09wKAXKzWyKYDHEttsPn8vcDrYkEzdoufHivQ5N+wWksugRhYjxS0zC0L9PWOZM8xzJsoEW5UESqJiyrcuYgazbvrmZDt2A1PlRSwBXjeiedJy2LUKELso32peo3DREKymSFYPCYOu0xdY7xC9VieebYwm4bWIkSVN7DzmJoKX1bj+3tE0rV9sF2zYEbQjXCqSbD/rZHmmIgDVkDbhUWSAUL4wyDAcsFjWiZcF2sr3Nww1jBy03/tcyCk3Tdfm64eJHWYnvR3gS+YhUynNgQJ4lge2XDiXit0OSBIJg0NC+rwpNmyP3MKJ9tkLjqWVmDBgR3s6q/MsrLzD5hiNe6EKW++2bqLH0nKLLESe+liHX7lB0x6S5HYwkae7ZzPEOlddN6v/YrmJ77lDSBHuOoWKYC6FqUuw9uvYzF/j+1FGqSG/RwkRRoP0Ry/L7DWTtyor8oBkcwBrEBZ4wcxnyRd8j8iyJS6z1ka81ZZTp9jYvvBXKIhsmQJKY1mnLJ8+/QiiGTB3f6p2nfa3OiZUc08mQwkEYkM1U0WpB8nZIcnPFV/nH+cxtYTRJcw+WhjrQLtze/Lk6xaZK0BbCtsN2C+RhI95RgjG4SInvIyEVmtzbfIyyutNWzd+AFiV5loUiLb2IBgRKSWqwXtkuZd42vO8uIdYiUYgmkSemg7zxlHqobrrOylPwycmXAWg65Q8Wn6X2liVc7UIvBedwjRqtszFQULum2KXmOMrcAUaSNPckn+acP5F3MBVku/iledther4x+dCw3bTCg96mFIFwlZCsriSSsVWm1gAN06VIQMOyWXqPtmHfsCkK9Q+tT9FQFOsE2m8FAk+k5XN9urOM1nb7XUtBZsLIizGXN1s9wH0nUUjnbIPpdy3wbDiPU8N3s1m+eOUxTl1z2HY6WDm5uhdRkY5Su1Wgrc7B5LCPCEaSNPci0tzNBRB0FRf6MU7p5gefGB+ytwUG9rDbqsAOR4fDuS5rIY7JJmOYTknKSLNQxYi3TQawCCeCJtmEI6+FhNY+mkWwe4whYsrosk+gfYgplekNuyJOkfU8axzhAoPp6IGqY/X7cJefDPnzF7/JpBGa6vh/5z9E7VsJtetdpN3z3z+wFJWAdBKCtiFe8nqph6aO/whiJElzN8ZoG9+grLz2VXK29RpHvFwgagmaGabdxfX22F/7PpAwwtQqEMdIJUED65vriwJdWfVujJvbgIYscSkrthv2nRFMtQpzU6w8HnHm5C1CyXGAw+GQQZ9mKIbHwnkmn1yieXGG2W/VvIviQRma7QeGCdQxsPbwc9wbUxB+RbJD4izfzxeAykmm0oFyUIzcb4gvfvUun6b9p9f4kcY3sAh/0H6cP/rcezn/1R7RjVWk4wMADSx5xZLXlOS2ECy1y4LVI3qMDwEjSZp7gl2csEGnQAPBtlOf/8kOaAbXWMzMFEw20CT0Kuh2fXlsRTBBgHa66+6E5bJbotATbRCU/Y3ZequLEWSyQe9Ug9Y54WylTdeFtJwSlvYVYV/rAcMJm/KJc6/y25c+ylyjhvRbj/ZKlm1zrvSgL9RS+3JdSegeN9TtpCX6edK+Bqaxvnf0EGAadRbenfCXn/0cMzYjQ/j5N7+fmReV+GYLaZWCzqX5XREb1EC8pEg3HSsa7RCjSZp7sVTov8cOpoJsNwcB6aRor/fwEe92P7dew50/QVGNwOCVf8rtlkLBGEwlxnR60O2tk2YUoUmExpF3zsz92F7fyAwR8pMTtM5EpFOOtLC8k87ydjRPRouG5FSNEpbWFVURPjn5Mv/28veQnZ4kaLaQNPNtPw9DcPfzUT+MCGdojttvxxbkuZ1imJRC1yJIYAdWIQM31P1Gmc7JHzvF6kc7/NnGi4TAm3nE0r8/zcVXmpilVV8Zpz/iaUC842uyXJTFoXGUuROMJmkeBlQxbX8CSS9Fh0Uh9hMicGqO1qU6eSzYVHGBeDlF8X7sYWIJqgGSVTGZg7w/MufVgDQsK8eZQ5z6ZX15IfRmY7KaYLvw+q052lnE7bkGTyZ3uBzf5FKwxKzNqIolFMPlcIEffte3+PLjH2BmYRLT6+Ha28yrbv5eG37fgpjUHW6xaYvRzm1Bhir1g0iz/H7F+r73edQ93N6tNiUIuP69dX7yA7/D2aBDV+FXlj/I3Es59vYy2mqVikYWLcc70bLg2Sr2v5f0COLIkqb2G5t3AOmkGIf3c0nTA0nmi7XeQ7sqFFE54WH9KLGz3vtajcEFIVLoQETERX6JJQWIqm/V63sa9aNrpxSRwWQQLwh5t8bNSpVfnTyBVHNOnVzhE6df5YcmXuI9UZNQDFMGfmT66/zee56nemuSSruLFIU3V7tXe849I0kz9KNsJM5ybnpAnFu970HgQc6l91jCD2bgNwl0SO77Mw8mn2mQRoPsY00+XvsOAN9IT/OvvvBR3vXqis+Fd7q+qd/kGCNILyNoF1TuGIJm9tDTd8cRR5Y0ByKrOxillG7qx/Z66YEtzbUosG9cZzo/TTad+PYYp0jmyaQfNZoyunShwcUWF3oDOpOW0WWhqBVcIAPvGFSwPUftlqM6D7bjMGnh0xCqFNUJPvPU9/GvPvJR/rcf+GU+lLxFwyiXwwU+/slv8nn3Ps7rKZLXDLq84i/AfmP4fTAosgwTzqaqtar40T8d9mHatM8PS3Tjroe2INVhG2OAPPdq7WmGdns+gtuP+f3B5wumVmXlE0/yT5//OaZMyi8338PP/taf4/KvdNC3rqOdznoTvxgcXUyzReWdmHgxwt5Z8QLJY+wII0maexbh7TS3lPlKsQ4rbe83VCkWlzCtNlG95gs61vpt1zJaLveHWIMt1X/CRsU/lq0vmzUK0CjAheVkTDl3bdICyR3SSX3qoYw+bGA5cbtOdX6Gf3Dqh/hvnv0cH07eoGEyPjX1Ml9+zwUWr89woj1NqIpxuk4GmyXZKGegNxNmPzc7rC7unL+ZGcCZgae6r16z/r73iwD3ilA3tR+t/7iDBv+i8L2ZWWltURqo7flKpZ/KKA3y5MIZrv+ZghO2xZ90L/GP/uiHeOJ3UoIr13GdzqaUirfO0FYLueWwQTBwx/Tz+SPekztCGEnSPCxonvuxu4OuJqriul1Is7uXsVtcvIMRwb5id1/UNwgwxmCjcEBOWFt6bhe+uJWWcmXOocYg3R71LGf582f5J/Jx2pcjPlR5ndPBCh8/f4Vffe/zxCsVJospgryAcjsHY4JD7oobiKaMbvpVfFWvpn/XjcwIlMt06bt/3quqvaHd6gGkdi8C2G4qYfCRW7+PGAdFmQYqisFNYaDzuUeTRndtd1n8MZMNbn3/LH/tI5/nO+kp/vev/ydc/E1IXr5Gsbzij/Ew+mLQ3R6yaXvFyJ41SBwHHH3S3Mn8ueqBLcu3hCu2Dm43XegKdxGLDJNnP8cGSBj471Sq7txlIVvqK5786jTXk2n+uf0wPAHvTd7muep1vvr4BW7Nn8amNSZbKbJqIEh9jjMr+/tK0tvgi26GCixle49uTpQYWc/9DYor99v/W7f0bCC27USpw9iCoLckyrui6v7rCkR9CmiDIPJ+RG1lX66pVum97zFqf/4mH669xj94/dNM/UaN+tfeophf9L21W6FvN5y69Zn94RvVmDm3haNPmjuBUxi+hmSXUyJ7jS2dIDee4INrWmTjBbGpC2DDhS3GT4l0e4TXV5h7MeTGxCy/XnkfJy40qZke75u5zm9fmmJ1pUL1VpVQFWmWS3HwRLxF1vhu4inuXvIOf4U+eW0RZctmAuxH0VBGsLpO2rJpiX8/PGjw4b6GcluIKe/XeVI20YsRJI7Ri2e5+sMhf+fiF7Aotz9/lse+eBs3v7C9YYTN/arDn7ObDonDvj4OGKNJmnuZS9yJ+IQ6P0McBP6izPPdj9dt9fkHcXL1LwhYn4K553O9aAVFgbS7VK61mH2xwZvxWb4w9QTPVa9zImpy7uQy187EtM7E1HMlKBziHDqkinOX7cRWH7c5kNmwJB4i8iH/ID/ttB5FA+tCxUWBbCbtnZw7/RTC/V532IQw3EQfBHD5Elf+8iT/5ad/j9PBMv/w6g9y6VcW0bev43q9nW3vXqYODns/HSBGkzT3CjuUhgO8tFeUr4unDesx7uTEMHZ9JrmfPxoWxj0oGa77bfMmQhVVgq4SrVrme3XaSYwVRz3q4aqOtG7J6gF2LUS6fQ3LMlrdzudt/syhiHdDEQn8+1oLYeD3Zenv7VMN+ZBO5qYc4na2YRiPwpK0P6pZq3Lrw5N84Htf4bnkGl9sPcnbf3iRx258x6sUHcaE1bBi/TEhziNNmnct6x74AuN9aFQHLn9SFF4AF7af3C+T9RIG69Vwa3w/Zd+aYZTsaI0Z+JCjYHqwmiW0XUSmlsgUSFKQ1QOyhiVcjZB2CN1djA0Of9/B8s5tuVT20aYdeHurKwkyzyHLfIV6v0SFRwhSTn8xN8PS96b82dlvcSef4J9/7aNc+nKGW2sdnhbmYEDg6O7/zTjSpLkjY7Xy+VqN/V09CyHM/MyutdDpeFItCu5p3jWk2yhR6E/0vr9NFKLWIIXzbT/NNVyni2YjpNlZtgGpgU4e0iwSnApGHDYqyOqQ1g1RLSBYDb2j4T5ob4rIBkL1wsPOF8rKG86eCwmPIvq5zCDATDRYec8sn373N2jYDr9854NMfDWm8tpNisMWyz6q+/8eeOD6VUQuiMgfisjLIvItEfnp8vEZEfmMiLxa/j1dPi4i8o9F5IqIfFNEnt/vL3FPWOvzk9uFCEU9ppisUMzU0YkaWq8iSez/hKVPzdDM8YblCQzZMPjPHhBmEqG1BNeooLUKUvPvOxKCv/156dwhzk8jtbOQ5axCq4gBiOKMrKGkE0Jet7gk8AIVu0mBbMb9CjKq/saVpb5pPPUulN6+YkQi9X2EGEGSmOL0NHeeN3zfxKss5nW+8MrjTL2awdLKWHH9gLEdRsmB/1FVvyoiDeArIvIZ4G8An1XVvy8iPwP8DPB3gT8HXC7/fAT4p+XfB4+gXB5vF6rk1YBsIgCFoFMhaOXYtSp2peUjxFbpud0379pcje63/ISBVyDq28/2l78AGiJZjJS+N4eZVxt4ARWFL+4YcBGExtEpQnJnaWYJqoKLlCIW8sTgogD7MITZl1Vj0wTRcI+kqpenK4qBve+B5oMPG2KQKEKmp7jzwQne9f2vE0rOv73xAaa/GFF97Ta61jqA7TieVfJ74YGMoqo3gBvlz00R+TZwDvgx4AfKp/0i8Dk8af4Y8M/UD37/iYhMiciZ8n0OFFJOz2w751IUaGBI6wYXQJAIsQFUkSLxd/2i8JMfIr66TuH7FM1QpFnm4gaeNeDVb/pOgLmDvNjfMbudoD+nL4ILhaKiVEOfNljLY1a6Cb1uiO2I91HK1Vs67FbRZ6iFxv++cXpoOHodEGZ/wmYU9tcBoV8xL2bqrF2AD0+/yfVsmlevneTsHYe0OriD7Cs+6E6QEcWOcpoi8hjwAeALwKkhIrwJnCp/Pge8PfSyd8rHNpCmiPwE8BMAF8/tQ2pVDAQWTaJtv0QLh0kdKt6ELFfBZgYpAiDBWot1iqQBKl1fYRdZnyAqSVIGLokGtevESaHeJbE/n5wdsvhr35QLX+F3oSWrCHndMZe0sKL08oB2L8KthlRWhXBNCdp+hp2+F05/5HQ732WYMIciy7uKdqp3N+Qf5fzlVhADcUzvRJX0VM5k0Oarq5cI34qp3Ol6ybdjdBMZFWx7fSUideD/A/47VV0d/rcyqtzR2ayqP6eqL6jqCydm90e8VaMQjXag3l4U2E5O0FNM6nN7KqBW0NCgsUWDcundr4qXledhJ0SGJnL6S1/yAkkzpNPzorBZeveo22HCWopqQNYQZDJlLl4jEEcri2ivxYTLlnhRiVcdYStHehmaZTvzltlEmGJNeYPZtB/76M/e9xvXjxNhgk/1VBM6c5bGyTUsjpeXTpHcEexqevDjvoNcvmEkcvGHhG2FeCIS4gnzX6jqL5cP3+ovu0XkDHC7fPwacGHo5efLx7aPPcjzibUQhbhKuP0XOUewsEbdCFk9GFSTpSgFMYz44kdeMPDZFvEV5PKCHvRkOm8BLFkpcFG2zGier7ccpSNUOQ8D0kZIbxpOzDY5E61w1c3STkNkMSJZEJJlR7RaYFuZJ80890v7HRDaMGEObjqwvszvv49zG/taj1tEJX5prrUKnVnDiXqLtou5PT/B3JJiuvskCrIVhvoxB2OX5TYeuxsZ2yBN8SzwfwPfVtX/a+iffg3468DfL//+1aHHf0pEfglfAFo5lHymNbhK6E3CtulIqXmO3LxDcPMOQb/6ncTrubbCod2uJ77CDUQrNrxHUXgi6PXWQ++hkT9KgYeRKWaUS2StxLRPWroXUj5x4h3mgiZXmWV1rUJyy1C74UjmM8Jmiml2odNdH9Hc1sjiummcBAESliuA0ktHjVkXARkQpx79tqL7QMIAV41wIbTSiG+tnUVuxcQrhb9pHZRCPJTEWW6XEcA+vIvnI4rtRJrfB/xV4EUR+Xr52P+CJ8t/LSJ/C7gK/MXy334L+GHgCtAG/uaebvF2UKoAaWAoYkO4zeW5FgWu3fY/9ydUhiZ6Bs8bPkm2utNvTs6rG82LX9bziRqHpBNCdbrD2XiZxGT0ioCiGRI1IWw5gnaGdDJvgVF6EW07yiyXdRIEvn/V9pXOy/9tJoDB/jpmEWYf4sd5NTDYHty+PYmIEi0br4fq1pWlDmwXlcLRg9XCw7h4PsLYTvX83wP3SmB8aovnK/CTD7NRe3EiiLWoNbhoJ7PnumFKRx2+Qn4Quo6HhP6NoaiEZHWYq7eZDnwbS6uIsGuWoKUEHYf0fF6WLPf7ZafVfyM+woxCT9SlbqnPW+rGKHMYj/g+3g0GxJQ7kkVH7/WYW605JlbAZIeU3y0Lh1qAWMqAQkFldKbbDgBHcyKo3//Xj/h2Mhl0HC9aYygqAXldOVltkkhGppblXoWgJYQdxfYKTFrmaPvpie3m00pv8EHDf18Qpd/nusWy/Fij38NqBElzKvM5EJDeMURNhyncxqLZQeYWy/ymOq8uJWF5LMU82nbPO8DRJM0SppsjI5I6HFmURZkiMRSJMhl2CSWn6Sqs9BJsx1tmSObKCLPYaOWw3YukLzox5NE+EEK5V4R5XDHcitXNiOc7BK2QIvE2JxSKlsMT/ZHSA0W/VU1lIHQtxqwLpxzxY3n0SLOMajTPMUtN4lq0I5+gYwlrKRKDqxXMRL4/cz5rsNSskrTAdn1fpmReKIPUtxrtpHIrYYDEkVeRAjQvBsW0DcLPO9ULOGoYjsqdIu0uNs28/LL4gp1GAVqJMI06xiluNypcD4t+KosydxeWWgvDI65HFEePNEtoUaBrLYLFZEyY24ALBAkdYRmarxUxaTekmiomc4MpJu3nMnfSBjQcZVrjW7ZcOemz1YU+rOh+nOFK76Fe+bs6pNtD4tLrPgqRagXp9eAwVLM2E2d/qKMvsnJEI87RJM3tNqPfA/18i3Z7mNXWmDQfhL7JmfF7qlChU0Roz2IyPzZJ7kWZ2anL4lCFftD0r+t5zLtm9/s4AM/wkcPQPD6uTIfgByC0PyGVZpCmSC+CJPZFtao32RsF4jwOaZbRJM3dYpPAreY52lw7XN+fRwTOgoiSqaWrEZ0ihEwwOd7J0jnfnF+4oTnwHTS0Dzeyl5Fqf9rnkcY+zWNr4WBo+KFPRprnSJ57Qs0yJI6ROF5/4WES50MGO48Kjg5pbiZMZ8DlXvnyUb8w9xNla4saQVXI1NJzIZ0iRHLBFN5TnbzYGGXudp+Wo5HD1sQbULpXDj9/JLFfY4SDXkg35D109wCFFoUnzzSDSuKXxuXflPJ5B9oXPBRxHvXr7dEmzQf538BojSqOKMQaighM4HAqdDWgWwSeNDMwme/PdGnmtSx3m2/Mc68INRA4kYGT5V0Y1Zzmfs5cD1TQ+4R5n/xEUaBiEOOdQUXEF2OCYH25vnlkd78J9AgXf4YxkqQp1qD3cCFdf9IWJ++oRiWHge1oIIrxF1sQUCRCFOWDQlDqAkwumMLdLWW3mwimKPwUUd8Bc9N2bLQV7g8XjNDxPKgBhy3tQO71/iW5Om8tJ3nubVryCDM1ic5OobUYyQrszQVccw3t9Y5FW9B+YiRJc8fJ5PEJsBE7jYasIa/CRLXLdNgmkZyssF47M1MkKwYTQLslMr+clKF5c+dbwQyeJDfnw0Z9iXdQzqLb+fdhO96i8Etz5zBG6F2YYPlyhMkmmX6lR/z6HdziMtrpjJbK1iOEkSTNBy7NxmKo98ZwBfYBxNMv0GgUkjWUmYofoayZHmtZRNAWbLfU/8yyoXnwXezzovBdDFupvfdbjIYtekcNo36eDW+feg0FvZlTzQs6cxfhv5jnnbUK4ZcucOrLJ4jfmKe4dvPYTPHsJR69cteYMO+PrRTR7/d0a9FqTDrjeKK+wOlghUQyltaqRCsQtHIv0NFXNNoFdFAp141q78asi6Fs1dQ+6tHmKEMV7fUobt1m9nNvcec7c/zoUy/x43/1s3T/7jLv/KfnsefPYOIR8al6hDBypFncrzI7Prj3xUByzdqNHuJbwZQSbbUq3TN1nnzXdT42cYVzwRIAvbfr1G8UhAstdK21e8vhfltRf9lY9HOj5fuYdSHigSDxYeMInWea5+TXb/Dsz97k13/9YzyV3ORfPvfP+MWf/od8+386TfH8M5hq9Uh95/3GyJEmsHWEtJUdwhgbISUBDS/R7/lUX23VepX2yYDnZ97mQrhAIjnLRZV40RAt50i7i3Z7Dz3f3JfG08KtN2oDA0fP/sRQKe58qDhq55Yq7uZtLvx+h3/0+qcoFC4FBT/18c9w68M1zNTksemx3AuM3J6yfVXvzRiW2h/jbojcrYZ+v+cGfha8mK3TvCR8uPY6s6YDwI1singBwpUu2u6u62buFkOWFcNGaQNNyP74XRis2ySPsafQNCV6e4H5L53iaj5BKIY/U3+Z1feluNmJ8T7fAUaTgYY9dkpnRzHy4CXncUXfzC2K1m8498kHeovhEKnXaJ2v0Humw2PhPFUpSDF8d+0UyZLDrPW8An2/RehhIrAhA7ZB1NnPc8K6R30Ylcd/fIPcS2hRoM01Zr6t/PvW0xQoMybn9Jkl8snK1oHKGFtiNPdUmefC2MGSc3wBPQClKAYw5LdzD+K0FolCiukGy09aPvL4m8yYFCuwXFR5bWmOeLlAur2BUPCeFGX6JNknznKypS8AIsYgUej/HNRFfJxyeVlOvFLwUvMsmToSEU5UW+S1YP3cGeOBGLmWo0Idpl7DFYWfry3NywA0P4Y2rtuAWOu9jODBc/YivmI6PcnqMw3C713kL578EjUjdFV5PT3JwpvTzN5c8wWgh12abwVV0ALFIhQDQRUvHRcgUoGiQAqH7seUyTHuwJBcudVu0FXFAnfaNWq9ch8fU6O0nWIkwzc3OzWoBPdFa4+Desqu0C+iROuum/fdV2K8S2ejysrjho+fv8JjwQIhQqHwWvck8R2LWev4NqP97JkcijjX/c3VO36GZaphryPB4xRZDkN8zthFhsA4HLDsDDevTxM0D0HI+BHGSJJm52IDKglSSdYT1H1nwjE2ojQrw9hBa8/99pUYQSoV0hMV2k9kfKj+BpPGz6x21XC1PUO0jPcCelhxjgdhQ4HIQZZ6AQpVJLDruc29wjHuwPCrkYS0YZhNvAfU6/kM0fUQ0+yOg5IdYORI04qheSHwhBmG5bSIW794xwd3HX1b3LJa3h+fu/9rDFpNaJ0KOXthgcfCO9TKAltPLdfXJgnX1JPXQVnE9gtEWe6FJXJvgyxhsHfFv2NMmFDqOVQTulOGc8kyhcIXW09SuwbS7ZXTXsdrn+wWI0eaAM3HwNWrXmQVv9wcR5lbQMx6CqPfQH7fpblvS3ITFdbOC++ZucGU6WGAAmXZVVhcqxK2deCpPdjv+3VBDZw/1euf9qePSu8ZX0V/SOI85oRJqYBEFOLKLE5TA/7drctUFkph4zG2jZEkzeByk3wq8SIE2S5MvI4JxFovQGsE7XTXHSLvF5VbS/tchfbTPT7YuErDFIRiyFS5lk/TWawQdrZBwHuJ4amh1EudeUUkGeh97gr9xhh00yoAACAASURBVPnNn3XcUKZwVISwrXxp/hJf757n6usniZdyNB/nM3eCkSTNuUbLn+x9x8Lx0uFuDE3OaJptuzVIrKU7ZZmaaXEyWB2cABnQLBIkM4fmq63OCxPrwFbDKyF5D/A9WKYfx3NoaL9JL6V2s+Cd757k/2/vzaPjus4Dz99371tqBUCAIEiQFFdRi7VZlmTJdmzHW2y323J77GnP8Ux7ztjJ6cnMJDk+J4nd6+SPOel097TTmZN2OmmPx+m2O3bsTGzLdiQnks/Im0TtFiVuoriCJHagUIWqevXenT/uK7AILkChQNQroH7n4LDqVbHeh8J73/3ut35/4i6kEo8Jrqd3bdQgWZMkLuUIQKsIValBpRpHVLuNG66g3gszDO2EyAb3xXUXGSUEWaE3XSYldsphZAyhgbJxkUDs2OPoGp3VbxQNY2FNGEIgrVWpdC3MywlDZK5E5nSBLU/1cjA8AJ6hmtekU771eXY7xS2LRCrNaqhJz1Uw5XJ3W34N6gESOx3y0sKyLN+vgKtDFNFCb/AIKIU+EggqaN9CVR+Kt9BKblU+tHv9mFoNiiVkJKS/VCE1PcC5X3KY3SVkRjbhzBRsE+Nuj80lSaTSnJ1P0VeYJSpXLs0d6XKJuCGHqU91XKzgllB4KjAEoSaKN+chhqpRVIxjB6mFa2xlNmJia9MIclkXc8WloTnL+ZyuogQWLHhTnxtUqSDFItlCkYFNe5i4Cy4+mGW4OIS8Vu0qzWWQSJ9m8VweMzPb/QNejcYORvU2azFmIQh0HYURGaQGQaSoGk1gIDCGitHM1fx4JlC9LnyNLc6GSLp1Dywa7dv1ua0YE4aYmp0XFM3PUxsdp/8HR9n684jyg3Mc/vUeggdusZkYXa5LIpVmzxFNVKl0A0CLiRWmKLlcmTU2t7je9xW/pgOYr7oExiHCju8KSaZCWhdjftvNQs1/w08UEs3Mkv/J66R/kuOhO49x4qOubROnunXo1yORSnPTsSBO1O5uzRdzRbK3qEv9J5ehXEwYoiuGUtmjEKUox9YmYMvrHDDXSiivp/A0/nTpWEytRjQ5zfCjo/zs6F4+9NBzlO/Zjcp2mxJfj0QqzfTpma51sZhrNRZuVJjLsMpNrYY/XaMynuZUZTOFyKNs7OdmVJXIN4R+wxiKxvNfS67VIv4sUQloRLxBMEEVc2aEXX+pGJnv5fVPCObW3d0xGNchkUqTsanutryRxm15o+JcQdK3CWp4UxUyZx2enb6J07V+SsZBY+h3ioT5kKDHAddZfk/LVVWcybwk1zPR/DyZgyc59q0DfP6t3+fER3PIrh2I53UV51VI5BUaTc+0W4Rk0Tgsrd4ZCC61zat3RF8OUYh+/QJDT1c49sRe/nzkLRyrDgGwyxvjlv0jzOzRSE/e9rVcbkXOapY6Np5voYyzm3p2wzCGcHyc7V95lf/zpffwmw8/wvHfyxDefxs6n+8qzkUkTmmGJuqmGTUilzrWm8buRbJoC92EQjGFAqkzMwy8HPLKoZv48ewBysahT5e4d9MZ5naHhP0526OznmB+1blNiyzRlhWnusxn2+28s4YYQzgzy7Y/9/nuhbv43bsf47VfVdTesAdx3K7ibCBxShPo+jMbuZaVV1ekpgkrMyaqBnBxjJ5Dkww8o/jhiVuYjjKkJOANmXMM7R+ntDNrrU1nGZ2GWlWcV/s/9bSjRsu6y40lCsk8c4qL39rF6eoA/+LNj3DsUz56YFNXcTaQUKXZvUmAK6zMxrzJy63MJhcZExHNFWF0gk1HSqhXcxyrbAVguzPF+4YPM7XfIRzI2xZ9dcW5WIGv1uLW8Pe+7PfsKsw1JxybYPj75/ivP3g7fbrEP3vHd5i7bxeqJ9f1N8cs+S2ISEpEnhaRF0XkkIj8Xnx8j4g8JSLHReTrIuLFx/34+fH49d3NCBStXvFcZ9MQLV9QJPWXlFyaOLkSV0ZsnZpiCXdkik2vRjw1s4dS5JNXZd6YOcXcGyqUtmeQnrxtK3atG2Y1E+AbFOWCK6LbR3VtiULCkQvs/ctZ/smLH2GnO8Hkp+cIbt9lU5G6OZzLsjQrwLuMMXcD9wDvF5EHgT8AvmCM2Q9MAZ+O3/9pYCo+/oX4fV2apT5QDq5MZNcaoujS1nwlCsUYompANDZB76FpfnLoZp4q7aNsHHY7E3zgjkOMvdGhunMTqid/5aCzJfp2rkSexsTrhZ/GiZVd1gRTqWCeP8ye35njHz/6P/HHd3+N3f/2KBMfvQM9OLDhq4aWVJrGMhc/deMfA7wL+GZ8/CvAR+LHD8fPiV9/t1yR9Hdt7PSSDc7ibTksKA5pGDTHShVmHRNhqgFqZo7UGZfnZ3dSiNK4EvFA/gTV/fMUt/qYbNoq6rryNg0yXUupdRPgO5soJDwzwi1/OssThdv5+MDT3P3rLzH+/n22amgDsywnhYhoEXkBGAV+CLwGTBuz0EzqLLA9frwdOAMQvz4DDFzlM39NRJ4RkWfGJrrR8su4rCyyYWtatzK1XlEA6ArqM3pKJbLnDUcnBrlQszfEXm+UN+w8T2GnwmT8Bqv3Gkq6q0DXHSaoYl55je/+0Ts4Ewzw21sf4y2/cZCLHz2ASqXaLV7bWJbSNMaExph7gB3AA8CtrZ7YGPOnxpj7jDH3DQ50/SSNLNSWL/IVipKF6Zyr1YXIhCGmGpAZC5kazXOqspnAKPpUmQc2nWR+yBBm3OYqdK63pb5aKWa3NDOxmKDKlr8+wp/8m3/A48UD/OrAk3zyNx4lfNOtdmrABqSpcJgxZhp4AngI6BORunNjB3AufnwO2AkQv94LTCz3HOFG919dEQAyl47Xh6iF4cp9mVfBVKukxqo4Ey4n5wcoGpes1LgtNYLZWiZMO5fyNVdK3XqOfbWiNeK4iOehfB/l+/a51jbYsFiRKm2j+PXJm13lumaEk1MMfvsIX/yzhykbzT/uO0zpn8/CHfsR12u3eGvOcqLngyLSFz9OA+8FXsUqz4/Fb/sU8O348Xfi58SvP266WcrL52oRcxHEcVGZjH0tDFevmUnck9O5OEPulPDi6DAXan24AjvdCbZvmaaWWaGSWvRnFyWxsnRQmQx6eAi5bR+1N99G7f7b0Du2ofJ5lOdeplhVLoczvBW1bzd6xzDiNhmIaFC6XYW7AowhnJhk+IvP8Vuf/d/48uw+vnfHV3nbl5+l+PffuOG26suxNLcBT4jIS8BB4IfGmEeA3wU+KyLHsT7LL8Xv/xIwEB//LPC5ZgTa0IGgegBocZqNKMSLxwiGoe0AtYqYyCDzFdKTETOzGcZqeRSQlRr9qRK1lLRuaTaiFHgulb2DXHhLH6fen+Li/WmifBoWRWZFK1QuS3X3ILN3DhBuyjd5Lqt4dS6LymXt9E7dVZwrIapUyD95nD/87ocITMR/0/Mc5z9WQQ0Nbqjvc8kl2xjzEvDGqxw/gfVvLj5eBj6+UoGCjVwN1JgL2WClidbWuqqXmK72dxRH0f3pkGjW5Wy1nwhwJaLHm+eCL4haYWKzMfaGMhEmUoiKk/N9n+l9PtP3VRkcmmHm+c1IObCTKMNLSe0C4DiUB32KQ5rsaYUst89qvAipbBo299tD50e7ZborxRiiwhy7Hylz9r91cCViqH+WcHMPnGmys34Hk7iEq8JGVZqNaUaNN7WItTK1xpQrracZXYtaDWcuwJlNM1bNAaAx9DgVaulVtjRFwHMp7hAO7LqAVhGlwiDMFGzz6YaFoe7ZqaWEao/9700pPa2Rvl6CLT04M2Wb2L/aVUZ1K+uyRW+dJuSHIe74HF+depAt3iyjLw3RWxhlY6hLS+KU5mTotlsES2M7tpjL/Iw34oYQdaUVKcq26IJLAaDVxtixuboY4M1mGCvnCI313fQ481ZpOi0ozQZrE2wwy6Q8ykM1bspN8erUENnzBjNXvLL5dKzgQk8wDkgQYZpYWMXzCAd7iXyNlMpEQW11FVrjdaI1ImIVfb3z1Eqvl2ttd2+0Im74fa41PkWmCzz6Xx7CKRn2PVPAnB/dUA3DE6c0nywdaLcIsXVno7qkYyd3GEJoB1RZ5RU1bJdbvJAlHlerBLNo5ri4DpLyMcWSbbRxo26aIEAVSqTGexiZ642357DFm6XaA8ZbpcVMFOI4BANZhveMk9ZVRo4OcvPRIqYaXP3m04ogJxCBKpaXn2EhCtXfx/TuDPkTc0Tjk5jaKn+H9cFlkbJTNHWs7JQgRjBRvBCKLN+l4HnozQMU795OmFI4pQhn3i5q+uwY0WyBqFy5IdasyuWQrYOUDgxABNnDo4Rnzl22WEdT02z/s18sDGszG0hhQhKV5tR+mshQWppFFuOyho/FN7Zk0phNPfYzwggJash8GVMLkcDOZJcwbP0CrietWwEvk108DxPFyvoGXpzGGFQlwJszFMo2/04BeVUmTBtYXEa5UpQgvkdlwOPWvlEmq1lypzTO6Cy1q1nRSjCeSy0LThmkGiz7VKI1taE+ammFnioSlis3ZtGpK85G8a+YELp8RS+eR7B7C+OfKfH2na9RCR0OTW5l6rlB9n2tipTmEVW9IS5EU62iaiHlPs30AUV+6zCD3ynYBi9hCKKsz7la2lDWZSOJU5oHX9nLgdVQmnVlqbX1Ccb5jQQ1e7EtdcXFW0rjOUS+g9EKVa6hXAepVKGsoFaDaoC49Yj2yhSn1Ld1YXTlcc+9ZNneSCIDQYAzb6hWdTxozeBKjcjhUoOQVqj3AE35lAY1B7IX+d7IHeTORpjZwlUDXCKCSXsEWcieM5j58vJP5zoUd2Rwyiaebrp8hds0DYpzYWsLzS+mJkJECPIuH9/3NP+g9zkAHs/fyh+99iv2+q3VbtjQQVOtEo5cYOAnBre0jbF7NP47b6bn8DQyOWOty3IFU7qB32XCSZzS7Hm1xW1gfasb5wOK64Dr2Yu3GtixsCa6fgDaxHOi5+eRYholacKMIko5SBSBtje/BDWMqIX53CaoNR9BVHqhGcbirb54HuJ5RLOFGx/xNREmjFCBASPouNuUFoNZDSPTGBsKVwrje8wPCjld5sy5AQ6cKmFK81dXAloTZj0i35CeiKAZpen7lHsVPaerVtneaH9gw1a9pQwHJYQpYX/qIlt1SGgM/XoOCQSZrxDdiAyKOsZgKhVqp8+Rm5gie3Y3J38bRspZci8MMHRwHu/kGNF8ecNEyxeTOKW5+cXKyv5jfSiX511Slo5zyTlfa+Iii+u6o7kiShRqvoJk03ZuThRB7ep+TFGCMcv0XTX8H5SKFW6DjErbMjXPtfPfb/BWqDHIpZ3osgReMVyaQd4qWmPSHvPbQgKjSZ32cEbHCavVq75dtCbosYGw1ISd271cJOVjNHgT8039v5Zp/DuuUFFHjtCniyhAxde2rgomCOLmzDd4AYhCork51CsnCE/cxb/56FcpPuDxL+/4CPu/Mog+f2HD9gpPnNL0Dh5tLr1dLlWZoLUd0aD0pVrpyGBqgVU8gf13WVZbHFEOp6eRQsF+dmzB1s9ro6SR3fZfNoqiOac/0ZUBJZXyEd+zW6G1uOHjbaTRkParC/GMYuSjqsAqJdSL41DalmX4wBjHS0NsOhxhJqeu/jcRAd9nbpuDNyV456YJm/Bpmv5e0pMRcm7MWmdrQctBQQWOQ6VHkVdlXFFExpBSAUZYeb7sSjCGaL5Meky43bvAVg2n7/sxP/ibd9JkicG6InFKk6AJX8lihek6l1JbauaSLzAIVh7tjpUnYWg3rPV0jPrLjTl/TS699cqUumwLKH2pAmgttpV1IkPkCBkvWLA058IUTsm6IlqVQ5TNzywNOfzy0AmembiJ7LmKzT+9xmeL51LtEVKTBmbnmrK4I88hNVHFlEodlTMprkulTxhQ82iECIPGEPnG7p6ULH9hblUWJcztDRnUBlc0r5UGSY9Vk9lRf42qkhLXv76pgEcc5ca1jR9sz0cDtRqmGixYaXXrsqX0oIYGuaZWW/hpHM3QFPUGHFzpy1Sei6RSmDAiqqzQXbFCIkfo8cvUszILYQp3DqisjrUrnsfcTuFN2dc5cWYQd3zOfo/XwvcwGjKjofV7LvtEAlpwR+cwa/wdtoIoGygrDxoycvkCYZRZ83nwKpPh3fe9TEZcClGNH71wG/7r4xs2cg4JVJrLVj6LAj5o2y7NVIP4J1aWQe1Sg4sblW6yks+tR5Kv4tSXlG8tiqC66nXmSxE50OOV0SJEwESQxZ0z1pfWKqIwaZ/5nQFbnRm8sx4yW7yu1WJSHqoGqfFq824KY2ByprPKJkURZdMEQwFZJaj4Fi1GHlKTVWsJuDxZBHZs5Z9vexQtwk/Lw2x7QhGNrWJKYAeSOKXZjNkvOh4JUW/yYOKteNCgLNeqnK0Z5SliXQlax1tzc9lr+P6CP2lNV3QTETlCr2stusDAZDWLWzSr49NUQtSTZvtNE2gi8qfBFIvXXihFEfkuTtHgTBabm4ckCgnNtaPyCUW0otaXYvOWWVxbeU+AYbzWgzsn1n21RopTZTKMvGczQ9qnFAX8sxcfpu/Zi0Sl0pqcP6kkTmkui3rPSaUubVfiLXlUDWw+XoLny4jjWuv4Kt3XxfMQpTCl+bW1MuPvKfRhqz8LQNlozs71kZqqXX8LvRzioNfs/hz/cOeznKttov/lElHx2kpNlBBlXHIXajA60ZQM4jqo2RJmvoktfbsRQVI+M/vSvGv7URsEImI6gh9P7qPnBETF0qr2Ur2mKI5D9c238q9+40soFF8r3MLwFz2iU+cSeU+tJZ2pNOs0NOy9ota3XSylqOt5mUpdKa+ILd2EG14BdC2Mhpy2uZBF4zBVSuMUl5lxsATie8zsUdzsX+B4eSvuxSW2zqIwSvAmq00ltYO96SnNd9bWHJBUisIu4Y2ZUwtb87EwzZGxLeRGAmtprsE1rnp7OP1+jzf7U8xFFf7tz3+F1NELN7ZAoENIntJs5oIw9ZLIaKFJwsLxJFLflitlU5UW5duJ1jYvs1ptW/AicoSU1AiNoRClKJZ8VDlY2ajgRkQhuSylvQF5Nc//N77/mlVAC8S7CGe23JzVXS8/La9h5sFqIAoyacq7q+x2xwEIMZwMBqm8nscfiYNmN/p3Uprw5h18/L0/wReHJ8ubGXrcIZqc6qzv8waRKKUZrmQFjQyE0UKlT5JZGOcQp4tcsTV3HCSdtoGsdlhIojAafBUQAoUoTTjnosq1llNMRGvCgTy37bdTUY4c2b5klY6IILXIBoua9GfaxaezrCJRQpRLMTw8yaCeR4tQMRHPze2i96igxq+Rz7rKqGyGCw/l+B82/ZyIiN9+7mP0PzuxtgUCK2GNFHqilGazLKQRNfp4kroSqsY80qu7EsTz7Gs3qrHEMogc8KRmI+e1HGpOr0q6kbgOc3tyfHjoRSbCHD2HnesrNbGVUqpci/Msm2gHp2zT5MTf5IsRRdCf4da+UfKxlV2MDC9M7aDveJWoMHfjrwsRZHiI7R8+yYA2XAxr9D2ShfOjrfu11wmdqTTr2/KwIWcyyb4rpVFxI+G6L9MsShYX10OyGRvQapffSAmRC66EBAbOVvtJjSukON+yFS/5HBceFO5Nn+TbE/cy9HRxyd9TPBc1N48pNpecLo5jO0Ml+Zq4CpLymTrg865Nr5IRTWBCnqsMc+4nO0gdOW+zKVo+yfUnfuq+Pi6+Y5BPDj/FwcoA7/3RbzDwyGHCmdnWz71OSFRFUERzq6gJ4148SaxOqFPPJ3XduAa+dmVupggqnbIpSLOF9lnLYhv9ulIjROIcTVYnR7MnR3qfvfF+fGov+y9MU1vq99TaNqhoxsIRAde1zVmSuuu4GiJIKsXcLtjtjqFQzERVHpt+A4MvhkSzhdYDgw2Lt6lUrm45DvRR3AH/4fV3cPHlLdz8jaJVmJ30Xd5gEqU0m6Leui0yC3NnEqk8Rdl8UsdZ6Lx+RTcjx7XNjutKtY1ErsGTkNCIzdEs2Aqrlm4aEWqb87zvpl9QNi4cy2IK55b4PwqUxszPN/d3jb/vdn+PTSMKyWcx+4ps1SW0+JyqpXns0O3cdniKaBVSp8R1kJ3DBFt7cSeKRMdOYoIGF4YIUgnY8UQFeayHW06eIRwb33BNhpeic5VmHRM3f73aqIgEsNDkw3HiNKJFKUlxbp64cdS8nX44UUQuKCICFJOVDH4har2XpyhKwynuzp5mrNZDz2ssmUJkx/0qonJzCf6ibBqaqa7CVnYNEdehOtzHg7tOkFdCaAyPzN7Dlsc9GLm4KnmyKp1i7JeGCD8yydSFXm77fI6wMSJuDLVz53EujIKJbFPoroV5BZ3p06zTmBOZxEFWcfDHtqvTsf/18q2uOLbOnFrNzshpl4UkgjiaMGVISUApcrk4lyM1ETTXROVqn+s6TNypudU/z5OFA/QfWkY9eL0uv9xc6pU4zkL5bCeheno4944M//3gz/BFMWcCvv7Y29j8ozOEs3Mtf75oTXD3Xh7+rSf43j1f4n98809sIG7xPROFmKC6NqlNHUoilKbBEJrIzjxfyR8qqZU/DVYmxlx1kqR47iWF2u7GEloT+RGehJSNS6GYQheDli1NcRwqe8qkJOTg2E0444Ult9wLA8qa2T3EEfer1fMnGhHY0o++b5qb3Sk0wpmay/CTIdHE5KoUOahclrO/nOYf9T2DK8JfHH1T0wtSF0silOa6RMTOw9HqklJcHM2Nt+amZjv4tD3a62jwI1ypUTQewZyHKrVYgSIKSfncvus8gVGcPzYYR8Ov85n1DlDRlbmsS50LuCIzIfGIYm5/L39/98v0KUVgIh4v3kb21TE7f2oVMLu3894PHaRfOYyFQs93c5f7M7ssm67SvFHUuxi58fjdq2yF7AwgD2q1eDhbe2904zrodA1XQqbDLKrgIOVK64nt6TS/NHCc6ShNzzG9vCYaSjUdgBIllxo6dxCiNeN3Ory/9yV8cSiYiK+9dj+MTqxOKa0IF97ax2c2P0mI4T9OvJ2Bn15o/XM3KIlSmssezdoBiLKpL+JoTHCVMQ31Ur8gsE0l2h2hjKto0ukqHhGTtRzelELmKy1ZmqI1pjfHHekzHKtspfdEbWkfad1ibNa/G09K7LStuaR8gttL7HWs7/JMLUPl6f7VycsE28/gfZMM65BTNeGJ//KAnVW+3tiITYij5gZdJJdYIUomDYApzduZ5Q2vK99H8jn7Wrt9mVglH+Z9NueKuBJxsjxA5kLrrdVEK+Zu7mVAFfni0beTPTx2+XdxDVmojy1e9oniGyaJAcHrIQo1OMBn7vwJfcohIOSrE29h288rq7N9VprongN8/Z4vUYgMD//t/8qOb5zc8O3dWiFRSjNsMrk9sYiyAR6lbPf44PLBaOK4SNoq1CRsy+uEaYdez1o3Fys9+DOmtWh+nGhe3KKpoikd6YPCdfpnNlIPnC37XLF1msRc3esgrkPlpn7ekT2MQjEZhjx29Db801Or8vk6l2Xk7Tn6FDw5v5tdfyWE4xOJueZWFVkbdZYopRmtkz+kKFvdgTE297Lx5hex83+8uGql3dvyOqKopTWbfGuBjM3n8Apha+lG2Mj5/BahEKXoOcHSI3jjdn8mDJtXgCsZO9JmlO8zs8dnhzNPRMSRYID082lYje7oSsPObbzvH/6cYmT4lz/9CNmXRjqvJj9hJEpprhdLUxw7PtiUyzaNaLGVmfKhGhDNFdso5SKUEKYUeccqtcn5DG6x1npiu+dS3hJysjpI/mzN3rDLWRyj5tKNREk8hriDriERpCfP9C2QEqFkQr4+/gCbf1FdFX+m8lzG7+/nMwM/5mBlOzse0UTr1cpcQxKjNLWo9eHRVPqybkWXWUv1KZPqGilIbUREqPlCWgeECLPFFLpUa9lyE88jd9MsR0pbSY2Wl7/d7zTf5AoQrYk295K6ZQaNMFJz+NHzt5E5Ota6NSiCGhrEfGyCnY7i91/9APmXRhPhP+90EqM0AQod5o+6grj6RTJpTGHOBjwarEzluQvNcaNiKTlbcwClCH0ho6oERlGdSqFmm6z7Xowook053rnjOD86ux89NrO8zzNR0+c1kUnW97kUIqh8nvF7e/n0gZ8SYPjj0Xex6xFDdP5iywuG8n0uvmcH/88dX6EQ1ch/uYfozMi6X4jWgkQpzbEw3W4RWmIh71LEruiLOhmJ51kLMwkpRosRIXJAS0RgNM6MRsotphspIcyn2JseY+5MT9x0eOnPM01uzeP/tEIp24QopK+H6QNwT+o0Y6Hib1++jczxqSWzC5aDGuhn7lfm6FchL1YHyL80uv59mWt0DSRCaUo8dW82SrVZkhYQsZ3Xfc9W+Czul+l5NvgTtKkr+1Io27VdYaiicWfFNh9uqbuRotrrMugUyJ7WUGkiU2CdW0SiNdUd/Wy6c5xhp8DB8i76n3ZXLaG9un+I//3uR9Ai/KsTH7Bjd9f5d7pWJEJp1jkZDLZbhBVzycpUV3YaVxrl+5hKNbZAk3nxRtouXuXIxZttvY+maMX8Zoc+XSQ7Ei2/iUazFkPcDb9jEEFl08zsTfGe4SO4GP7z2QfZdKSyOvmTSnPh/jR3++e4EGomHx3urKmcCSdRSvPY/FC7RVgZ9eCP70OlYiPm9ZtYBJXywffbOsZiWSiIEMrGxZtdhVnnWlMcFgZUkeyF4MZ1cEryd3o1RCG9PcwcgAdzx4mAUy8O45+aWJXuTCrls+X9Z+lTEd+Yvp+hp0qd1180wSxbaYqIFpHnReSR+PkeEXlKRI6LyNdFxIuP+/Hz4/Hru5d7jm8+c1+z8icClfKRdApTLhM2tncTQWUytvKnXE50gwQRiXtpGkqRT2a81rIPTOVz1O4tUDQe/sjs8pLVE9qxajVRKZ/q7s303D3BbneSQ9Ut7Pi7kOjiWMtbc3EczG17+P193+Kpylb+5v96G87zx1ZJ8oSTwOT23wRebXj+ZhTlaAAAEt1JREFUB8AXjDH7gSng0/HxTwNT8fEvxO9bFt5oB/ZEVtqWSyp9xUxq8TxUPmc7tifdCa8uBYKKkY8712KOpgimJ8cv7TpBIUo3P1FyHSPpFLO7Unxw5yE0hicLt5A5Nbsq14jKZDj3rl7yEvBPX3qYLT+fXLUa9qQjKkG15yKyA/h7wH+KnwvwLuCb8Vu+Anwkfvxw/Jz49XfH71+S/OvLEzoxiFyauVIu26hnbCWJ46AyGRuVLswlX2nGXdtdCZkJM+hiiy3hgNrmHG/tPcaxylDLNezrBhHY3M/U7fDGzCkmojTfeuleGBltfVFRGnZs5Q0fOcyRYAuZR3owp0eSl6nR4SzX0vxD4HdgIf98AJg2xtQdJWeB7fHj7cAZgPj1mfj9S9J7MuGKZRGita3uiUxc6dJgZfq+zdesVjujv2M8idJXAYUwhSpVW87RnN+a4mbvAgend7e/wXJSEEVlRy/5N0wwqGf5WfFm+p7ybACo1dzMlM/YgwP8+rbH+Xcn3svmZ6c2VABInLXZqS6pNEXkQ8CoMebZ1TyxiPyaiDwjIs+MTdiV0D9fWM1T3FjqnYxSKUypdLliVFaZmmpANDvXGSu90kSeQRMxW0vZlnAtIFozN6zZ6ZR47tRNybe01wjRmpk9Hu/ZcZTAOPz50QfY/EKp9QCQCGqgn/Ajk/SpMrPf24acPr+xAkCuuyanWY6l+VbgwyJyEvgL7Lb83wN9IlJX7TuA+njBc8BOgPj1XuCK7gPGmD81xtxnjLlvcMDOg5GpzpmtLI6L5HPWd9dYX14fx6s00exsooM/jYhW1NIGV0JOlfptY41WEts9l+k7AxSQeS7d9WeCvTZ6ckw8UOOh3HG+N3M3uW/34Bw+3fLCqnyfkYdv4s/u/M8cnN/N9u+e23izyvftXJPTLKk0jTGfN8bsMMbsBj4BPG6M+STwBPCx+G2fAr4dP/5O/Jz49ceNWd6+o2OskbhTkaRTV3QxEs9D9fXa1KNO+X3A+mU9+2c6X+xpebCWpFPs3jvKeOiSP92dagjxzKiBTdx/+wlcqfFXr9xD/0szq5KbqQb66f/IWTJS4w9efJ+NxG+w77ywL78m52klRv+7wGdF5DjWZ/ml+PiXgIH4+GeBzy33A02xMxqjitZIJmOraBpzL5W2wZ8wtBHLTrpolcI4Bi0Rk4WsHTXRAtKT551bjnG6tonsuY0Rvb0uIojvU9nZx3sGXuVEZYjen6SQsxdbXlzFcSjevZ1/sud7nKn1MvCdzKrNFuokzv3K2pRRNuU5Ncb8CPhR/PgE8MBV3lMGPr4SYVpuQ7ZGWF+mD5WqHYMKC0nsksva7ka11uuH1xQloK2SrxS9ltONwoE8d2dO8/js7Tijs3Q35yC5LBO3+2x1pvnCyfey+cUSZq7YegAon+f0BxR9ap7fPPIJ+n9+gVon+NFXmU8+8PM1OU+iKoI6wu9Vb++m1WUTJG039pStLS+2fiOsOUohbqwoiw5ErU2gnN+a5m7vAj88eQvMtD63u9MRrWHzJgr7IkqRz7mDw7inx1tvziFCtG87H37LsxyqDBN8dYjowjqc/7MM7suuTc5iopRmJ3SqUSnfRswLczadw0SI66F683Hwp0DUgWkeRiuUZxcAZ1Y1N2piEeI6jN/pkFWC/mmvXUQ2MnFl2PQdm3jngy/z/ck72fF41TYEXoUA0JFfTfNg7jX+j7/8OAN/c7wjr7/V4D3p8TU5T8KUZsKtMxFrTYpgypUFK0GlU4jvY0ole8Em/fe4GkqhdURoFHpeWGbs7qqI41DaE1CMDPnT4arUU3cyojXS38fMXsUv9R3lyRduJXVqFVrAiaA29fH37n2JH88eYPjHAaZQ6MzrbxVwRa/JeZKlNBOOaI24ro2YV23FjA0KpTHVID7WoResVrhuSIhCV7DjJlaIeB6bt81wLNhE5nylM9wuNxBxHILt/QR3FRl2phh4RtsZQC3XmbvUdg6yP3OR771wF+nXJlalF2en4tBVmslDa0w8xgIT2W41vm8tz2Kx84I/jSiF59QIjLZKswUk5bOrd5Ifzt6BOzXfEW6XG4lkM0zfnOb9N7/CSG0T/a+UWq8Hj+elzxzIcmhuO1uf0KvWi7NT0Qls2LGxUdqWacXWpInMQuqRCYLOKJW8DkYER0ex0lxB5/RG0imG0zP8bHQPMtd6eWBHIwL9fUzdDvflXuf743fijkyuQgWQQvX3cfEtEX93+Bb6Xp7uzjJfI7pKcznEjTkknVoYiCauY32ZKd825OiQyp9rogWtIkqhj9PivRduyrLdn+bii0OYjVaVsgjRmsIdg9z90DFmwiyv/vUtrbeAi6vOSrcOcf9dr3HTNzS8dmZjlUy2ka7SXAb1sbvi2FQciUdb4DqYuWJnVf4sQWA0OjAtNesIMx6+CsielUt5rBsUcRymbtG8rf84Pxh9A4MvVFYhAKRQ/ZsYeZvLaClP5ujEhmn/lgS6SnMpRBCtwPVsRFkpm9wet30znRotX4wxhJGiFHmooLXfJ8g5pCQgezHa2EEgESSfp3JXibwqc/SZXaROTrZuZaZ8Kvu3kL17kpHntsHF8Q3ty1xrOrDr79pSb/8mjsZEEeLbOUCiFVGhtH7mSEcQ1DTFmo9uWWkqxmt5vJlaS/menY5oDf293L/rFE8X9jB00NhgTUsfqlC9PVx8UwpTq7D1Z2HXl7nGdC3NJbDbcA+0RpRC0mlbQgmdnWK0CDGGWqSYDtLocit9NIVKn+K10iDezPpxW6wEcRyqwz3cnj/P3x69lfyxQsvbaJVOEewZYv6N84Qv9pI7PLmxrfk20FWa16PeM9NzwXUg5YNWUKsRTkx1fvBnEWGoODvXh1torWt7cZtwcOQmnNHZjXtDiyC9PYzem+Lw3FaGv+Uhx0+3ds2IYA7s5vgnUqhTKXZ9ZwZz6uy6Wbg7ha7SvB6i7IRJ18F4LkYrm260qB3cesCIYIwwPpdFlVuwoEVR2RJSOp+zXas26A29UGt+R4WfHt9L7thMy13UxXGZvKsHkwsZfMGgzo6uH/dQB5EYpRkmLQFaBHEd68v0XHC0zbmrhURzxfXneI+vhOJ0GlVeeeqKaI30Vsme0nYu0AZFfJ/S7h7esGeEnoMp5MJYawutCCqXZeIeg3fOpefIzIYumWwniVGaEcn644uOk9l9L7YyxVqZlcr6rKVWChGDnnJaGnUhWuGnA/KnIjudcyMigurJM3aXy+7sJAOHKi23gBOtYdsgDFYYOGSQCxu7ZLKdJEhpJsjSjKt/JJ3CpH1wFFKLkDCyF/96szIB42qUMvjjyo66WCHieRgjZEcqGzbZWrQm2rKJ0oEKI/M9+KcnW1Zwkk4zfVc/jPr0HCnYooF1eB12AolRmqUoAatmnLSu0imktwf6+wjzKYyjIKjBfHl9+pBECDMOtZoiO2Ja6qAvvT2UJ1O456fXnd93uUg6zeTdfey/aZSXf7wfM3KxNQWnNGpwgPPvrTH8pEG9fpao3E1mbxeJyNM0GIoJ8GmK1nb0bi4LA31EWZ8o7aDma9bKXMezu2tph1rN4M+2kJAuQrQphy5opLhB/Zmx73FmP2QixdAzUevjLLSmunMTm4dmyb2miDbqd5sQEmNptl1l1tOLMhnozRNlfTAGXQzQxer6tTIBRFHLKMKyg1toISFdFJWtOfwJFc+BX58LzPUQrYkG+ohuLnFmtJ/ciULrAaBsmslbU5QDBzU23dndtNYBiVGa01GbjV5RSCYN/b1E+RRGQM0HqNl5pDhvlcA63W6KEoKMQEXhlGorbkAsWjO/2SFzIW6ftwERx2H+pjzZTAXv1TRqfKa1xUMU0tfL9G2G4ukem7mxARejJJEYpVmIvPadvN7FqCdPlLH9MVWlBtXAKsyZAqZYWr8+OlHUUoIuKvRsZcWLg3gupS2K3pOVDduoQ3JZ5oYdSmWPTUdCzGyhtc/TmsruzagtZTa9Ii3nenZpnYT4NGEizLXn5CILXYyIIlTJWkgS1KA0TzRbsMns6zkSrITQE3KnBTU1S22FSlP15CkPGvwTYyv+jE4n2rmVqTsM3qEcvS9ebK0uXAS9uZ/X3+0TTkRseWqGaD1fhx1CQpSmoRT5bTm3HWHh2OqfcgWp2ZvdBAHMlzFBbf1amA3owJAajYhmV5gwLQLpFE5R4kqgtnup1x4RCjfnMQL9hyMYHW/p2hGtCbdvJthVofcpOyO9S/tJzPb80ak71v6kSltlKQLYuesmCDDlslWY1eqGuPlFa7xZQ+ZceeWRXlGYtI8uY7vbb0DEcZnvF/InFL2HpmyUu5WEdt9n/J48AL2vBxu6wmpJ1NrMB4KEWJoAM9UU0Jr/pylEEBWXSqZTSDptj8ejLFCxIo3Mune8i+8TOaBLLdTUm4jKUA4VxPPr1/l3djVEK1QIPUcDOHeh5Si36skzczOYokPm1AzReqxEWy3WMNE/EUpTo/ir/T+EEQhMeENGcV6vtn2tBjI1yqBFEZiQD37iM6gnn1+T818VEfBcnIpBTRaorbRjuyiqfQ7lQXPpc1tVnPEOgMV/n/r3mCTFLLKw8LqFVZgZJcLsg7swO8r4x9PxbKr1v+tZMfVrZQ1IhNJs5EbNLl4rxdiMDK5ofvj1L7dJmhvBs/afz7RXirWivghGGAITUjE1TtQe5T9cfBdPvr6fyAj7hsZ5U/9p+34UN/kT7PNGGdQFNIaycXipspMv/f7DbP7xeaKRC6DsOIvCDk1Y1vScNzajI58nKpUu9UVwHZuCFIbxjmiRUq1fb0lcZFYZ0Wu3PZeV5uStJvfdnTJPP7oTsBdiEhRcly5dksVjJZc/fOsvE46O2QONuktpHjnzdEtGl952/FljzH1LvS9xlmZXYXbp0uVqvC8T8L7nH7vOO9bG2uxqqC5dunRpgkQoTZOwXppdunTpci0S4dMUkQJwpN1yrIDNwHi7hWiSrsxrRyfKvZFl3mWMGVzqTUnxaR5ZjgM2aYjIM50md1fmtaMT5e7KvDSJ2J536dKlS6fQVZpdunTp0gRJUZp/2m4BVkgnyt2Vee3oRLm7Mi9BIgJBXbp06dIpJMXS7NKlS5eOoO1KU0TeLyJHROS4iHyu3fLUEZH/W0RGReTlhmP9IvJDETkW/7spPi4i8kfx7/CSiNzbJpl3isgTIvKKiBwSkd/sELlTIvK0iLwYy/178fE9IvJULN/XRcSLj/vx8+Px67vbIXcsixaR50XkkU6QWUROisgvROQFEXkmPpb066NPRL4pIodF5FUReaitMhtj2vaDrXt6DdgLeMCLwO3tlKlBtrcD9wIvNxz718Dn4sefA/4gfvxB4AeAAA8CT7VJ5m3AvfHjPHAUuL0D5BYgFz92gadieb4BfCI+/ifA/xw//nXgT+LHnwC+3sbr5LPA14BH4ueJlhk4CWxedCzp18dXgM/Ejz2gr50yt+VCa/gyHgIebXj+eeDz7ZRpkXy7FynNI8C2+PE2bH4pwH8E/rurva/N8n8beG8nyQ1kgOeAN2MTlp3F1wrwKPBQ/NiJ3ydtkHUH8HfAu4BH4hs16TJfTWkm9voAeoHXF39X7ZS53dvz7cCZhudn42NJZcgYcz5+fAEYih8n7veIt39vxFptiZc73ua+AIwCP8TuQKaNMfXOu42yLcgdvz4DDKytxAD8IfA7XJpAPUDyZTbAYyLyrIj8WnwsydfHHmAM+HLsBvlPIpKljTK3W2l2LMYuY4lMPRCRHPAt4LeMMbONryVVbmNMaIy5B2u9PQDc2maRrouIfAgYNcY8225ZmuRtxph7gQ8A/4uIvL3xxQReHw7WTfZFY8wbgSJ2O77AWsvcbqV5DtjZ8HxHfCypXBSRbQDxv6Px8cT8HiLiYhXmV40xfxUfTrzcdYwx08AT2K1tn4jUS30bZVuQO369F5hYY1HfCnxYRE4Cf4Hdov97ki0zxphz8b+jwP+LXaCSfH2cBc4aY56Kn38Tq0TbJnO7leZB4OY44uhhHeTfabNM1+M7wKfix5/C+gzrx/9RHLl7EJhp2DqsGSIiwJeAV40x/67hpaTLPSgiffHjNNYP+ypWeX4sfttiueu/z8eAx2NrY80wxnzeGLPDGLMbe90+boz5JAmWWUSyIpKvPwbeB7xMgq8PY8wF4IyI3BIfejfwSltlXkun7jUcvR/ERnlfA/5pu+VpkOu/AueBALvafRrrg/o74Bjwt0B//F4B/jj+HX4B3Ncmmd+G3aa8BLwQ/3ywA+S+C3g+lvtl4F/Ex/cCTwPHgb8E/Ph4Kn5+PH59b5uvlXdyKXqeWJlj2V6Mfw7V77cOuD7uAZ6Jr4+/Bja1U+ZuRVCXLl26NEG7t+ddunTp0lF0lWaXLl26NEFXaXbp0qVLE3SVZpcuXbo0QVdpdunSpUsTdJVmly5dujRBV2l26dKlSxN0lWaXLl26NMH/DyXMCXX+eBMoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa23bdf5110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "img = np.reshape(imgs_mask_test[0],(480,640))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa239c81b50>"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa22e000e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "img1 = np.reshape(Y_Test[0],(480,640))\n",
    "plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.copy(img)\n",
    "img1[img1>.3] =1\n",
    "plt.imshow(img1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
